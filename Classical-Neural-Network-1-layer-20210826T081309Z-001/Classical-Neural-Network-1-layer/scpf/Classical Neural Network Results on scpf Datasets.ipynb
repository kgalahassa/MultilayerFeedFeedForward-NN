{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\pottsmodels\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From <ipython-input-1-a74d20627388>:94: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "LOSS at step 0: - Train loss value : 1154.701782 -- Test loss value : 683.128479\n",
      "LOSS at step 0: - Train loss value : 476.647430 -- Test loss value : 682.931396\n",
      "LOSS at step 20: - Train loss value : 749.076294 -- Test loss value : 676.307800\n",
      "LOSS at step 20: - Train loss value : 1195.392212 -- Test loss value : 676.151733\n",
      "LOSS at step 40: - Train loss value : 857.700073 -- Test loss value : 670.101440\n",
      "LOSS at step 40: - Train loss value : 526.549072 -- Test loss value : 669.959656\n",
      "LOSS at step 60: - Train loss value : 899.596069 -- Test loss value : 664.068359\n",
      "LOSS at step 60: - Train loss value : 1102.836060 -- Test loss value : 663.919250\n",
      "LOSS at step 80: - Train loss value : 15489.024414 -- Test loss value : 657.730103\n",
      "LOSS at step 80: - Train loss value : 726.981873 -- Test loss value : 657.570923\n",
      "LOSS at step 100: - Train loss value : 403.899994 -- Test loss value : 651.754517\n",
      "LOSS at step 100: - Train loss value : 15896.342773 -- Test loss value : 651.604187\n",
      "LOSS at step 120: - Train loss value : 1393.987183 -- Test loss value : 645.438904\n",
      "LOSS at step 120: - Train loss value : 949.618103 -- Test loss value : 645.268066\n",
      "LOSS at step 140: - Train loss value : 970.523132 -- Test loss value : 638.327087\n",
      "LOSS at step 140: - Train loss value : 16100.440430 -- Test loss value : 638.132935\n",
      "LOSS at step 160: - Train loss value : 1551.113037 -- Test loss value : 631.398499\n",
      "LOSS at step 160: - Train loss value : 699.718506 -- Test loss value : 631.234436\n",
      "LOSS at step 180: - Train loss value : 1686.866699 -- Test loss value : 624.196655\n",
      "LOSS at step 180: - Train loss value : 15148.818359 -- Test loss value : 623.989380\n",
      "LOSS at step 200: - Train loss value : 1352.406128 -- Test loss value : 616.676392\n",
      "LOSS at step 200: - Train loss value : 492.087280 -- Test loss value : 616.491455\n",
      "LOSS at step 220: - Train loss value : 14766.434570 -- Test loss value : 608.377441\n",
      "LOSS at step 220: - Train loss value : 912.425903 -- Test loss value : 608.147583\n",
      "LOSS at step 240: - Train loss value : 791.928345 -- Test loss value : 600.100952\n",
      "LOSS at step 240: - Train loss value : 1055.895630 -- Test loss value : 599.901672\n",
      "LOSS at step 260: - Train loss value : 809.256531 -- Test loss value : 591.912903\n",
      "LOSS at step 260: - Train loss value : 15660.193359 -- Test loss value : 591.679749\n",
      "LOSS at step 280: - Train loss value : 547.747986 -- Test loss value : 582.868469\n",
      "LOSS at step 280: - Train loss value : 15326.947266 -- Test loss value : 582.630432\n",
      "LOSS at step 300: - Train loss value : 407.969238 -- Test loss value : 573.752686\n",
      "LOSS at step 300: - Train loss value : 812.234253 -- Test loss value : 573.523071\n",
      "LOSS at step 320: - Train loss value : 1004.844910 -- Test loss value : 563.248596\n",
      "LOSS at step 320: - Train loss value : 521.796814 -- Test loss value : 562.950134\n",
      "LOSS at step 340: - Train loss value : 16096.734375 -- Test loss value : 552.580872\n",
      "LOSS at step 340: - Train loss value : 306.723450 -- Test loss value : 552.329834\n",
      "LOSS at step 360: - Train loss value : 803.941101 -- Test loss value : 542.716919\n",
      "LOSS at step 360: - Train loss value : 329.380890 -- Test loss value : 542.457703\n",
      "LOSS at step 380: - Train loss value : 826.543640 -- Test loss value : 531.479675\n",
      "LOSS at step 380: - Train loss value : 542.346924 -- Test loss value : 531.231140\n",
      "LOSS at step 400: - Train loss value : 984.003662 -- Test loss value : 520.692139\n",
      "LOSS at step 400: - Train loss value : 947.753906 -- Test loss value : 520.438904\n",
      "LOSS at step 420: - Train loss value : 1095.839355 -- Test loss value : 509.383087\n",
      "LOSS at step 420: - Train loss value : 593.721863 -- Test loss value : 509.074097\n",
      "LOSS at step 440: - Train loss value : 467.956329 -- Test loss value : 497.532990\n",
      "LOSS at step 440: - Train loss value : 1188.136230 -- Test loss value : 497.224823\n",
      "LOSS at step 460: - Train loss value : 867.859497 -- Test loss value : 485.058472\n",
      "LOSS at step 460: - Train loss value : 15402.466797 -- Test loss value : 484.738495\n",
      "LOSS at step 480: - Train loss value : 824.403564 -- Test loss value : 473.341827\n",
      "LOSS at step 480: - Train loss value : 14828.001953 -- Test loss value : 473.025726\n",
      "LOSS at step 500: - Train loss value : 405.865692 -- Test loss value : 462.460938\n",
      "LOSS at step 500: - Train loss value : 373.947296 -- Test loss value : 462.264648\n",
      "LOSS at step 520: - Train loss value : 598.576965 -- Test loss value : 453.480103\n",
      "LOSS at step 520: - Train loss value : 345.139099 -- Test loss value : 453.280365\n",
      "LOSS at step 540: - Train loss value : 341.801788 -- Test loss value : 444.139679\n",
      "LOSS at step 540: - Train loss value : 561.743225 -- Test loss value : 443.928162\n",
      "LOSS at step 560: - Train loss value : 734.259338 -- Test loss value : 435.536774\n",
      "LOSS at step 560: - Train loss value : 14847.682617 -- Test loss value : 435.327850\n",
      "LOSS at step 580: - Train loss value : 1149.209473 -- Test loss value : 427.951721\n",
      "LOSS at step 580: - Train loss value : 222.205154 -- Test loss value : 427.790314\n",
      "LOSS at step 600: - Train loss value : 1150.295166 -- Test loss value : 422.015167\n",
      "LOSS at step 600: - Train loss value : 14647.163086 -- Test loss value : 421.861298\n",
      "LOSS at step 620: - Train loss value : 15193.581055 -- Test loss value : 415.416565\n",
      "LOSS at step 620: - Train loss value : 562.553711 -- Test loss value : 415.226532\n",
      "LOSS at step 640: - Train loss value : 298.697906 -- Test loss value : 409.226685\n",
      "LOSS at step 640: - Train loss value : 1347.138184 -- Test loss value : 409.103455\n",
      "LOSS at step 660: - Train loss value : 660.444702 -- Test loss value : 404.564423\n",
      "LOSS at step 660: - Train loss value : 14574.115234 -- Test loss value : 404.446930\n",
      "LOSS at step 680: - Train loss value : 188.083771 -- Test loss value : 400.090515\n",
      "LOSS at step 680: - Train loss value : 186.795334 -- Test loss value : 400.011444\n",
      "LOSS at step 700: - Train loss value : 115.828316 -- Test loss value : 397.459442\n",
      "LOSS at step 700: - Train loss value : 14088.227539 -- Test loss value : 397.394897\n",
      "LOSS at step 720: - Train loss value : 999.520630 -- Test loss value : 394.563446\n",
      "LOSS at step 720: - Train loss value : 529.273254 -- Test loss value : 394.504181\n",
      "LOSS at step 740: - Train loss value : 386.523376 -- Test loss value : 392.549622\n",
      "LOSS at step 740: - Train loss value : 641.017212 -- Test loss value : 392.508850\n",
      "LOSS at step 760: - Train loss value : 543.743774 -- Test loss value : 391.001434\n",
      "LOSS at step 760: - Train loss value : 115.904099 -- Test loss value : 390.961029\n",
      "LOSS at step 780: - Train loss value : 229.374191 -- Test loss value : 390.055573\n",
      "LOSS at step 780: - Train loss value : 14558.748047 -- Test loss value : 390.024689\n",
      "LOSS at step 800: - Train loss value : 672.293213 -- Test loss value : 389.209717\n",
      "LOSS at step 800: - Train loss value : 480.477814 -- Test loss value : 389.200012\n",
      "LOSS at step 820: - Train loss value : 813.589600 -- Test loss value : 388.981964\n",
      "LOSS at step 820: - Train loss value : 348.569061 -- Test loss value : 388.980499\n",
      "LOSS at step 840: - Train loss value : 181.720200 -- Test loss value : 388.925415\n",
      "LOSS at step 840: - Train loss value : 963.445740 -- Test loss value : 388.927307\n",
      "LOSS at step 860: - Train loss value : 14722.035156 -- Test loss value : 389.096436\n",
      "LOSS at step 860: - Train loss value : 560.714661 -- Test loss value : 389.106903\n",
      "LOSS at step 880: - Train loss value : 311.585022 -- Test loss value : 389.434631\n",
      "LOSS at step 880: - Train loss value : 14696.380859 -- Test loss value : 389.447388\n",
      "LOSS at step 900: - Train loss value : 201.068817 -- Test loss value : 389.816589\n",
      "LOSS at step 900: - Train loss value : 1009.939209 -- Test loss value : 389.825653\n",
      "LOSS at step 920: - Train loss value : 721.460632 -- Test loss value : 389.996490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 920: - Train loss value : 409.040497 -- Test loss value : 390.003632\n",
      "LOSS at step 940: - Train loss value : 174.143356 -- Test loss value : 390.426727\n",
      "LOSS at step 940: - Train loss value : 522.111145 -- Test loss value : 390.438293\n",
      "LOSS at step 960: - Train loss value : 767.118713 -- Test loss value : 390.904510\n",
      "LOSS at step 960: - Train loss value : 781.696472 -- Test loss value : 390.916260\n",
      "LOSS at step 980: - Train loss value : 104.071175 -- Test loss value : 391.443909\n",
      "LOSS at step 980: - Train loss value : 14498.909180 -- Test loss value : 391.491150\n",
      "LOSS at step 1000: - Train loss value : 488.078125 -- Test loss value : 392.645416\n",
      "LOSS at step 1000: - Train loss value : 716.452148 -- Test loss value : 392.656982\n",
      "LOSS at step 1020: - Train loss value : 705.306091 -- Test loss value : 393.054718\n",
      "LOSS at step 1020: - Train loss value : 813.453369 -- Test loss value : 393.089294\n",
      "LOSS at step 1040: - Train loss value : 301.585999 -- Test loss value : 394.042847\n",
      "LOSS at step 1040: - Train loss value : 1143.375488 -- Test loss value : 394.072479\n",
      "LOSS at step 1060: - Train loss value : 325.170288 -- Test loss value : 395.628418\n",
      "LOSS at step 1060: - Train loss value : 292.349762 -- Test loss value : 395.641357\n",
      "LOSS at step 1080: - Train loss value : 493.028259 -- Test loss value : 396.414032\n",
      "LOSS at step 1080: - Train loss value : 209.102890 -- Test loss value : 396.448914\n",
      "LOSS at step 1100: - Train loss value : 14936.716797 -- Test loss value : 396.645081\n",
      "LOSS at step 1100: - Train loss value : 518.764221 -- Test loss value : 396.673218\n",
      "LOSS at step 1120: - Train loss value : 148.854309 -- Test loss value : 397.618073\n",
      "LOSS at step 1120: - Train loss value : 601.213257 -- Test loss value : 397.630615\n",
      "LOSS at step 1140: - Train loss value : 303.935028 -- Test loss value : 397.383209\n",
      "LOSS at step 1140: - Train loss value : 511.846771 -- Test loss value : 397.368713\n",
      "LOSS at step 1160: - Train loss value : 363.112579 -- Test loss value : 398.297241\n",
      "LOSS at step 1160: - Train loss value : 538.900269 -- Test loss value : 398.347839\n",
      "LOSS at step 1180: - Train loss value : 326.207733 -- Test loss value : 399.210144\n",
      "LOSS at step 1180: - Train loss value : 311.754974 -- Test loss value : 399.183594\n",
      "LOSS at step 1200: - Train loss value : 13778.722656 -- Test loss value : 400.237732\n",
      "LOSS at step 1200: - Train loss value : 758.139587 -- Test loss value : 400.267426\n",
      "LOSS at step 1220: - Train loss value : 339.401550 -- Test loss value : 400.842682\n",
      "LOSS at step 1220: - Train loss value : 720.373291 -- Test loss value : 400.896362\n",
      "LOSS at step 1240: - Train loss value : 14456.291016 -- Test loss value : 401.385834\n",
      "LOSS at step 1240: - Train loss value : 1824.105469 -- Test loss value : 401.418365\n",
      "LOSS at step 1260: - Train loss value : 638.510986 -- Test loss value : 403.055420\n",
      "LOSS at step 1260: - Train loss value : 430.066223 -- Test loss value : 403.091858\n",
      "LOSS at step 1280: - Train loss value : 299.675781 -- Test loss value : 403.773071\n",
      "LOSS at step 1280: - Train loss value : 221.698303 -- Test loss value : 403.767914\n",
      "LOSS at step 1300: - Train loss value : 670.500549 -- Test loss value : 403.986267\n",
      "LOSS at step 1300: - Train loss value : 1289.872559 -- Test loss value : 404.007080\n",
      "LOSS at step 1320: - Train loss value : 314.128937 -- Test loss value : 403.493500\n",
      "LOSS at step 1320: - Train loss value : 956.227722 -- Test loss value : 403.447113\n",
      "LOSS at step 1340: - Train loss value : 14460.850586 -- Test loss value : 402.970642\n",
      "LOSS at step 1340: - Train loss value : 560.801636 -- Test loss value : 402.982391\n",
      "LOSS at step 1360: - Train loss value : 399.988403 -- Test loss value : 402.517273\n",
      "LOSS at step 1360: - Train loss value : 113.211250 -- Test loss value : 402.470856\n",
      "LOSS at step 1380: - Train loss value : 13949.626953 -- Test loss value : 403.148468\n",
      "LOSS at step 1380: - Train loss value : 545.627991 -- Test loss value : 403.154846\n",
      "LOSS at step 1400: - Train loss value : 393.827606 -- Test loss value : 404.497711\n",
      "LOSS at step 1400: - Train loss value : 536.449036 -- Test loss value : 404.461212\n",
      "LOSS at step 1420: - Train loss value : 15268.262695 -- Test loss value : 403.412903\n",
      "LOSS at step 1420: - Train loss value : 199.155472 -- Test loss value : 403.386719\n",
      "LOSS at step 1440: - Train loss value : 500.756134 -- Test loss value : 403.352356\n",
      "LOSS at step 1440: - Train loss value : 944.959290 -- Test loss value : 403.366028\n",
      "LOSS at step 1460: - Train loss value : 584.735229 -- Test loss value : 403.310120\n",
      "LOSS at step 1460: - Train loss value : 248.068466 -- Test loss value : 403.258301\n",
      "LOSS at step 1480: - Train loss value : 284.141022 -- Test loss value : 402.017639\n",
      "LOSS at step 1480: - Train loss value : 690.881836 -- Test loss value : 401.996063\n",
      "LOSS at step 1500: - Train loss value : 259.927399 -- Test loss value : 402.028320\n",
      "LOSS at step 1500: - Train loss value : 609.506165 -- Test loss value : 402.066406\n",
      "LOSS at step 1520: - Train loss value : 13877.642578 -- Test loss value : 402.518372\n",
      "LOSS at step 1520: - Train loss value : 309.250122 -- Test loss value : 402.596680\n",
      "LOSS at step 1540: - Train loss value : 379.480347 -- Test loss value : 402.810303\n",
      "LOSS at step 1540: - Train loss value : 443.016083 -- Test loss value : 402.799377\n",
      "LOSS at step 1560: - Train loss value : 549.692383 -- Test loss value : 403.096619\n",
      "LOSS at step 1560: - Train loss value : 462.747711 -- Test loss value : 403.066528\n",
      "LOSS at step 1580: - Train loss value : 13781.744141 -- Test loss value : 403.575775\n",
      "LOSS at step 1580: - Train loss value : 239.247391 -- Test loss value : 403.559570\n",
      "LOSS at step 1600: - Train loss value : 752.025146 -- Test loss value : 403.826111\n",
      "LOSS at step 1600: - Train loss value : 335.721497 -- Test loss value : 403.830597\n",
      "LOSS at step 1620: - Train loss value : 775.281372 -- Test loss value : 405.644165\n",
      "LOSS at step 1620: - Train loss value : 726.907104 -- Test loss value : 405.649078\n",
      "LOSS at step 1640: - Train loss value : 407.836304 -- Test loss value : 405.478729\n",
      "LOSS at step 1640: - Train loss value : 903.971252 -- Test loss value : 405.514587\n",
      "LOSS at step 1660: - Train loss value : 14111.675781 -- Test loss value : 405.365662\n",
      "LOSS at step 1660: - Train loss value : 957.292969 -- Test loss value : 405.435608\n",
      "LOSS at step 1680: - Train loss value : 639.460632 -- Test loss value : 407.075287\n",
      "LOSS at step 1680: - Train loss value : 696.346924 -- Test loss value : 407.070007\n",
      "LOSS at step 1700: - Train loss value : 424.033661 -- Test loss value : 405.324188\n",
      "LOSS at step 1700: - Train loss value : 13743.218750 -- Test loss value : 405.310089\n",
      "LOSS at step 1720: - Train loss value : 190.367920 -- Test loss value : 403.318298\n",
      "LOSS at step 1720: - Train loss value : 15754.791016 -- Test loss value : 403.285461\n",
      "LOSS at step 1740: - Train loss value : 654.917358 -- Test loss value : 403.375397\n",
      "LOSS at step 1740: - Train loss value : 315.914062 -- Test loss value : 403.406158\n",
      "LOSS at step 1760: - Train loss value : 14343.436523 -- Test loss value : 403.278870\n",
      "LOSS at step 1760: - Train loss value : 437.923462 -- Test loss value : 403.257843\n",
      "LOSS at step 1780: - Train loss value : 532.211975 -- Test loss value : 402.505249\n",
      "LOSS at step 1780: - Train loss value : 634.255310 -- Test loss value : 402.453339\n",
      "LOSS at step 1800: - Train loss value : 515.670959 -- Test loss value : 402.313263\n",
      "LOSS at step 1800: - Train loss value : 235.870514 -- Test loss value : 402.256836\n",
      "LOSS at step 1820: - Train loss value : 361.975250 -- Test loss value : 402.471436\n",
      "LOSS at step 1820: - Train loss value : 14948.588867 -- Test loss value : 402.500732\n",
      "LOSS at step 1840: - Train loss value : 1128.072266 -- Test loss value : 403.281250\n",
      "LOSS at step 1840: - Train loss value : 13961.913086 -- Test loss value : 403.309570\n",
      "LOSS at step 1860: - Train loss value : 429.118469 -- Test loss value : 404.255035\n",
      "LOSS at step 1860: - Train loss value : 905.475830 -- Test loss value : 404.281616\n",
      "LOSS at step 1880: - Train loss value : 406.767700 -- Test loss value : 404.045624\n",
      "LOSS at step 1880: - Train loss value : 529.870544 -- Test loss value : 404.041931\n",
      "LOSS at step 1900: - Train loss value : 13693.232422 -- Test loss value : 403.650726\n",
      "LOSS at step 1900: - Train loss value : 353.547333 -- Test loss value : 403.646606\n",
      "LOSS at step 1920: - Train loss value : 14464.350586 -- Test loss value : 404.335388\n",
      "LOSS at step 1920: - Train loss value : 746.136719 -- Test loss value : 404.360413\n",
      "LOSS at step 1940: - Train loss value : 194.556671 -- Test loss value : 405.014313\n",
      "LOSS at step 1940: - Train loss value : 14657.855469 -- Test loss value : 405.033325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 1960: - Train loss value : 364.991486 -- Test loss value : 404.249359\n",
      "LOSS at step 1960: - Train loss value : 14237.113281 -- Test loss value : 404.292755\n",
      "LOSS at step 1980: - Train loss value : 680.628052 -- Test loss value : 404.014862\n",
      "LOSS at step 1980: - Train loss value : 222.194397 -- Test loss value : 403.987823\n",
      "LOSS at step 2000: - Train loss value : 642.269104 -- Test loss value : 404.099091\n",
      "LOSS at step 2000: - Train loss value : 421.417786 -- Test loss value : 404.096863\n",
      "LOSS at step 2020: - Train loss value : 294.027710 -- Test loss value : 404.549377\n",
      "LOSS at step 2020: - Train loss value : 579.331238 -- Test loss value : 404.566376\n",
      "LOSS at step 2040: - Train loss value : 212.068665 -- Test loss value : 404.519806\n",
      "LOSS at step 2040: - Train loss value : 196.527740 -- Test loss value : 404.477509\n",
      "LOSS at step 2060: - Train loss value : 329.053375 -- Test loss value : 403.771790\n",
      "LOSS at step 2060: - Train loss value : 445.371948 -- Test loss value : 403.722382\n",
      "LOSS at step 2080: - Train loss value : 358.480438 -- Test loss value : 402.625275\n",
      "LOSS at step 2080: - Train loss value : 598.762634 -- Test loss value : 402.622986\n",
      "LOSS at step 2100: - Train loss value : 300.271454 -- Test loss value : 402.767059\n",
      "LOSS at step 2100: - Train loss value : 781.033569 -- Test loss value : 402.774475\n",
      "LOSS at step 2120: - Train loss value : 446.269135 -- Test loss value : 403.346405\n",
      "LOSS at step 2120: - Train loss value : 536.353516 -- Test loss value : 403.336792\n",
      "LOSS at step 2140: - Train loss value : 679.763794 -- Test loss value : 403.147797\n",
      "LOSS at step 2140: - Train loss value : 14382.243164 -- Test loss value : 403.148071\n",
      "LOSS at step 2160: - Train loss value : 570.742188 -- Test loss value : 403.758698\n",
      "LOSS at step 2160: - Train loss value : 14020.661133 -- Test loss value : 403.782043\n",
      "LOSS at step 2180: - Train loss value : 318.095123 -- Test loss value : 402.608795\n",
      "LOSS at step 2180: - Train loss value : 1123.542480 -- Test loss value : 402.551453\n",
      "LOSS at step 2200: - Train loss value : 13862.075195 -- Test loss value : 403.481049\n",
      "LOSS at step 2200: - Train loss value : 404.396606 -- Test loss value : 403.565247\n",
      "LOSS at step 2220: - Train loss value : 236.887726 -- Test loss value : 406.091675\n",
      "LOSS at step 2220: - Train loss value : 15109.763672 -- Test loss value : 406.154846\n",
      "LOSS at step 2240: - Train loss value : 405.519653 -- Test loss value : 406.274841\n",
      "LOSS at step 2240: - Train loss value : 14628.840820 -- Test loss value : 406.283936\n",
      "LOSS at step 2260: - Train loss value : 205.725067 -- Test loss value : 406.728394\n",
      "LOSS at step 2260: - Train loss value : 254.659409 -- Test loss value : 406.658142\n",
      "LOSS at step 2280: - Train loss value : 811.133789 -- Test loss value : 405.417084\n",
      "LOSS at step 2280: - Train loss value : 343.174194 -- Test loss value : 405.409271\n",
      "LOSS at step 2300: - Train loss value : 597.157959 -- Test loss value : 404.873291\n",
      "LOSS at step 2300: - Train loss value : 746.800781 -- Test loss value : 404.859558\n",
      "LOSS at step 2320: - Train loss value : 753.953125 -- Test loss value : 404.687408\n",
      "LOSS at step 2320: - Train loss value : 13808.823242 -- Test loss value : 404.687500\n",
      "LOSS at step 2340: - Train loss value : 476.148834 -- Test loss value : 405.970062\n",
      "LOSS at step 2340: - Train loss value : 287.211884 -- Test loss value : 405.966187\n",
      "LOSS at step 2360: - Train loss value : 335.373688 -- Test loss value : 407.022400\n",
      "LOSS at step 2360: - Train loss value : 13772.503906 -- Test loss value : 407.048279\n",
      "LOSS at step 2380: - Train loss value : 1075.911499 -- Test loss value : 407.654846\n",
      "LOSS at step 2380: - Train loss value : 896.203430 -- Test loss value : 407.592499\n",
      "LOSS at step 2400: - Train loss value : 521.296143 -- Test loss value : 407.249512\n",
      "LOSS at step 2400: - Train loss value : 15466.244141 -- Test loss value : 407.209869\n",
      "LOSS at step 2420: - Train loss value : 13851.237305 -- Test loss value : 405.328339\n",
      "LOSS at step 2420: - Train loss value : 653.918579 -- Test loss value : 405.350891\n",
      "LOSS at step 2440: - Train loss value : 338.320923 -- Test loss value : 403.664185\n",
      "LOSS at step 2440: - Train loss value : 350.002167 -- Test loss value : 403.650146\n",
      "LOSS at step 2460: - Train loss value : 1153.784790 -- Test loss value : 402.391479\n",
      "LOSS at step 2460: - Train loss value : 359.394928 -- Test loss value : 402.313141\n",
      "LOSS at step 2480: - Train loss value : 516.638550 -- Test loss value : 401.839294\n",
      "LOSS at step 2480: - Train loss value : 581.641296 -- Test loss value : 401.881775\n",
      "LOSS at step 2500: - Train loss value : 608.182678 -- Test loss value : 403.426910\n",
      "LOSS at step 2500: - Train loss value : 636.695068 -- Test loss value : 403.482971\n",
      "LOSS at step 2520: - Train loss value : 631.087036 -- Test loss value : 404.829346\n",
      "LOSS at step 2520: - Train loss value : 356.562622 -- Test loss value : 404.883118\n",
      "LOSS at step 2540: - Train loss value : 823.504578 -- Test loss value : 405.571198\n",
      "LOSS at step 2540: - Train loss value : 14286.105469 -- Test loss value : 405.586121\n",
      "LOSS at step 2560: - Train loss value : 605.381409 -- Test loss value : 404.489410\n",
      "LOSS at step 2560: - Train loss value : 480.052856 -- Test loss value : 404.421021\n",
      "LOSS at step 2580: - Train loss value : 14411.984375 -- Test loss value : 404.281891\n",
      "LOSS at step 2580: - Train loss value : 504.339264 -- Test loss value : 404.319427\n",
      "LOSS at step 2600: - Train loss value : 545.613464 -- Test loss value : 404.715729\n",
      "LOSS at step 2600: - Train loss value : 475.747375 -- Test loss value : 404.655701\n",
      "LOSS at step 2620: - Train loss value : 14330.362305 -- Test loss value : 403.098328\n",
      "LOSS at step 2620: - Train loss value : 736.202087 -- Test loss value : 403.060425\n",
      "LOSS at step 2640: - Train loss value : 939.398376 -- Test loss value : 402.678833\n",
      "LOSS at step 2640: - Train loss value : 291.248016 -- Test loss value : 402.648315\n",
      "LOSS at step 2660: - Train loss value : 13912.347656 -- Test loss value : 401.642700\n",
      "LOSS at step 2660: - Train loss value : 363.070465 -- Test loss value : 401.652802\n",
      "LOSS at step 2680: - Train loss value : 156.014450 -- Test loss value : 400.796783\n",
      "LOSS at step 2680: - Train loss value : 227.420731 -- Test loss value : 400.773926\n",
      "LOSS at step 2700: - Train loss value : 773.025024 -- Test loss value : 400.691437\n",
      "LOSS at step 2700: - Train loss value : 251.645706 -- Test loss value : 400.692566\n",
      "LOSS at step 2720: - Train loss value : 229.568878 -- Test loss value : 401.268005\n",
      "LOSS at step 2720: - Train loss value : 549.991882 -- Test loss value : 401.337250\n",
      "LOSS at step 2740: - Train loss value : 731.984375 -- Test loss value : 401.295990\n",
      "LOSS at step 2740: - Train loss value : 773.203369 -- Test loss value : 401.243500\n",
      "LOSS at step 2760: - Train loss value : 211.415146 -- Test loss value : 402.303650\n",
      "LOSS at step 2760: - Train loss value : 811.053833 -- Test loss value : 402.344910\n",
      "LOSS at step 2780: - Train loss value : 1161.576782 -- Test loss value : 401.881165\n",
      "LOSS at step 2780: - Train loss value : 171.345245 -- Test loss value : 401.864532\n",
      "LOSS at step 2800: - Train loss value : 14097.900391 -- Test loss value : 402.515747\n",
      "LOSS at step 2800: - Train loss value : 220.006317 -- Test loss value : 402.571167\n",
      "LOSS at step 2820: - Train loss value : 400.272278 -- Test loss value : 403.585571\n",
      "LOSS at step 2820: - Train loss value : 314.586792 -- Test loss value : 403.593475\n",
      "LOSS at step 2840: - Train loss value : 363.915161 -- Test loss value : 403.010071\n",
      "LOSS at step 2840: - Train loss value : 678.565002 -- Test loss value : 402.963501\n",
      "LOSS at step 2860: - Train loss value : 14872.245117 -- Test loss value : 403.088593\n",
      "LOSS at step 2860: - Train loss value : 174.200287 -- Test loss value : 403.120728\n",
      "LOSS at step 2880: - Train loss value : 709.851624 -- Test loss value : 402.210083\n",
      "LOSS at step 2880: - Train loss value : 1078.784058 -- Test loss value : 402.243927\n",
      "LOSS at step 2900: - Train loss value : 1179.396484 -- Test loss value : 402.244202\n",
      "LOSS at step 2900: - Train loss value : 419.794006 -- Test loss value : 402.211487\n",
      "LOSS at step 2920: - Train loss value : 369.480927 -- Test loss value : 402.551849\n",
      "LOSS at step 2920: - Train loss value : 14365.156250 -- Test loss value : 402.563904\n",
      "LOSS at step 2940: - Train loss value : 296.605469 -- Test loss value : 402.814056\n",
      "LOSS at step 2940: - Train loss value : 1150.661743 -- Test loss value : 402.750397\n",
      "LOSS at step 2960: - Train loss value : 739.686584 -- Test loss value : 400.986755\n",
      "LOSS at step 2960: - Train loss value : 321.387573 -- Test loss value : 400.947876\n",
      "LOSS at step 2980: - Train loss value : 272.636047 -- Test loss value : 401.069550\n",
      "LOSS at step 2980: - Train loss value : 267.871887 -- Test loss value : 401.040894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 3000: - Train loss value : 714.054382 -- Test loss value : 401.293854\n",
      "LOSS at step 3000: - Train loss value : 185.861679 -- Test loss value : 401.294403\n",
      "LOSS at step 3020: - Train loss value : 233.895218 -- Test loss value : 402.796997\n",
      "LOSS at step 3020: - Train loss value : 157.075714 -- Test loss value : 402.810150\n",
      "LOSS at step 3040: - Train loss value : 171.287140 -- Test loss value : 402.286865\n",
      "LOSS at step 3040: - Train loss value : 614.005920 -- Test loss value : 402.272980\n",
      "LOSS at step 3060: - Train loss value : 502.615509 -- Test loss value : 402.584045\n",
      "LOSS at step 3060: - Train loss value : 760.342896 -- Test loss value : 402.607635\n",
      "LOSS at step 3080: - Train loss value : 280.301483 -- Test loss value : 403.737976\n",
      "LOSS at step 3080: - Train loss value : 226.556747 -- Test loss value : 403.766602\n",
      "LOSS at step 3100: - Train loss value : 242.095001 -- Test loss value : 403.401855\n",
      "LOSS at step 3100: - Train loss value : 714.443298 -- Test loss value : 403.340515\n",
      "LOSS at step 3120: - Train loss value : 971.886902 -- Test loss value : 401.988495\n",
      "LOSS at step 3120: - Train loss value : 329.477631 -- Test loss value : 401.952362\n",
      "LOSS at step 3140: - Train loss value : 448.277740 -- Test loss value : 400.830414\n",
      "LOSS at step 3140: - Train loss value : 705.125671 -- Test loss value : 400.779175\n",
      "LOSS at step 3160: - Train loss value : 593.540833 -- Test loss value : 400.299835\n",
      "LOSS at step 3160: - Train loss value : 775.082825 -- Test loss value : 400.320953\n",
      "LOSS at step 3180: - Train loss value : 276.312347 -- Test loss value : 400.776031\n",
      "LOSS at step 3180: - Train loss value : 408.296783 -- Test loss value : 400.783417\n",
      "LOSS at step 3200: - Train loss value : 351.292969 -- Test loss value : 400.247070\n",
      "LOSS at step 3200: - Train loss value : 14492.916016 -- Test loss value : 400.267456\n",
      "LOSS at step 3220: - Train loss value : 278.875916 -- Test loss value : 400.821533\n",
      "LOSS at step 3220: - Train loss value : 653.419250 -- Test loss value : 400.824799\n",
      "LOSS at step 3240: - Train loss value : 979.819275 -- Test loss value : 403.962189\n",
      "LOSS at step 3240: - Train loss value : 220.397491 -- Test loss value : 404.014465\n",
      "LOSS at step 3260: - Train loss value : 707.105713 -- Test loss value : 403.807709\n",
      "LOSS at step 3260: - Train loss value : 14278.364258 -- Test loss value : 403.807831\n",
      "LOSS at step 3280: - Train loss value : 950.716431 -- Test loss value : 402.968292\n",
      "LOSS at step 3280: - Train loss value : 13786.483398 -- Test loss value : 402.957794\n",
      "LOSS at step 3300: - Train loss value : 534.428650 -- Test loss value : 402.453613\n",
      "LOSS at step 3300: - Train loss value : 365.559235 -- Test loss value : 402.444458\n",
      "LOSS at step 3320: - Train loss value : 552.663330 -- Test loss value : 402.171753\n",
      "LOSS at step 3320: - Train loss value : 730.208069 -- Test loss value : 402.152313\n",
      "LOSS at step 3340: - Train loss value : 331.010468 -- Test loss value : 402.629395\n",
      "LOSS at step 3340: - Train loss value : 457.646179 -- Test loss value : 402.657196\n",
      "LOSS at step 3360: - Train loss value : 371.643646 -- Test loss value : 403.668060\n",
      "LOSS at step 3360: - Train loss value : 13908.003906 -- Test loss value : 403.742950\n",
      "LOSS at step 3380: - Train loss value : 614.576904 -- Test loss value : 404.359650\n",
      "LOSS at step 3380: - Train loss value : 278.589325 -- Test loss value : 404.359833\n",
      "LOSS at step 3400: - Train loss value : 577.116394 -- Test loss value : 406.345673\n",
      "LOSS at step 3400: - Train loss value : 428.293610 -- Test loss value : 406.415283\n",
      "LOSS at step 3420: - Train loss value : 14199.239258 -- Test loss value : 406.771729\n",
      "LOSS at step 3420: - Train loss value : 552.175781 -- Test loss value : 406.808289\n",
      "LOSS at step 3440: - Train loss value : 591.003113 -- Test loss value : 406.548950\n",
      "LOSS at step 3440: - Train loss value : 373.066223 -- Test loss value : 406.529449\n",
      "LOSS at step 3460: - Train loss value : 346.937561 -- Test loss value : 406.444733\n",
      "LOSS at step 3460: - Train loss value : 503.023224 -- Test loss value : 406.423492\n",
      "LOSS at step 3480: - Train loss value : 579.060913 -- Test loss value : 407.283997\n",
      "LOSS at step 3480: - Train loss value : 13701.031250 -- Test loss value : 407.353577\n"
     ]
    }
   ],
   "source": [
    "#ssh sydney\n",
    "#setPython3 \n",
    "\n",
    "import numpy \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import arff\n",
    "data = arff.loadarff('scpf.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df.dropna(inplace=True)\n",
    "covariables = df.iloc[:,0:23].values\n",
    "response = df.iloc[:,23:26].values\n",
    "positions = np.arange(143)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "covariables_train, covariables_test, response_train, response_test,positions_train,positions_test = train_test_split(covariables, response,positions, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "##APPRENTISSAGE\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = numpy.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = numpy.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = numpy.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "#import tensorflow as tf \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "# Import data\n",
    "# Tensorflow is finicky about shapes, so resize\n",
    "\n",
    "\n",
    "X_datatrain, X_datatest, targets_train, targets_test = covariables_train, covariables_test, response_train, response_test\n",
    "\n",
    "#les dimensions de X for training\n",
    "NbrLignes_train = X_datatrain.shape[0]  ###\n",
    "NbrColonnes_train= X_datatrain.shape[1] ###\n",
    "\n",
    "#les dimensions de X for testing\n",
    "NbrLignes_test = X_datatest.shape[0]  ###\n",
    "NbrColonnes_test = X_datatest.shape[1] ###\n",
    "#Y_data = targets \n",
    "\n",
    "Y_datatrain = tf.reshape(targets_train, [NbrLignes_train,targets_train.shape[1]])\n",
    "Y_datatest = tf.reshape(targets_test, [NbrLignes_test,targets_train.shape[1]])\n",
    "\n",
    "#build the model\n",
    "\n",
    "# input X: batch_size x NbrColonnes, the first dimension (None) will index the data in the mini-batch\n",
    "Xfill = tf.placeholder(tf.float32, shape= [None, NbrColonnes_train])\n",
    "# correct answers will go here\n",
    "Yfill = tf.placeholder(tf.float32, shape=[None, targets_train.shape[1]]) \n",
    "#Let's add some layers\n",
    "\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "L = 10 #200\n",
    "M = 3 #160\n",
    "#N = 100\n",
    "#O = 30\n",
    "\n",
    "# Weights initialised with small random values between -0.2 and +0.2\n",
    "# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "W1 = tf.Variable(tf.truncated_normal([NbrColonnes_train, L], stddev=0.1))  # \n",
    "B1 = tf.Variable(tf.ones([L])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([M])/10)\n",
    "#W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "#B3 = tf.Variable(tf.ones([N])/10)\n",
    "#W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "#B4 = tf.Variable(tf.ones([O])/10)\n",
    "#W5 = tf.Variable(tf.truncated_normal([O, 6], stddev=0.1))\n",
    "#B5 = tf.Variable(tf.zeros([6]))\n",
    "\n",
    "# The model, with dropout at each layer\n",
    "\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(Xfill, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "#Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "#Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "#Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "#Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "#Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "#Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "#Y_ = tf.matmul(Y4d, W5) + B5\n",
    "\n",
    "\n",
    "Y_ = tf.matmul(Y1d, W2) + B2\n",
    "\n",
    "cross_entropy = tf.losses.mean_squared_error(Yfill, Y_)\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "#Modification de l'accuracy \n",
    "\n",
    "#calcul de l'accuracy, une nouvelle maniÃ¨re: :  \n",
    "\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy) \n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 100.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# Gradient descent loop for 500 steps\n",
    "for i in range(3500):\n",
    " # Select random minibatch\n",
    "    j = 1\n",
    "    ##### shuffling our data::: really important\n",
    "    X_datasf, Y_datasf = shuffle_in_unison(X_datatrain, Y_datatrain.eval())\n",
    "    \n",
    "    ##### defining a decreasing learning rate: \n",
    "    # learning rate decay\n",
    "    learning_rate = 0.0001001329829992624\n",
    "    #learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    \n",
    "    ##### \n",
    "    for start, end in zip(range(0, NbrLignes_train, 52), range(20, NbrLignes_train, 52)):\n",
    "\n",
    "             X_batch,Y_batch = X_datasf[start:end,],Y_datasf[start:end,]\n",
    "             j +=1\n",
    "             # Do gradient descent step\n",
    "             _, loss_val_train = sess.run([train_step, cross_entropy], feed_dict={Xfill: X_batch, Yfill: Y_batch, pkeep: 0.75, lr: learning_rate})\n",
    "             if i %20==0:    \n",
    "                 \n",
    "             \n",
    "                #if j%20==0:\n",
    "                 loss_val_test = sess.run(cross_entropy, feed_dict={Xfill: X_datatest, Yfill: Y_datatest.eval(), pkeep: 1.0, lr:  learning_rate})\n",
    "                 train_loss.append(loss_val_train)\n",
    "                 \n",
    "                 test_loss.append(loss_val_test)\n",
    "                 print('LOSS at step %s: - Train loss value : %f -- Test loss value : %f' % (i, loss_val_train,loss_val_test))\n",
    "#print(test_loss)\n",
    "#print(test_loss)\n",
    "#LOSS at step 420: - Train loss value : 22.385496 -- Test loss value : 9.721767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us compute the aRRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001001329829992624\n",
    "\n",
    "Y_testpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatest, tf.float32).eval(), Yfill: tf.cast(Y_datatest, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n",
    "#numpy.savetxt('Y_testpred_with_stock_%s_and_ai_%s_modelcut_%s_vartimeframe_%s.txt'%(stock, ai, cut, vartimeframe), Y_testpred, fmt='%f')\n",
    "Y_trainpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatrain, tf.float32).eval(), Yfill: tf.cast(Y_datatrain, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.32908   ,  2.0517406 ,  0.37311298],\n",
       "       [50.756817  ,  3.0611475 ,  0.49672794],\n",
       "       [46.503292  ,  2.8344946 ,  0.46529433],\n",
       "       [43.17923   ,  2.6629682 ,  0.4450575 ],\n",
       "       [44.60006   ,  2.7436042 ,  0.45936495],\n",
       "       [32.598854  ,  2.1172276 ,  0.38081616],\n",
       "       [29.939213  ,  1.9880545 ,  0.37086087],\n",
       "       [35.061325  ,  2.2522986 ,  0.40199453],\n",
       "       [34.12699   ,  2.1958644 ,  0.3899514 ],\n",
       "       [43.69701   ,  2.6971931 ,  0.45401222],\n",
       "       [38.10405   ,  2.4011838 ,  0.41423917],\n",
       "       [27.238756  ,  1.848175  ,  0.35400942],\n",
       "       [35.196274  ,  2.2512095 ,  0.39659166],\n",
       "       [36.5485    ,  2.320959  ,  0.40480283],\n",
       "       [34.335625  ,  2.2138534 ,  0.39680508],\n",
       "       [34.707607  ,  2.2332668 ,  0.39923874],\n",
       "       [34.101562  ,  2.2021065 ,  0.39563605],\n",
       "       [33.747128  ,  2.1765046 ,  0.38782564],\n",
       "       [29.20662   ,  1.9496484 ,  0.3659347 ],\n",
       "       [31.26982   ,  2.0485868 ,  0.37267816],\n",
       "       [36.34099   ,  2.3102508 ,  0.40353912],\n",
       "       [42.190315  ,  2.6195104 ,  0.44488984],\n",
       "       [34.513565  ,  2.223488  ,  0.39823836],\n",
       "       [37.116913  ,  2.357958  ,  0.41419065],\n",
       "       [37.18519   ,  2.3607006 ,  0.41400295],\n",
       "       [46.99082   ,  2.8665724 ,  0.47361204],\n",
       "       [33.078697  ,  2.141986  ,  0.3837356 ],\n",
       "       [42.571457  ,  2.6316156 ,  0.4413647 ],\n",
       "       [32.466038  ,  2.1176243 ,  0.385612  ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_testpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.080463\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 16.365589\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.142866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aRRMSE_list = []\n",
    "\n",
    "for i in range(Y_testpred.shape[1]) :\n",
    "    \n",
    "    Errors_test =  Y_testpred[:,i] - response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    ytrain_mean_i = np.tile(Y_trainpred[:,i].mean(axis = 0), (covariables_test.shape[0], 1))\n",
    "\n",
    "    Errors_relative = ytrain_mean_i - response_test[:,i]\n",
    "\n",
    "    Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%Final_SPNNR_aRRMSE)\n",
    "\n",
    "    aRRMSE_list.append(Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.529639547931865"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking aRRMSE on PCA or inverted PCA. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if old way calculation is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final_Partition aRRMSE is : 82.647642\n"
     ]
    }
   ],
   "source": [
    "Errors_test =  Y_testpred - response_test\n",
    "\n",
    "\n",
    "print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "ytrain_mean = np.tile(Y_trainpred.mean(axis = 0), (response_test.shape[0], 1))\n",
    "\n",
    "Errors_relative = ytrain_mean - response_test\n",
    "\n",
    "Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "print(\"Final_Partition aRRMSE is : %f\"%Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The answer is yes ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking True aRRMSE of S-SPNNR on Slump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_covariables_train, spnnr_covariables_test, spnnr_response_train, spnnr_response_test,spnnr_positions_train,spnnr_positions_test = train_test_split(covariables, response,positions, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_Y_testpred =np.loadtxt('All_Y_test_predictions.out', delimiter=',' )\n",
    "spnnr_Y_trainpred =np.loadtxt('All_Y_train_predictions.out', delimiter=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 9.212108\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 17.784301\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.862991\n"
     ]
    }
   ],
   "source": [
    "spnnr_aRRMSE_list = []\n",
    "\n",
    "for i in range(spnnr_Y_testpred.shape[1]) :\n",
    "    \n",
    "    spnnr_Errors_test =  spnnr_Y_testpred[:,i] - spnnr_response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    spnnr_ytrain_mean_i = np.tile(spnnr_Y_trainpred[:,i].mean(axis = 0), (spnnr_covariables_test.shape[0], 1))\n",
    "\n",
    "    spnnr_Errors_relative = spnnr_ytrain_mean_i - spnnr_response_test[:,i]\n",
    "\n",
    "    spnnr_Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(spnnr_Errors_test)**2)/(LA.norm(spnnr_Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%spnnr_Final_SPNNR_aRRMSE)\n",
    "\n",
    "    spnnr_aRRMSE_list.append(spnnr_Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.286466834678842"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spnnr_aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
