{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\pottsmodels\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From <ipython-input-2-5731ee7358f7>:92: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "LOSS at step 0: - Train loss value : 1196.576050 -- Test loss value : 1170.264771\n",
      "LOSS at step 0: - Train loss value : 906.296631 -- Test loss value : 1165.759399\n",
      "LOSS at step 20: - Train loss value : 875.503906 -- Test loss value : 996.524536\n",
      "LOSS at step 20: - Train loss value : 867.867432 -- Test loss value : 992.395142\n",
      "LOSS at step 40: - Train loss value : 1038.165405 -- Test loss value : 840.014893\n",
      "LOSS at step 40: - Train loss value : 691.284912 -- Test loss value : 836.389771\n",
      "LOSS at step 60: - Train loss value : 797.314026 -- Test loss value : 704.669983\n",
      "LOSS at step 60: - Train loss value : 632.049927 -- Test loss value : 701.495300\n",
      "LOSS at step 80: - Train loss value : 735.226685 -- Test loss value : 583.577271\n",
      "LOSS at step 80: - Train loss value : 561.468567 -- Test loss value : 580.802307\n",
      "LOSS at step 100: - Train loss value : 492.952667 -- Test loss value : 485.676086\n",
      "LOSS at step 100: - Train loss value : 515.129028 -- Test loss value : 483.492584\n",
      "LOSS at step 120: - Train loss value : 394.058258 -- Test loss value : 404.154022\n",
      "LOSS at step 120: - Train loss value : 467.160583 -- Test loss value : 402.453278\n",
      "LOSS at step 140: - Train loss value : 394.139160 -- Test loss value : 340.741486\n",
      "LOSS at step 140: - Train loss value : 391.169403 -- Test loss value : 339.324890\n",
      "LOSS at step 160: - Train loss value : 376.049225 -- Test loss value : 290.702972\n",
      "LOSS at step 160: - Train loss value : 252.037567 -- Test loss value : 289.697388\n",
      "LOSS at step 180: - Train loss value : 418.000305 -- Test loss value : 250.500534\n",
      "LOSS at step 180: - Train loss value : 248.803650 -- Test loss value : 249.674957\n",
      "LOSS at step 200: - Train loss value : 580.912659 -- Test loss value : 226.089783\n",
      "LOSS at step 200: - Train loss value : 391.636322 -- Test loss value : 225.660980\n",
      "LOSS at step 220: - Train loss value : 352.905579 -- Test loss value : 206.091690\n",
      "LOSS at step 220: - Train loss value : 342.657166 -- Test loss value : 205.584106\n",
      "LOSS at step 240: - Train loss value : 298.023651 -- Test loss value : 188.410400\n",
      "LOSS at step 240: - Train loss value : 279.022369 -- Test loss value : 188.257874\n",
      "LOSS at step 260: - Train loss value : 479.724030 -- Test loss value : 178.677628\n",
      "LOSS at step 260: - Train loss value : 418.588745 -- Test loss value : 178.535233\n",
      "LOSS at step 280: - Train loss value : 240.382217 -- Test loss value : 172.681412\n",
      "LOSS at step 280: - Train loss value : 207.241730 -- Test loss value : 172.621033\n",
      "LOSS at step 300: - Train loss value : 347.418152 -- Test loss value : 167.995132\n",
      "LOSS at step 300: - Train loss value : 468.025085 -- Test loss value : 167.806686\n",
      "LOSS at step 320: - Train loss value : 293.005768 -- Test loss value : 163.077881\n",
      "LOSS at step 320: - Train loss value : 354.476990 -- Test loss value : 162.933060\n",
      "LOSS at step 340: - Train loss value : 343.745972 -- Test loss value : 158.489395\n",
      "LOSS at step 340: - Train loss value : 182.806549 -- Test loss value : 158.518173\n",
      "LOSS at step 360: - Train loss value : 387.678253 -- Test loss value : 158.394333\n",
      "LOSS at step 360: - Train loss value : 289.358063 -- Test loss value : 158.396606\n",
      "LOSS at step 380: - Train loss value : 392.426178 -- Test loss value : 157.882446\n",
      "LOSS at step 380: - Train loss value : 341.167755 -- Test loss value : 157.815414\n",
      "LOSS at step 400: - Train loss value : 206.103256 -- Test loss value : 157.340866\n",
      "LOSS at step 400: - Train loss value : 307.836426 -- Test loss value : 157.459930\n",
      "LOSS at step 420: - Train loss value : 234.526459 -- Test loss value : 159.589188\n",
      "LOSS at step 420: - Train loss value : 249.852570 -- Test loss value : 159.608032\n",
      "LOSS at step 440: - Train loss value : 258.575256 -- Test loss value : 156.183807\n",
      "LOSS at step 440: - Train loss value : 325.023895 -- Test loss value : 155.899216\n",
      "LOSS at step 460: - Train loss value : 297.024750 -- Test loss value : 153.968109\n",
      "LOSS at step 460: - Train loss value : 361.489594 -- Test loss value : 154.121353\n",
      "LOSS at step 480: - Train loss value : 226.804855 -- Test loss value : 156.560410\n",
      "LOSS at step 480: - Train loss value : 235.533951 -- Test loss value : 156.521179\n",
      "LOSS at step 500: - Train loss value : 190.498047 -- Test loss value : 156.079834\n",
      "LOSS at step 500: - Train loss value : 220.014603 -- Test loss value : 156.057938\n",
      "LOSS at step 520: - Train loss value : 365.235809 -- Test loss value : 156.813385\n",
      "LOSS at step 520: - Train loss value : 272.876892 -- Test loss value : 156.641815\n",
      "LOSS at step 540: - Train loss value : 381.300842 -- Test loss value : 153.494156\n",
      "LOSS at step 540: - Train loss value : 301.085938 -- Test loss value : 153.449982\n",
      "LOSS at step 560: - Train loss value : 340.443481 -- Test loss value : 150.121796\n",
      "LOSS at step 560: - Train loss value : 376.027466 -- Test loss value : 149.925430\n",
      "LOSS at step 580: - Train loss value : 220.356018 -- Test loss value : 150.185333\n",
      "LOSS at step 580: - Train loss value : 196.603790 -- Test loss value : 150.063644\n",
      "LOSS at step 600: - Train loss value : 292.452148 -- Test loss value : 151.838165\n",
      "LOSS at step 600: - Train loss value : 288.613281 -- Test loss value : 151.762558\n",
      "LOSS at step 620: - Train loss value : 329.216919 -- Test loss value : 149.053436\n",
      "LOSS at step 620: - Train loss value : 267.301727 -- Test loss value : 148.924026\n",
      "LOSS at step 640: - Train loss value : 237.874832 -- Test loss value : 147.429169\n",
      "LOSS at step 640: - Train loss value : 380.119690 -- Test loss value : 147.340393\n",
      "LOSS at step 660: - Train loss value : 439.543488 -- Test loss value : 150.287537\n",
      "LOSS at step 660: - Train loss value : 256.335571 -- Test loss value : 150.138916\n",
      "LOSS at step 680: - Train loss value : 244.923630 -- Test loss value : 145.874313\n",
      "LOSS at step 680: - Train loss value : 352.297058 -- Test loss value : 145.976822\n",
      "LOSS at step 700: - Train loss value : 226.498062 -- Test loss value : 147.084106\n",
      "LOSS at step 700: - Train loss value : 171.623718 -- Test loss value : 147.222534\n",
      "LOSS at step 720: - Train loss value : 172.961014 -- Test loss value : 148.861237\n",
      "LOSS at step 720: - Train loss value : 267.473053 -- Test loss value : 148.923370\n",
      "LOSS at step 740: - Train loss value : 332.051819 -- Test loss value : 147.336838\n",
      "LOSS at step 740: - Train loss value : 292.830078 -- Test loss value : 147.474243\n",
      "LOSS at step 760: - Train loss value : 293.204559 -- Test loss value : 145.092758\n",
      "LOSS at step 760: - Train loss value : 277.646759 -- Test loss value : 145.102554\n",
      "LOSS at step 780: - Train loss value : 265.494720 -- Test loss value : 139.575897\n",
      "LOSS at step 780: - Train loss value : 366.651550 -- Test loss value : 139.198547\n",
      "LOSS at step 800: - Train loss value : 278.769409 -- Test loss value : 140.904831\n",
      "LOSS at step 800: - Train loss value : 353.507812 -- Test loss value : 141.014267\n",
      "LOSS at step 820: - Train loss value : 397.210022 -- Test loss value : 143.206940\n",
      "LOSS at step 820: - Train loss value : 434.484253 -- Test loss value : 143.118866\n",
      "LOSS at step 840: - Train loss value : 240.911484 -- Test loss value : 141.328995\n",
      "LOSS at step 840: - Train loss value : 161.081375 -- Test loss value : 141.108688\n",
      "LOSS at step 860: - Train loss value : 237.838577 -- Test loss value : 140.893509\n",
      "LOSS at step 860: - Train loss value : 241.221054 -- Test loss value : 140.703903\n",
      "LOSS at step 880: - Train loss value : 360.079163 -- Test loss value : 137.378204\n",
      "LOSS at step 880: - Train loss value : 335.084320 -- Test loss value : 137.483719\n",
      "LOSS at step 900: - Train loss value : 268.029053 -- Test loss value : 142.869049\n",
      "LOSS at step 900: - Train loss value : 444.840118 -- Test loss value : 143.025146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 920: - Train loss value : 425.909637 -- Test loss value : 139.982300\n",
      "LOSS at step 920: - Train loss value : 298.631409 -- Test loss value : 139.749069\n",
      "LOSS at step 940: - Train loss value : 229.307602 -- Test loss value : 138.202148\n",
      "LOSS at step 940: - Train loss value : 217.111435 -- Test loss value : 138.448624\n",
      "LOSS at step 960: - Train loss value : 262.016022 -- Test loss value : 142.595963\n",
      "LOSS at step 960: - Train loss value : 208.229752 -- Test loss value : 142.487625\n",
      "LOSS at step 980: - Train loss value : 282.583405 -- Test loss value : 140.946304\n",
      "LOSS at step 980: - Train loss value : 216.577316 -- Test loss value : 141.231781\n",
      "LOSS at step 1000: - Train loss value : 343.850037 -- Test loss value : 142.747391\n",
      "LOSS at step 1000: - Train loss value : 268.309998 -- Test loss value : 142.693787\n",
      "LOSS at step 1020: - Train loss value : 408.647736 -- Test loss value : 141.829910\n",
      "LOSS at step 1020: - Train loss value : 267.076935 -- Test loss value : 141.780167\n",
      "LOSS at step 1040: - Train loss value : 337.536102 -- Test loss value : 143.714111\n",
      "LOSS at step 1040: - Train loss value : 247.014221 -- Test loss value : 143.683334\n",
      "LOSS at step 1060: - Train loss value : 234.693542 -- Test loss value : 140.021271\n",
      "LOSS at step 1060: - Train loss value : 314.606110 -- Test loss value : 140.068390\n",
      "LOSS at step 1080: - Train loss value : 172.918549 -- Test loss value : 136.339386\n",
      "LOSS at step 1080: - Train loss value : 337.725220 -- Test loss value : 136.513672\n",
      "LOSS at step 1100: - Train loss value : 277.844604 -- Test loss value : 139.447174\n",
      "LOSS at step 1100: - Train loss value : 258.027740 -- Test loss value : 139.448380\n",
      "LOSS at step 1120: - Train loss value : 352.706970 -- Test loss value : 146.258163\n",
      "LOSS at step 1120: - Train loss value : 276.597656 -- Test loss value : 146.265427\n",
      "LOSS at step 1140: - Train loss value : 253.073761 -- Test loss value : 140.145157\n",
      "LOSS at step 1140: - Train loss value : 340.576111 -- Test loss value : 140.229080\n",
      "LOSS at step 1160: - Train loss value : 211.383621 -- Test loss value : 138.995728\n",
      "LOSS at step 1160: - Train loss value : 261.093231 -- Test loss value : 139.138397\n",
      "LOSS at step 1180: - Train loss value : 209.268616 -- Test loss value : 135.848022\n",
      "LOSS at step 1180: - Train loss value : 211.817688 -- Test loss value : 135.768387\n",
      "LOSS at step 1200: - Train loss value : 282.547028 -- Test loss value : 140.658310\n",
      "LOSS at step 1200: - Train loss value : 197.004242 -- Test loss value : 140.830765\n",
      "LOSS at step 1220: - Train loss value : 242.678970 -- Test loss value : 137.673965\n",
      "LOSS at step 1220: - Train loss value : 236.952957 -- Test loss value : 137.488419\n",
      "LOSS at step 1240: - Train loss value : 246.416534 -- Test loss value : 139.103897\n",
      "LOSS at step 1240: - Train loss value : 298.402405 -- Test loss value : 138.931305\n",
      "LOSS at step 1260: - Train loss value : 231.940750 -- Test loss value : 140.076096\n",
      "LOSS at step 1260: - Train loss value : 258.492554 -- Test loss value : 139.767639\n",
      "LOSS at step 1280: - Train loss value : 246.807388 -- Test loss value : 134.707870\n",
      "LOSS at step 1280: - Train loss value : 204.285431 -- Test loss value : 135.300797\n",
      "LOSS at step 1300: - Train loss value : 250.199066 -- Test loss value : 140.969040\n",
      "LOSS at step 1300: - Train loss value : 172.955215 -- Test loss value : 140.974274\n",
      "LOSS at step 1320: - Train loss value : 320.577301 -- Test loss value : 142.865112\n",
      "LOSS at step 1320: - Train loss value : 280.902740 -- Test loss value : 143.106140\n",
      "LOSS at step 1340: - Train loss value : 375.246735 -- Test loss value : 140.125214\n",
      "LOSS at step 1340: - Train loss value : 296.011902 -- Test loss value : 140.313873\n",
      "LOSS at step 1360: - Train loss value : 272.746368 -- Test loss value : 138.823837\n",
      "LOSS at step 1360: - Train loss value : 279.942261 -- Test loss value : 139.021439\n",
      "LOSS at step 1380: - Train loss value : 266.699554 -- Test loss value : 143.623978\n",
      "LOSS at step 1380: - Train loss value : 386.688080 -- Test loss value : 143.328583\n",
      "LOSS at step 1400: - Train loss value : 206.961395 -- Test loss value : 136.286530\n",
      "LOSS at step 1400: - Train loss value : 270.491516 -- Test loss value : 136.070831\n",
      "LOSS at step 1420: - Train loss value : 497.398621 -- Test loss value : 135.817963\n",
      "LOSS at step 1420: - Train loss value : 328.419891 -- Test loss value : 135.549118\n",
      "LOSS at step 1440: - Train loss value : 124.598686 -- Test loss value : 138.384354\n",
      "LOSS at step 1440: - Train loss value : 246.595139 -- Test loss value : 138.401413\n",
      "LOSS at step 1460: - Train loss value : 239.121582 -- Test loss value : 138.691284\n",
      "LOSS at step 1460: - Train loss value : 187.283401 -- Test loss value : 138.925125\n",
      "LOSS at step 1480: - Train loss value : 338.201355 -- Test loss value : 139.681335\n",
      "LOSS at step 1480: - Train loss value : 272.987732 -- Test loss value : 139.668579\n",
      "LOSS at step 1500: - Train loss value : 383.297180 -- Test loss value : 133.989319\n",
      "LOSS at step 1500: - Train loss value : 240.269135 -- Test loss value : 133.794464\n",
      "LOSS at step 1520: - Train loss value : 327.230804 -- Test loss value : 136.059555\n",
      "LOSS at step 1520: - Train loss value : 304.685669 -- Test loss value : 136.258881\n",
      "LOSS at step 1540: - Train loss value : 413.348694 -- Test loss value : 138.427628\n",
      "LOSS at step 1540: - Train loss value : 381.751434 -- Test loss value : 138.095917\n",
      "LOSS at step 1560: - Train loss value : 316.795837 -- Test loss value : 134.516556\n",
      "LOSS at step 1560: - Train loss value : 172.253860 -- Test loss value : 134.392227\n",
      "LOSS at step 1580: - Train loss value : 299.565765 -- Test loss value : 131.849152\n",
      "LOSS at step 1580: - Train loss value : 196.041824 -- Test loss value : 132.237671\n",
      "LOSS at step 1600: - Train loss value : 190.038376 -- Test loss value : 132.492111\n",
      "LOSS at step 1600: - Train loss value : 298.573914 -- Test loss value : 132.439240\n",
      "LOSS at step 1620: - Train loss value : 324.987671 -- Test loss value : 132.118210\n",
      "LOSS at step 1620: - Train loss value : 210.655594 -- Test loss value : 132.209396\n",
      "LOSS at step 1640: - Train loss value : 214.009232 -- Test loss value : 140.320312\n",
      "LOSS at step 1640: - Train loss value : 164.761383 -- Test loss value : 140.502670\n",
      "LOSS at step 1660: - Train loss value : 318.310944 -- Test loss value : 139.347702\n",
      "LOSS at step 1660: - Train loss value : 271.430450 -- Test loss value : 139.510330\n",
      "LOSS at step 1680: - Train loss value : 231.718918 -- Test loss value : 135.828323\n",
      "LOSS at step 1680: - Train loss value : 372.460876 -- Test loss value : 135.854828\n",
      "LOSS at step 1700: - Train loss value : 235.755569 -- Test loss value : 134.212524\n",
      "LOSS at step 1700: - Train loss value : 307.257996 -- Test loss value : 134.546783\n",
      "LOSS at step 1720: - Train loss value : 290.298309 -- Test loss value : 142.688217\n",
      "LOSS at step 1720: - Train loss value : 297.360016 -- Test loss value : 142.790329\n",
      "LOSS at step 1740: - Train loss value : 217.861115 -- Test loss value : 137.640350\n",
      "LOSS at step 1740: - Train loss value : 205.396057 -- Test loss value : 137.848083\n",
      "LOSS at step 1760: - Train loss value : 218.091827 -- Test loss value : 135.744141\n",
      "LOSS at step 1760: - Train loss value : 223.649841 -- Test loss value : 135.755829\n",
      "LOSS at step 1780: - Train loss value : 295.326111 -- Test loss value : 134.553909\n",
      "LOSS at step 1780: - Train loss value : 349.667816 -- Test loss value : 134.566849\n",
      "LOSS at step 1800: - Train loss value : 257.373199 -- Test loss value : 130.095703\n",
      "LOSS at step 1800: - Train loss value : 273.321808 -- Test loss value : 129.929810\n",
      "LOSS at step 1820: - Train loss value : 254.093246 -- Test loss value : 132.072083\n",
      "LOSS at step 1820: - Train loss value : 231.983582 -- Test loss value : 132.170319\n",
      "LOSS at step 1840: - Train loss value : 311.260468 -- Test loss value : 138.651505\n",
      "LOSS at step 1840: - Train loss value : 270.388916 -- Test loss value : 138.629120\n",
      "LOSS at step 1860: - Train loss value : 196.241730 -- Test loss value : 133.064484\n",
      "LOSS at step 1860: - Train loss value : 342.737000 -- Test loss value : 133.346741\n",
      "LOSS at step 1880: - Train loss value : 315.563477 -- Test loss value : 130.599976\n",
      "LOSS at step 1880: - Train loss value : 281.016724 -- Test loss value : 130.530228\n",
      "LOSS at step 1900: - Train loss value : 226.858871 -- Test loss value : 133.679581\n",
      "LOSS at step 1900: - Train loss value : 405.620239 -- Test loss value : 133.419586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 1920: - Train loss value : 238.574707 -- Test loss value : 129.841049\n",
      "LOSS at step 1920: - Train loss value : 244.610214 -- Test loss value : 129.698502\n",
      "LOSS at step 1940: - Train loss value : 358.130157 -- Test loss value : 131.917297\n",
      "LOSS at step 1940: - Train loss value : 240.876785 -- Test loss value : 131.753845\n",
      "LOSS at step 1960: - Train loss value : 346.051941 -- Test loss value : 134.675888\n",
      "LOSS at step 1960: - Train loss value : 170.462830 -- Test loss value : 134.620804\n",
      "LOSS at step 1980: - Train loss value : 189.264450 -- Test loss value : 128.287994\n",
      "LOSS at step 1980: - Train loss value : 332.491028 -- Test loss value : 128.338486\n",
      "LOSS at step 2000: - Train loss value : 290.347748 -- Test loss value : 134.178726\n",
      "LOSS at step 2000: - Train loss value : 222.124481 -- Test loss value : 134.167053\n",
      "LOSS at step 2020: - Train loss value : 358.808990 -- Test loss value : 134.902008\n",
      "LOSS at step 2020: - Train loss value : 304.752289 -- Test loss value : 134.866943\n",
      "LOSS at step 2040: - Train loss value : 258.361176 -- Test loss value : 133.049225\n",
      "LOSS at step 2040: - Train loss value : 182.635605 -- Test loss value : 133.226028\n",
      "LOSS at step 2060: - Train loss value : 266.904175 -- Test loss value : 138.975372\n",
      "LOSS at step 2060: - Train loss value : 212.562759 -- Test loss value : 138.944077\n",
      "LOSS at step 2080: - Train loss value : 288.553711 -- Test loss value : 133.838638\n",
      "LOSS at step 2080: - Train loss value : 360.936981 -- Test loss value : 133.544327\n",
      "LOSS at step 2100: - Train loss value : 315.882233 -- Test loss value : 131.585022\n",
      "LOSS at step 2100: - Train loss value : 221.121094 -- Test loss value : 131.622726\n",
      "LOSS at step 2120: - Train loss value : 237.831375 -- Test loss value : 131.663742\n",
      "LOSS at step 2120: - Train loss value : 317.064911 -- Test loss value : 131.476944\n",
      "LOSS at step 2140: - Train loss value : 255.501816 -- Test loss value : 134.709946\n",
      "LOSS at step 2140: - Train loss value : 292.229492 -- Test loss value : 134.651718\n",
      "LOSS at step 2160: - Train loss value : 242.726624 -- Test loss value : 134.927902\n",
      "LOSS at step 2160: - Train loss value : 179.202286 -- Test loss value : 134.975433\n",
      "LOSS at step 2180: - Train loss value : 279.881165 -- Test loss value : 134.010239\n",
      "LOSS at step 2180: - Train loss value : 248.909256 -- Test loss value : 134.069870\n",
      "LOSS at step 2200: - Train loss value : 331.301331 -- Test loss value : 128.377121\n",
      "LOSS at step 2200: - Train loss value : 204.854721 -- Test loss value : 128.565750\n",
      "LOSS at step 2220: - Train loss value : 210.299744 -- Test loss value : 137.459930\n",
      "LOSS at step 2220: - Train loss value : 163.536118 -- Test loss value : 137.517181\n",
      "LOSS at step 2240: - Train loss value : 238.683762 -- Test loss value : 131.348129\n",
      "LOSS at step 2240: - Train loss value : 196.984344 -- Test loss value : 131.051147\n",
      "LOSS at step 2260: - Train loss value : 190.233398 -- Test loss value : 132.208862\n",
      "LOSS at step 2260: - Train loss value : 312.954224 -- Test loss value : 132.103699\n",
      "LOSS at step 2280: - Train loss value : 228.908005 -- Test loss value : 131.230118\n",
      "LOSS at step 2280: - Train loss value : 422.075470 -- Test loss value : 131.556442\n",
      "LOSS at step 2300: - Train loss value : 216.117538 -- Test loss value : 136.984314\n",
      "LOSS at step 2300: - Train loss value : 240.208832 -- Test loss value : 136.913788\n",
      "LOSS at step 2320: - Train loss value : 293.227234 -- Test loss value : 132.978210\n",
      "LOSS at step 2320: - Train loss value : 308.095306 -- Test loss value : 133.039581\n",
      "LOSS at step 2340: - Train loss value : 321.960663 -- Test loss value : 133.006943\n",
      "LOSS at step 2340: - Train loss value : 213.766739 -- Test loss value : 132.604721\n",
      "LOSS at step 2360: - Train loss value : 308.016052 -- Test loss value : 129.958115\n",
      "LOSS at step 2360: - Train loss value : 354.134247 -- Test loss value : 129.621017\n",
      "LOSS at step 2380: - Train loss value : 194.790573 -- Test loss value : 130.374008\n",
      "LOSS at step 2380: - Train loss value : 125.060539 -- Test loss value : 130.569794\n",
      "LOSS at step 2400: - Train loss value : 289.779785 -- Test loss value : 135.748611\n",
      "LOSS at step 2400: - Train loss value : 199.135315 -- Test loss value : 135.517456\n",
      "LOSS at step 2420: - Train loss value : 266.219208 -- Test loss value : 129.639008\n",
      "LOSS at step 2420: - Train loss value : 201.906677 -- Test loss value : 129.795456\n",
      "LOSS at step 2440: - Train loss value : 310.734436 -- Test loss value : 132.566315\n",
      "LOSS at step 2440: - Train loss value : 254.543427 -- Test loss value : 132.248001\n",
      "LOSS at step 2460: - Train loss value : 218.695557 -- Test loss value : 133.133560\n",
      "LOSS at step 2460: - Train loss value : 319.848511 -- Test loss value : 133.058548\n",
      "LOSS at step 2480: - Train loss value : 343.071625 -- Test loss value : 135.563522\n",
      "LOSS at step 2480: - Train loss value : 189.263535 -- Test loss value : 135.393509\n",
      "LOSS at step 2500: - Train loss value : 201.505661 -- Test loss value : 127.981125\n",
      "LOSS at step 2500: - Train loss value : 280.996185 -- Test loss value : 127.859512\n",
      "LOSS at step 2520: - Train loss value : 278.779022 -- Test loss value : 131.452011\n",
      "LOSS at step 2520: - Train loss value : 188.427826 -- Test loss value : 131.435822\n",
      "LOSS at step 2540: - Train loss value : 200.528732 -- Test loss value : 127.252121\n",
      "LOSS at step 2540: - Train loss value : 241.307129 -- Test loss value : 127.205872\n",
      "LOSS at step 2560: - Train loss value : 388.721741 -- Test loss value : 124.998520\n",
      "LOSS at step 2560: - Train loss value : 229.900131 -- Test loss value : 125.087791\n",
      "LOSS at step 2580: - Train loss value : 189.202469 -- Test loss value : 129.999100\n",
      "LOSS at step 2580: - Train loss value : 327.558136 -- Test loss value : 129.872589\n",
      "LOSS at step 2600: - Train loss value : 284.953674 -- Test loss value : 131.677139\n",
      "LOSS at step 2600: - Train loss value : 232.941956 -- Test loss value : 131.729019\n",
      "LOSS at step 2620: - Train loss value : 312.437164 -- Test loss value : 133.756088\n",
      "LOSS at step 2620: - Train loss value : 294.402435 -- Test loss value : 134.031723\n",
      "LOSS at step 2640: - Train loss value : 215.732254 -- Test loss value : 131.473099\n",
      "LOSS at step 2640: - Train loss value : 150.637955 -- Test loss value : 131.465042\n",
      "LOSS at step 2660: - Train loss value : 187.933090 -- Test loss value : 132.688370\n",
      "LOSS at step 2660: - Train loss value : 260.156921 -- Test loss value : 132.647888\n",
      "LOSS at step 2680: - Train loss value : 270.258942 -- Test loss value : 130.968109\n",
      "LOSS at step 2680: - Train loss value : 360.002533 -- Test loss value : 131.166718\n",
      "LOSS at step 2700: - Train loss value : 284.836853 -- Test loss value : 130.248291\n",
      "LOSS at step 2700: - Train loss value : 213.703384 -- Test loss value : 130.662323\n",
      "LOSS at step 2720: - Train loss value : 177.426971 -- Test loss value : 130.710785\n",
      "LOSS at step 2720: - Train loss value : 261.677155 -- Test loss value : 130.226151\n",
      "LOSS at step 2740: - Train loss value : 443.854675 -- Test loss value : 130.364090\n",
      "LOSS at step 2740: - Train loss value : 438.999023 -- Test loss value : 130.493683\n",
      "LOSS at step 2760: - Train loss value : 150.838104 -- Test loss value : 132.207565\n",
      "LOSS at step 2760: - Train loss value : 192.255142 -- Test loss value : 131.727036\n",
      "LOSS at step 2780: - Train loss value : 173.259613 -- Test loss value : 128.225266\n",
      "LOSS at step 2780: - Train loss value : 265.386414 -- Test loss value : 128.399200\n",
      "LOSS at step 2800: - Train loss value : 190.631042 -- Test loss value : 126.238220\n",
      "LOSS at step 2800: - Train loss value : 216.888992 -- Test loss value : 125.820763\n",
      "LOSS at step 2820: - Train loss value : 277.930847 -- Test loss value : 127.109558\n",
      "LOSS at step 2820: - Train loss value : 330.759430 -- Test loss value : 127.232689\n",
      "LOSS at step 2840: - Train loss value : 168.543167 -- Test loss value : 131.185852\n",
      "LOSS at step 2840: - Train loss value : 223.502274 -- Test loss value : 131.222061\n",
      "LOSS at step 2860: - Train loss value : 217.635590 -- Test loss value : 128.905151\n",
      "LOSS at step 2860: - Train loss value : 171.440643 -- Test loss value : 128.694794\n",
      "LOSS at step 2880: - Train loss value : 322.199310 -- Test loss value : 125.125572\n",
      "LOSS at step 2880: - Train loss value : 180.496475 -- Test loss value : 125.160736\n",
      "LOSS at step 2900: - Train loss value : 349.717255 -- Test loss value : 128.920425\n",
      "LOSS at step 2900: - Train loss value : 195.532867 -- Test loss value : 128.914185\n",
      "LOSS at step 2920: - Train loss value : 423.008331 -- Test loss value : 130.465195\n",
      "LOSS at step 2920: - Train loss value : 205.825851 -- Test loss value : 130.388153\n",
      "LOSS at step 2940: - Train loss value : 329.637878 -- Test loss value : 129.124008\n",
      "LOSS at step 2940: - Train loss value : 296.705536 -- Test loss value : 128.849380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 2960: - Train loss value : 245.171738 -- Test loss value : 127.901955\n",
      "LOSS at step 2960: - Train loss value : 351.296753 -- Test loss value : 127.772987\n",
      "LOSS at step 2980: - Train loss value : 218.320145 -- Test loss value : 128.556656\n",
      "LOSS at step 2980: - Train loss value : 202.293259 -- Test loss value : 128.367004\n",
      "LOSS at step 3000: - Train loss value : 406.541870 -- Test loss value : 129.099854\n",
      "LOSS at step 3000: - Train loss value : 265.323669 -- Test loss value : 129.427155\n",
      "LOSS at step 3020: - Train loss value : 163.468918 -- Test loss value : 130.874664\n",
      "LOSS at step 3020: - Train loss value : 260.467834 -- Test loss value : 130.851608\n",
      "LOSS at step 3040: - Train loss value : 285.740936 -- Test loss value : 128.036133\n",
      "LOSS at step 3040: - Train loss value : 162.991318 -- Test loss value : 128.130188\n",
      "LOSS at step 3060: - Train loss value : 449.164062 -- Test loss value : 125.553284\n",
      "LOSS at step 3060: - Train loss value : 157.160553 -- Test loss value : 125.516342\n",
      "LOSS at step 3080: - Train loss value : 308.245178 -- Test loss value : 130.870102\n",
      "LOSS at step 3080: - Train loss value : 196.025970 -- Test loss value : 131.056824\n",
      "LOSS at step 3100: - Train loss value : 256.556458 -- Test loss value : 130.777664\n",
      "LOSS at step 3100: - Train loss value : 221.440231 -- Test loss value : 130.637604\n",
      "LOSS at step 3120: - Train loss value : 186.536621 -- Test loss value : 125.305275\n",
      "LOSS at step 3120: - Train loss value : 246.655655 -- Test loss value : 125.570709\n",
      "LOSS at step 3140: - Train loss value : 329.929749 -- Test loss value : 127.554062\n",
      "LOSS at step 3140: - Train loss value : 269.028656 -- Test loss value : 127.745430\n",
      "LOSS at step 3160: - Train loss value : 306.808716 -- Test loss value : 127.455704\n",
      "LOSS at step 3160: - Train loss value : 160.341415 -- Test loss value : 127.490189\n",
      "LOSS at step 3180: - Train loss value : 234.770248 -- Test loss value : 136.396942\n",
      "LOSS at step 3180: - Train loss value : 159.412018 -- Test loss value : 136.620453\n",
      "LOSS at step 3200: - Train loss value : 256.019653 -- Test loss value : 132.792221\n",
      "LOSS at step 3200: - Train loss value : 194.002411 -- Test loss value : 132.806320\n",
      "LOSS at step 3220: - Train loss value : 168.310379 -- Test loss value : 129.439789\n",
      "LOSS at step 3220: - Train loss value : 201.901230 -- Test loss value : 129.420578\n",
      "LOSS at step 3240: - Train loss value : 287.245789 -- Test loss value : 127.526878\n",
      "LOSS at step 3240: - Train loss value : 189.926956 -- Test loss value : 127.457291\n",
      "LOSS at step 3260: - Train loss value : 407.448242 -- Test loss value : 128.909561\n",
      "LOSS at step 3260: - Train loss value : 172.052246 -- Test loss value : 128.792099\n",
      "LOSS at step 3280: - Train loss value : 179.414062 -- Test loss value : 131.219254\n",
      "LOSS at step 3280: - Train loss value : 334.309174 -- Test loss value : 131.068985\n",
      "LOSS at step 3300: - Train loss value : 306.894714 -- Test loss value : 131.113983\n",
      "LOSS at step 3300: - Train loss value : 300.278137 -- Test loss value : 131.142731\n",
      "LOSS at step 3320: - Train loss value : 260.351868 -- Test loss value : 125.869370\n",
      "LOSS at step 3320: - Train loss value : 271.428741 -- Test loss value : 125.535690\n",
      "LOSS at step 3340: - Train loss value : 217.048279 -- Test loss value : 127.265800\n",
      "LOSS at step 3340: - Train loss value : 273.790955 -- Test loss value : 127.204277\n",
      "LOSS at step 3360: - Train loss value : 350.607758 -- Test loss value : 126.813309\n",
      "LOSS at step 3360: - Train loss value : 204.863419 -- Test loss value : 127.013626\n",
      "LOSS at step 3380: - Train loss value : 404.750916 -- Test loss value : 129.118393\n",
      "LOSS at step 3380: - Train loss value : 290.007690 -- Test loss value : 129.013245\n",
      "LOSS at step 3400: - Train loss value : 182.521835 -- Test loss value : 132.816055\n",
      "LOSS at step 3400: - Train loss value : 261.479706 -- Test loss value : 132.570343\n",
      "LOSS at step 3420: - Train loss value : 276.171478 -- Test loss value : 135.797318\n",
      "LOSS at step 3420: - Train loss value : 213.481644 -- Test loss value : 136.187759\n",
      "LOSS at step 3440: - Train loss value : 290.142212 -- Test loss value : 129.201645\n",
      "LOSS at step 3440: - Train loss value : 236.674347 -- Test loss value : 128.984436\n",
      "LOSS at step 3460: - Train loss value : 282.540161 -- Test loss value : 127.801445\n",
      "LOSS at step 3460: - Train loss value : 159.819855 -- Test loss value : 127.890556\n",
      "LOSS at step 3480: - Train loss value : 189.439606 -- Test loss value : 128.604767\n",
      "LOSS at step 3480: - Train loss value : 275.358459 -- Test loss value : 128.660950\n",
      "LOSS at step 3500: - Train loss value : 198.588287 -- Test loss value : 127.645607\n",
      "LOSS at step 3500: - Train loss value : 163.096451 -- Test loss value : 127.552841\n",
      "LOSS at step 3520: - Train loss value : 148.202469 -- Test loss value : 126.920372\n",
      "LOSS at step 3520: - Train loss value : 273.972870 -- Test loss value : 126.902924\n",
      "LOSS at step 3540: - Train loss value : 430.664917 -- Test loss value : 128.704941\n",
      "LOSS at step 3540: - Train loss value : 309.548004 -- Test loss value : 128.647156\n",
      "LOSS at step 3560: - Train loss value : 180.235291 -- Test loss value : 132.005539\n",
      "LOSS at step 3560: - Train loss value : 249.390869 -- Test loss value : 131.946442\n",
      "LOSS at step 3580: - Train loss value : 297.880798 -- Test loss value : 126.073158\n",
      "LOSS at step 3580: - Train loss value : 176.586517 -- Test loss value : 126.032768\n",
      "LOSS at step 3600: - Train loss value : 282.346039 -- Test loss value : 121.619064\n",
      "LOSS at step 3600: - Train loss value : 387.369995 -- Test loss value : 121.314140\n",
      "LOSS at step 3620: - Train loss value : 281.414246 -- Test loss value : 124.152481\n",
      "LOSS at step 3620: - Train loss value : 181.656647 -- Test loss value : 124.246979\n",
      "LOSS at step 3640: - Train loss value : 320.755402 -- Test loss value : 124.418236\n",
      "LOSS at step 3640: - Train loss value : 402.727081 -- Test loss value : 124.612022\n",
      "LOSS at step 3660: - Train loss value : 329.630310 -- Test loss value : 135.202209\n",
      "LOSS at step 3660: - Train loss value : 269.969208 -- Test loss value : 135.283920\n",
      "LOSS at step 3680: - Train loss value : 218.539154 -- Test loss value : 132.587173\n",
      "LOSS at step 3680: - Train loss value : 297.380035 -- Test loss value : 132.665604\n",
      "LOSS at step 3700: - Train loss value : 200.503265 -- Test loss value : 131.508469\n",
      "LOSS at step 3700: - Train loss value : 296.810272 -- Test loss value : 131.023758\n",
      "LOSS at step 3720: - Train loss value : 294.659058 -- Test loss value : 128.496719\n",
      "LOSS at step 3720: - Train loss value : 397.640533 -- Test loss value : 128.469315\n",
      "LOSS at step 3740: - Train loss value : 212.895004 -- Test loss value : 133.144501\n",
      "LOSS at step 3740: - Train loss value : 145.768555 -- Test loss value : 133.133896\n",
      "LOSS at step 3760: - Train loss value : 296.409241 -- Test loss value : 128.178757\n",
      "LOSS at step 3760: - Train loss value : 302.248688 -- Test loss value : 128.212570\n",
      "LOSS at step 3780: - Train loss value : 232.370468 -- Test loss value : 127.527466\n",
      "LOSS at step 3780: - Train loss value : 258.810730 -- Test loss value : 127.745438\n",
      "LOSS at step 3800: - Train loss value : 323.862366 -- Test loss value : 123.384872\n",
      "LOSS at step 3800: - Train loss value : 214.596512 -- Test loss value : 123.014206\n",
      "LOSS at step 3820: - Train loss value : 328.214844 -- Test loss value : 130.782928\n",
      "LOSS at step 3820: - Train loss value : 217.791687 -- Test loss value : 131.233582\n",
      "LOSS at step 3840: - Train loss value : 225.997070 -- Test loss value : 132.350769\n",
      "LOSS at step 3840: - Train loss value : 212.349670 -- Test loss value : 131.977310\n",
      "LOSS at step 3860: - Train loss value : 215.435944 -- Test loss value : 129.126129\n",
      "LOSS at step 3860: - Train loss value : 233.446121 -- Test loss value : 128.867447\n",
      "LOSS at step 3880: - Train loss value : 236.925293 -- Test loss value : 129.524231\n",
      "LOSS at step 3880: - Train loss value : 265.676575 -- Test loss value : 129.445557\n",
      "LOSS at step 3900: - Train loss value : 193.230331 -- Test loss value : 129.133377\n",
      "LOSS at step 3900: - Train loss value : 223.861877 -- Test loss value : 129.167618\n",
      "LOSS at step 3920: - Train loss value : 360.676300 -- Test loss value : 126.398117\n",
      "LOSS at step 3920: - Train loss value : 338.137085 -- Test loss value : 125.953903\n",
      "LOSS at step 3940: - Train loss value : 260.371429 -- Test loss value : 127.393616\n",
      "LOSS at step 3940: - Train loss value : 198.070297 -- Test loss value : 127.304283\n",
      "LOSS at step 3960: - Train loss value : 308.489716 -- Test loss value : 124.662788\n",
      "LOSS at step 3960: - Train loss value : 170.287140 -- Test loss value : 124.267685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 3980: - Train loss value : 184.743546 -- Test loss value : 125.535110\n",
      "LOSS at step 3980: - Train loss value : 315.678345 -- Test loss value : 125.573402\n",
      "LOSS at step 4000: - Train loss value : 279.919739 -- Test loss value : 127.079697\n",
      "LOSS at step 4000: - Train loss value : 177.311005 -- Test loss value : 127.493446\n",
      "LOSS at step 4020: - Train loss value : 234.219208 -- Test loss value : 133.101929\n",
      "LOSS at step 4020: - Train loss value : 256.451477 -- Test loss value : 132.952881\n",
      "LOSS at step 4040: - Train loss value : 232.720245 -- Test loss value : 131.789963\n",
      "LOSS at step 4040: - Train loss value : 317.015289 -- Test loss value : 131.896683\n",
      "LOSS at step 4060: - Train loss value : 241.096481 -- Test loss value : 127.905876\n",
      "LOSS at step 4060: - Train loss value : 260.745331 -- Test loss value : 128.203690\n",
      "LOSS at step 4080: - Train loss value : 209.573669 -- Test loss value : 123.813675\n",
      "LOSS at step 4080: - Train loss value : 215.845688 -- Test loss value : 123.734749\n",
      "LOSS at step 4100: - Train loss value : 331.896179 -- Test loss value : 123.381943\n",
      "LOSS at step 4100: - Train loss value : 260.686707 -- Test loss value : 123.453064\n",
      "LOSS at step 4120: - Train loss value : 268.670105 -- Test loss value : 124.796646\n",
      "LOSS at step 4120: - Train loss value : 238.212692 -- Test loss value : 124.756912\n",
      "LOSS at step 4140: - Train loss value : 200.106339 -- Test loss value : 128.184967\n",
      "LOSS at step 4140: - Train loss value : 479.082733 -- Test loss value : 128.339691\n",
      "LOSS at step 4160: - Train loss value : 366.526978 -- Test loss value : 126.182899\n",
      "LOSS at step 4160: - Train loss value : 215.698929 -- Test loss value : 126.312828\n",
      "LOSS at step 4180: - Train loss value : 222.969589 -- Test loss value : 129.124557\n",
      "LOSS at step 4180: - Train loss value : 327.212769 -- Test loss value : 129.182205\n",
      "LOSS at step 4200: - Train loss value : 319.099884 -- Test loss value : 129.946136\n",
      "LOSS at step 4200: - Train loss value : 233.771713 -- Test loss value : 130.451004\n",
      "LOSS at step 4220: - Train loss value : 246.074738 -- Test loss value : 128.066071\n",
      "LOSS at step 4220: - Train loss value : 288.967377 -- Test loss value : 127.687141\n",
      "LOSS at step 4240: - Train loss value : 174.028717 -- Test loss value : 124.600258\n",
      "LOSS at step 4240: - Train loss value : 223.618942 -- Test loss value : 124.850914\n",
      "LOSS at step 4260: - Train loss value : 151.635696 -- Test loss value : 126.482300\n",
      "LOSS at step 4260: - Train loss value : 388.612854 -- Test loss value : 126.436409\n",
      "LOSS at step 4280: - Train loss value : 239.822601 -- Test loss value : 130.894348\n",
      "LOSS at step 4280: - Train loss value : 321.712769 -- Test loss value : 130.674683\n",
      "LOSS at step 4300: - Train loss value : 240.023956 -- Test loss value : 123.846016\n",
      "LOSS at step 4300: - Train loss value : 253.342911 -- Test loss value : 123.670769\n",
      "LOSS at step 4320: - Train loss value : 252.853912 -- Test loss value : 122.980095\n",
      "LOSS at step 4320: - Train loss value : 224.255569 -- Test loss value : 122.686920\n",
      "LOSS at step 4340: - Train loss value : 258.949036 -- Test loss value : 117.677299\n",
      "LOSS at step 4340: - Train loss value : 298.268250 -- Test loss value : 117.642830\n",
      "LOSS at step 4360: - Train loss value : 258.473053 -- Test loss value : 124.591057\n",
      "LOSS at step 4360: - Train loss value : 222.304199 -- Test loss value : 125.249832\n",
      "LOSS at step 4380: - Train loss value : 236.540955 -- Test loss value : 127.514236\n",
      "LOSS at step 4380: - Train loss value : 204.859940 -- Test loss value : 127.683510\n",
      "LOSS at step 4400: - Train loss value : 247.411041 -- Test loss value : 134.079956\n",
      "LOSS at step 4400: - Train loss value : 211.830673 -- Test loss value : 134.209869\n",
      "LOSS at step 4420: - Train loss value : 253.587921 -- Test loss value : 131.354370\n",
      "LOSS at step 4420: - Train loss value : 295.715546 -- Test loss value : 131.254608\n",
      "LOSS at step 4440: - Train loss value : 290.112457 -- Test loss value : 125.757996\n",
      "LOSS at step 4440: - Train loss value : 307.861725 -- Test loss value : 125.752678\n",
      "LOSS at step 4460: - Train loss value : 130.062836 -- Test loss value : 126.925842\n",
      "LOSS at step 4460: - Train loss value : 311.483368 -- Test loss value : 126.494370\n",
      "LOSS at step 4480: - Train loss value : 188.829910 -- Test loss value : 125.678780\n",
      "LOSS at step 4480: - Train loss value : 257.455658 -- Test loss value : 125.635063\n",
      "LOSS at step 4500: - Train loss value : 196.900681 -- Test loss value : 119.392891\n",
      "LOSS at step 4500: - Train loss value : 142.834381 -- Test loss value : 119.330734\n",
      "LOSS at step 4520: - Train loss value : 424.001923 -- Test loss value : 126.785988\n",
      "LOSS at step 4520: - Train loss value : 298.331573 -- Test loss value : 127.505859\n",
      "LOSS at step 4540: - Train loss value : 187.376144 -- Test loss value : 130.236023\n",
      "LOSS at step 4540: - Train loss value : 364.534119 -- Test loss value : 129.915268\n",
      "LOSS at step 4560: - Train loss value : 177.678452 -- Test loss value : 129.315018\n",
      "LOSS at step 4560: - Train loss value : 230.871094 -- Test loss value : 129.481476\n",
      "LOSS at step 4580: - Train loss value : 281.278290 -- Test loss value : 128.810394\n",
      "LOSS at step 4580: - Train loss value : 345.795410 -- Test loss value : 128.530701\n",
      "LOSS at step 4600: - Train loss value : 280.095520 -- Test loss value : 126.111702\n",
      "LOSS at step 4600: - Train loss value : 257.691040 -- Test loss value : 125.841263\n",
      "LOSS at step 4620: - Train loss value : 286.564362 -- Test loss value : 129.961655\n",
      "LOSS at step 4620: - Train loss value : 220.187180 -- Test loss value : 129.713989\n",
      "LOSS at step 4640: - Train loss value : 257.813873 -- Test loss value : 123.768738\n",
      "LOSS at step 4640: - Train loss value : 240.847504 -- Test loss value : 123.780609\n",
      "LOSS at step 4660: - Train loss value : 187.207840 -- Test loss value : 122.534599\n",
      "LOSS at step 4660: - Train loss value : 313.167145 -- Test loss value : 122.202522\n",
      "LOSS at step 4680: - Train loss value : 159.700623 -- Test loss value : 120.659515\n",
      "LOSS at step 4680: - Train loss value : 210.908798 -- Test loss value : 120.615067\n",
      "LOSS at step 4700: - Train loss value : 222.995865 -- Test loss value : 118.769989\n",
      "LOSS at step 4700: - Train loss value : 234.427277 -- Test loss value : 118.900688\n",
      "LOSS at step 4720: - Train loss value : 261.778687 -- Test loss value : 115.316391\n",
      "LOSS at step 4720: - Train loss value : 288.697815 -- Test loss value : 115.504860\n",
      "LOSS at step 4740: - Train loss value : 186.838562 -- Test loss value : 121.202271\n",
      "LOSS at step 4740: - Train loss value : 197.865952 -- Test loss value : 121.361885\n",
      "LOSS at step 4760: - Train loss value : 295.912689 -- Test loss value : 115.912918\n",
      "LOSS at step 4760: - Train loss value : 185.373016 -- Test loss value : 115.724884\n",
      "LOSS at step 4780: - Train loss value : 196.240051 -- Test loss value : 116.266106\n",
      "LOSS at step 4780: - Train loss value : 207.223099 -- Test loss value : 116.472069\n",
      "LOSS at step 4800: - Train loss value : 173.759476 -- Test loss value : 120.431984\n",
      "LOSS at step 4800: - Train loss value : 274.786499 -- Test loss value : 120.412514\n",
      "LOSS at step 4820: - Train loss value : 235.414307 -- Test loss value : 112.613197\n",
      "LOSS at step 4820: - Train loss value : 186.178162 -- Test loss value : 112.699020\n",
      "LOSS at step 4840: - Train loss value : 204.613373 -- Test loss value : 117.718163\n",
      "LOSS at step 4840: - Train loss value : 232.377701 -- Test loss value : 117.451424\n",
      "LOSS at step 4860: - Train loss value : 225.875320 -- Test loss value : 117.698586\n",
      "LOSS at step 4860: - Train loss value : 234.623383 -- Test loss value : 117.934540\n",
      "LOSS at step 4880: - Train loss value : 274.032257 -- Test loss value : 116.240448\n",
      "LOSS at step 4880: - Train loss value : 236.915985 -- Test loss value : 116.042503\n",
      "LOSS at step 4900: - Train loss value : 201.338699 -- Test loss value : 116.959763\n",
      "LOSS at step 4900: - Train loss value : 224.124405 -- Test loss value : 116.837082\n",
      "LOSS at step 4920: - Train loss value : 271.873749 -- Test loss value : 119.435585\n",
      "LOSS at step 4920: - Train loss value : 246.501419 -- Test loss value : 118.953049\n",
      "LOSS at step 4940: - Train loss value : 294.534668 -- Test loss value : 114.399399\n",
      "LOSS at step 4940: - Train loss value : 265.166046 -- Test loss value : 114.828194\n",
      "LOSS at step 4960: - Train loss value : 159.243744 -- Test loss value : 115.573097\n",
      "LOSS at step 4960: - Train loss value : 231.162277 -- Test loss value : 116.078728\n",
      "LOSS at step 4980: - Train loss value : 209.949615 -- Test loss value : 115.935577\n",
      "LOSS at step 4980: - Train loss value : 253.522720 -- Test loss value : 116.124733\n"
     ]
    }
   ],
   "source": [
    "#ssh sydney\n",
    "#setPython3 \n",
    "\n",
    "import numpy \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "data = arff.loadarff('slump.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "covariables = df.iloc[:,0:7].values\n",
    "response = df.iloc[:,7:10].values\n",
    "positions = np.arange(103)\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "covariables_train, covariables_test, response_train, response_test,positions_train,positions_test = train_test_split(covariables, response,positions, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "##APPRENTISSAGE\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = numpy.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = numpy.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = numpy.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "#import tensorflow as tf \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "# Import data\n",
    "# Tensorflow is finicky about shapes, so resize\n",
    "\n",
    "\n",
    "X_datatrain, X_datatest, targets_train, targets_test = covariables_train, covariables_test, response_train, response_test\n",
    "\n",
    "#les dimensions de X for training\n",
    "NbrLignes_train = X_datatrain.shape[0]  ###\n",
    "NbrColonnes_train= X_datatrain.shape[1] ###\n",
    "\n",
    "#les dimensions de X for testing\n",
    "NbrLignes_test = X_datatest.shape[0]  ###\n",
    "NbrColonnes_test = X_datatest.shape[1] ###\n",
    "#Y_data = targets \n",
    "\n",
    "Y_datatrain = tf.reshape(targets_train, [NbrLignes_train,3])\n",
    "Y_datatest = tf.reshape(targets_test, [NbrLignes_test,3])\n",
    "\n",
    "#build the model\n",
    "\n",
    "# input X: batch_size x NbrColonnes, the first dimension (None) will index the data in the mini-batch\n",
    "Xfill = tf.placeholder(tf.float32, shape= [None, NbrColonnes_train])\n",
    "# correct answers will go here\n",
    "Yfill = tf.placeholder(tf.float32, shape=[None, 3]) \n",
    "#Let's add some layers\n",
    "\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "L = 10 #200\n",
    "M = 3 #160\n",
    "#N = 100\n",
    "#O = 30\n",
    "\n",
    "# Weights initialised with small random values between -0.2 and +0.2\n",
    "# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "W1 = tf.Variable(tf.truncated_normal([NbrColonnes_train, L], stddev=0.1))  # \n",
    "B1 = tf.Variable(tf.ones([L])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([M])/10)\n",
    "#W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "#B3 = tf.Variable(tf.ones([N])/10)\n",
    "#W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "#B4 = tf.Variable(tf.ones([O])/10)\n",
    "#W5 = tf.Variable(tf.truncated_normal([O, 6], stddev=0.1))\n",
    "#B5 = tf.Variable(tf.zeros([6]))\n",
    "\n",
    "# The model, with dropout at each layer\n",
    "\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(Xfill, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "#Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "#Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "#Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "#Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "#Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "#Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "#Y_ = tf.matmul(Y4d, W5) + B5\n",
    "\n",
    "\n",
    "Y_ = tf.matmul(Y1d, W2) + B2\n",
    "\n",
    "cross_entropy = tf.losses.mean_squared_error(Yfill, Y_)\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "#Modification de l'accuracy \n",
    "\n",
    "#calcul de l'accuracy, une nouvelle manière: :  \n",
    "\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy) \n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 100.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# Gradient descent loop for 500 steps\n",
    "for i in range(5000):\n",
    " # Select random minibatch\n",
    "    j = 1\n",
    "    ##### shuffling our data::: really important\n",
    "    X_datasf, Y_datasf = shuffle_in_unison(X_datatrain, Y_datatrain.eval())\n",
    "    \n",
    "    ##### defining a decreasing learning rate: \n",
    "    # learning rate decay\n",
    "    learning_rate = 0.0001001329829992624\n",
    "    #learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    \n",
    "    ##### \n",
    "    for start, end in zip(range(0, NbrLignes_train, 52), range(20, NbrLignes_train, 52)):\n",
    "\n",
    "             X_batch,Y_batch = X_datasf[start:end,],Y_datasf[start:end,]\n",
    "             j +=1\n",
    "             # Do gradient descent step\n",
    "             _, loss_val_train = sess.run([train_step, cross_entropy], feed_dict={Xfill: X_batch, Yfill: Y_batch, pkeep: 0.75, lr: learning_rate})\n",
    "             if i %20==0:    \n",
    "                 \n",
    "             \n",
    "                #if j%20==0:\n",
    "                 loss_val_test = sess.run(cross_entropy, feed_dict={Xfill: X_datatest, Yfill: Y_datatest.eval(), pkeep: 1.0, lr:  learning_rate})\n",
    "                 train_loss.append(loss_val_train)\n",
    "                 \n",
    "                 test_loss.append(loss_val_test)\n",
    "                 print('LOSS at step %s: - Train loss value : %f -- Test loss value : %f' % (i, loss_val_train,loss_val_test))\n",
    "#print(test_loss)\n",
    "#print(test_loss)\n",
    "#LOSS at step 420: - Train loss value : 22.385496 -- Test loss value : 9.721767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us compute the aRRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001001329829992624\n",
    "\n",
    "Y_testpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatest, tf.float32).eval(), Yfill: tf.cast(Y_datatest, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n",
    "#numpy.savetxt('Y_testpred_with_stock_%s_and_ai_%s_modelcut_%s_vartimeframe_%s.txt'%(stock, ai, cut, vartimeframe), Y_testpred, fmt='%f')\n",
    "Y_trainpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatrain, tf.float32).eval(), Yfill: tf.cast(Y_datatrain, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.910864, 53.838516, 40.455544],\n",
       "       [18.839981, 52.02758 , 38.427677],\n",
       "       [15.699167, 42.041897, 31.618938],\n",
       "       [16.038692, 42.742336, 32.426117],\n",
       "       [14.491761, 39.834255, 29.272587],\n",
       "       [14.654827, 40.90637 , 29.597027],\n",
       "       [13.763636, 36.87045 , 27.866917],\n",
       "       [15.696049, 42.071995, 31.588068],\n",
       "       [16.743916, 47.289356, 33.97391 ],\n",
       "       [16.108744, 43.038776, 32.495644],\n",
       "       [19.139694, 51.445244, 38.959476],\n",
       "       [21.174046, 57.59976 , 42.879658],\n",
       "       [21.17463 , 58.040955, 42.919914],\n",
       "       [14.756723, 39.54933 , 29.950932],\n",
       "       [16.716276, 45.23922 , 34.046696],\n",
       "       [13.846725, 37.386703, 27.868904],\n",
       "       [16.96393 , 46.222107, 34.53545 ],\n",
       "       [18.240725, 50.326744, 36.972187],\n",
       "       [16.324738, 46.180153, 33.06686 ],\n",
       "       [17.366587, 47.06668 , 35.038673],\n",
       "       [16.107454, 42.79973 , 32.65476 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_testpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 20.147923\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 19.145161\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 17.259488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aRRMSE_list = []\n",
    "\n",
    "for i in [0,1,2] :\n",
    "    \n",
    "    Errors_test =  Y_testpred[:,i] - response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    ytrain_mean_i = np.tile(Y_trainpred[:,i].mean(axis = 0), (covariables_test.shape[0], 1))\n",
    "\n",
    "    Errors_relative = ytrain_mean_i - response_test[:,i]\n",
    "\n",
    "    Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%Final_SPNNR_aRRMSE)\n",
    "\n",
    "    aRRMSE_list.append(Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.850857264121945"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if old way calculation is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final_Partition aRRMSE is : 84.178972\n"
     ]
    }
   ],
   "source": [
    "Errors_test =  Y_testpred - response_test\n",
    "\n",
    "\n",
    "print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "ytrain_mean = np.tile(Y_trainpred.mean(axis = 0), (response_test.shape[0], 1))\n",
    "\n",
    "Errors_relative = ytrain_mean - response_test\n",
    "\n",
    "Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "print(\"Final_Partition aRRMSE is : %f\"%Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The answer is yes ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking True aRRMSE of S-SPNNR on Slump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_covariables_train, spnnr_covariables_test, spnnr_response_train, spnnr_response_test,spnnr_positions_train,spnnr_positions_test = train_test_split(covariables, response,positions, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_Y_testpred =np.loadtxt('All_Y_test_predictions.out', delimiter=',' )\n",
    "spnnr_Y_trainpred =np.loadtxt('All_Y_train_predictions.out', delimiter=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 27.482091\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 21.644258\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 16.574829\n"
     ]
    }
   ],
   "source": [
    "spnnr_aRRMSE_list = []\n",
    "\n",
    "for i in [0,1,2] :\n",
    "    \n",
    "    spnnr_Errors_test =  spnnr_Y_testpred[:,i] - spnnr_response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    spnnr_ytrain_mean_i = np.tile(spnnr_Y_trainpred[:,i].mean(axis = 0), (spnnr_covariables_test.shape[0], 1))\n",
    "\n",
    "    spnnr_Errors_relative = spnnr_ytrain_mean_i - spnnr_response_test[:,i]\n",
    "\n",
    "    spnnr_Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(spnnr_Errors_test)**2)/(LA.norm(spnnr_Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%spnnr_Final_SPNNR_aRRMSE)\n",
    "\n",
    "    spnnr_aRRMSE_list.append(spnnr_Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.90039258118671"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spnnr_aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
