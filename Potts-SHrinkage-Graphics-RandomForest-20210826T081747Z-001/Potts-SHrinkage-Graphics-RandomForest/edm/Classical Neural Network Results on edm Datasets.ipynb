{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 0: - Train loss value : 0.418338 -- Test loss value : 0.302441\n",
      "LOSS at step 0: - Train loss value : 0.324999 -- Test loss value : 0.302395\n",
      "LOSS at step 20: - Train loss value : 0.451582 -- Test loss value : 0.301165\n",
      "LOSS at step 20: - Train loss value : 0.357303 -- Test loss value : 0.301152\n",
      "LOSS at step 40: - Train loss value : 0.254216 -- Test loss value : 0.301246\n",
      "LOSS at step 40: - Train loss value : 0.334239 -- Test loss value : 0.301254\n",
      "LOSS at step 60: - Train loss value : 0.367973 -- Test loss value : 0.302130\n",
      "LOSS at step 60: - Train loss value : 0.211268 -- Test loss value : 0.302150\n",
      "LOSS at step 80: - Train loss value : 0.373455 -- Test loss value : 0.303518\n",
      "LOSS at step 80: - Train loss value : 0.204125 -- Test loss value : 0.303540\n",
      "LOSS at step 100: - Train loss value : 0.319951 -- Test loss value : 0.304260\n",
      "LOSS at step 100: - Train loss value : 0.296266 -- Test loss value : 0.304267\n",
      "LOSS at step 120: - Train loss value : 0.301519 -- Test loss value : 0.304361\n",
      "LOSS at step 120: - Train loss value : 0.226171 -- Test loss value : 0.304373\n",
      "LOSS at step 140: - Train loss value : 0.315357 -- Test loss value : 0.304904\n",
      "LOSS at step 140: - Train loss value : 0.326696 -- Test loss value : 0.304931\n",
      "LOSS at step 160: - Train loss value : 0.341196 -- Test loss value : 0.305290\n",
      "LOSS at step 160: - Train loss value : 0.254035 -- Test loss value : 0.305257\n",
      "LOSS at step 180: - Train loss value : 0.341030 -- Test loss value : 0.304745\n",
      "LOSS at step 180: - Train loss value : 0.164579 -- Test loss value : 0.304770\n",
      "LOSS at step 200: - Train loss value : 0.318467 -- Test loss value : 0.304801\n",
      "LOSS at step 200: - Train loss value : 0.251910 -- Test loss value : 0.304776\n",
      "LOSS at step 220: - Train loss value : 0.280835 -- Test loss value : 0.304842\n",
      "LOSS at step 220: - Train loss value : 0.352478 -- Test loss value : 0.304872\n",
      "LOSS at step 240: - Train loss value : 0.292368 -- Test loss value : 0.304261\n",
      "LOSS at step 240: - Train loss value : 0.362884 -- Test loss value : 0.304224\n",
      "LOSS at step 260: - Train loss value : 0.274814 -- Test loss value : 0.303107\n",
      "LOSS at step 260: - Train loss value : 0.331500 -- Test loss value : 0.303064\n",
      "LOSS at step 280: - Train loss value : 0.301146 -- Test loss value : 0.301945\n",
      "LOSS at step 280: - Train loss value : 0.267563 -- Test loss value : 0.301932\n",
      "LOSS at step 300: - Train loss value : 0.343784 -- Test loss value : 0.301450\n",
      "LOSS at step 300: - Train loss value : 0.207066 -- Test loss value : 0.301400\n",
      "LOSS at step 320: - Train loss value : 0.242997 -- Test loss value : 0.300833\n",
      "LOSS at step 320: - Train loss value : 0.303645 -- Test loss value : 0.300800\n",
      "LOSS at step 340: - Train loss value : 0.246327 -- Test loss value : 0.299700\n",
      "LOSS at step 340: - Train loss value : 0.275989 -- Test loss value : 0.299686\n",
      "LOSS at step 360: - Train loss value : 0.254734 -- Test loss value : 0.299652\n",
      "LOSS at step 360: - Train loss value : 0.264465 -- Test loss value : 0.299641\n",
      "LOSS at step 380: - Train loss value : 0.313499 -- Test loss value : 0.298344\n",
      "LOSS at step 380: - Train loss value : 0.243552 -- Test loss value : 0.298319\n",
      "LOSS at step 400: - Train loss value : 0.303874 -- Test loss value : 0.297890\n",
      "LOSS at step 400: - Train loss value : 0.171846 -- Test loss value : 0.297882\n",
      "LOSS at step 420: - Train loss value : 0.306564 -- Test loss value : 0.296622\n",
      "LOSS at step 420: - Train loss value : 0.277930 -- Test loss value : 0.296622\n",
      "LOSS at step 440: - Train loss value : 0.265960 -- Test loss value : 0.294523\n",
      "LOSS at step 440: - Train loss value : 0.349373 -- Test loss value : 0.294457\n",
      "LOSS at step 460: - Train loss value : 0.191409 -- Test loss value : 0.293168\n",
      "LOSS at step 460: - Train loss value : 0.315275 -- Test loss value : 0.293194\n",
      "LOSS at step 480: - Train loss value : 0.228248 -- Test loss value : 0.291173\n",
      "LOSS at step 480: - Train loss value : 0.229527 -- Test loss value : 0.291101\n",
      "LOSS at step 500: - Train loss value : 0.292783 -- Test loss value : 0.290218\n",
      "LOSS at step 500: - Train loss value : 0.258608 -- Test loss value : 0.290174\n",
      "LOSS at step 520: - Train loss value : 0.364466 -- Test loss value : 0.289758\n",
      "LOSS at step 520: - Train loss value : 0.306314 -- Test loss value : 0.289709\n",
      "LOSS at step 540: - Train loss value : 0.219452 -- Test loss value : 0.289333\n",
      "LOSS at step 540: - Train loss value : 0.255548 -- Test loss value : 0.289312\n",
      "LOSS at step 560: - Train loss value : 0.329255 -- Test loss value : 0.288195\n",
      "LOSS at step 560: - Train loss value : 0.263863 -- Test loss value : 0.288141\n",
      "LOSS at step 580: - Train loss value : 0.277679 -- Test loss value : 0.286882\n",
      "LOSS at step 580: - Train loss value : 0.312634 -- Test loss value : 0.286896\n",
      "LOSS at step 600: - Train loss value : 0.273066 -- Test loss value : 0.284291\n",
      "LOSS at step 600: - Train loss value : 0.348714 -- Test loss value : 0.284181\n",
      "LOSS at step 620: - Train loss value : 0.274772 -- Test loss value : 0.282705\n",
      "LOSS at step 620: - Train loss value : 0.327275 -- Test loss value : 0.282769\n",
      "LOSS at step 640: - Train loss value : 0.304570 -- Test loss value : 0.283565\n",
      "LOSS at step 640: - Train loss value : 0.235006 -- Test loss value : 0.283477\n",
      "LOSS at step 660: - Train loss value : 0.248496 -- Test loss value : 0.281834\n",
      "LOSS at step 660: - Train loss value : 0.219278 -- Test loss value : 0.281787\n",
      "LOSS at step 680: - Train loss value : 0.335330 -- Test loss value : 0.280148\n",
      "LOSS at step 680: - Train loss value : 0.220646 -- Test loss value : 0.280077\n",
      "LOSS at step 700: - Train loss value : 0.271082 -- Test loss value : 0.277885\n",
      "LOSS at step 700: - Train loss value : 0.307337 -- Test loss value : 0.277884\n",
      "LOSS at step 720: - Train loss value : 0.309873 -- Test loss value : 0.277461\n",
      "LOSS at step 720: - Train loss value : 0.235919 -- Test loss value : 0.277438\n",
      "LOSS at step 740: - Train loss value : 0.184766 -- Test loss value : 0.277057\n",
      "LOSS at step 740: - Train loss value : 0.275626 -- Test loss value : 0.277027\n",
      "LOSS at step 760: - Train loss value : 0.322654 -- Test loss value : 0.276840\n",
      "LOSS at step 760: - Train loss value : 0.168856 -- Test loss value : 0.276837\n",
      "LOSS at step 780: - Train loss value : 0.317339 -- Test loss value : 0.274697\n",
      "LOSS at step 780: - Train loss value : 0.212739 -- Test loss value : 0.274688\n",
      "LOSS at step 800: - Train loss value : 0.244033 -- Test loss value : 0.272464\n",
      "LOSS at step 800: - Train loss value : 0.266763 -- Test loss value : 0.272432\n",
      "LOSS at step 820: - Train loss value : 0.254755 -- Test loss value : 0.271232\n",
      "LOSS at step 820: - Train loss value : 0.240040 -- Test loss value : 0.271151\n",
      "LOSS at step 840: - Train loss value : 0.204447 -- Test loss value : 0.269721\n",
      "LOSS at step 840: - Train loss value : 0.285379 -- Test loss value : 0.269710\n",
      "LOSS at step 860: - Train loss value : 0.180724 -- Test loss value : 0.268955\n",
      "LOSS at step 860: - Train loss value : 0.274613 -- Test loss value : 0.268960\n",
      "LOSS at step 880: - Train loss value : 0.267137 -- Test loss value : 0.267809\n",
      "LOSS at step 880: - Train loss value : 0.293799 -- Test loss value : 0.267819\n",
      "LOSS at step 900: - Train loss value : 0.300064 -- Test loss value : 0.265744\n",
      "LOSS at step 900: - Train loss value : 0.210423 -- Test loss value : 0.265715\n",
      "LOSS at step 920: - Train loss value : 0.209621 -- Test loss value : 0.265029\n",
      "LOSS at step 920: - Train loss value : 0.257165 -- Test loss value : 0.264928\n",
      "LOSS at step 940: - Train loss value : 0.169033 -- Test loss value : 0.262440\n",
      "LOSS at step 940: - Train loss value : 0.351109 -- Test loss value : 0.262408\n",
      "LOSS at step 960: - Train loss value : 0.236029 -- Test loss value : 0.261881\n",
      "LOSS at step 960: - Train loss value : 0.236899 -- Test loss value : 0.261841\n",
      "LOSS at step 980: - Train loss value : 0.234854 -- Test loss value : 0.261946\n",
      "LOSS at step 980: - Train loss value : 0.274717 -- Test loss value : 0.261893\n",
      "LOSS at step 1000: - Train loss value : 0.300737 -- Test loss value : 0.258773\n",
      "LOSS at step 1000: - Train loss value : 0.239801 -- Test loss value : 0.258774\n",
      "LOSS at step 1020: - Train loss value : 0.185407 -- Test loss value : 0.258910\n",
      "LOSS at step 1020: - Train loss value : 0.310564 -- Test loss value : 0.258871\n",
      "LOSS at step 1040: - Train loss value : 0.206294 -- Test loss value : 0.258856\n",
      "LOSS at step 1040: - Train loss value : 0.145067 -- Test loss value : 0.258795\n",
      "LOSS at step 1060: - Train loss value : 0.226904 -- Test loss value : 0.256810\n",
      "LOSS at step 1060: - Train loss value : 0.273036 -- Test loss value : 0.256788\n",
      "LOSS at step 1080: - Train loss value : 0.246501 -- Test loss value : 0.256610\n",
      "LOSS at step 1080: - Train loss value : 0.200490 -- Test loss value : 0.256538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 1100: - Train loss value : 0.222474 -- Test loss value : 0.256960\n",
      "LOSS at step 1100: - Train loss value : 0.246057 -- Test loss value : 0.256896\n",
      "LOSS at step 1120: - Train loss value : 0.269341 -- Test loss value : 0.255620\n",
      "LOSS at step 1120: - Train loss value : 0.256348 -- Test loss value : 0.255548\n",
      "LOSS at step 1140: - Train loss value : 0.218486 -- Test loss value : 0.252673\n",
      "LOSS at step 1140: - Train loss value : 0.281894 -- Test loss value : 0.252564\n",
      "LOSS at step 1160: - Train loss value : 0.204581 -- Test loss value : 0.252355\n",
      "LOSS at step 1160: - Train loss value : 0.231482 -- Test loss value : 0.252280\n",
      "LOSS at step 1180: - Train loss value : 0.283342 -- Test loss value : 0.251004\n",
      "LOSS at step 1180: - Train loss value : 0.178573 -- Test loss value : 0.250913\n",
      "LOSS at step 1200: - Train loss value : 0.240520 -- Test loss value : 0.250758\n",
      "LOSS at step 1200: - Train loss value : 0.276499 -- Test loss value : 0.250809\n",
      "LOSS at step 1220: - Train loss value : 0.261027 -- Test loss value : 0.250449\n",
      "LOSS at step 1220: - Train loss value : 0.211074 -- Test loss value : 0.250335\n",
      "LOSS at step 1240: - Train loss value : 0.192881 -- Test loss value : 0.248083\n",
      "LOSS at step 1240: - Train loss value : 0.306532 -- Test loss value : 0.248089\n",
      "LOSS at step 1260: - Train loss value : 0.252016 -- Test loss value : 0.246937\n",
      "LOSS at step 1260: - Train loss value : 0.197878 -- Test loss value : 0.246944\n",
      "LOSS at step 1280: - Train loss value : 0.278000 -- Test loss value : 0.248129\n",
      "LOSS at step 1280: - Train loss value : 0.145925 -- Test loss value : 0.248172\n",
      "LOSS at step 1300: - Train loss value : 0.210557 -- Test loss value : 0.246664\n",
      "LOSS at step 1300: - Train loss value : 0.202686 -- Test loss value : 0.246597\n",
      "LOSS at step 1320: - Train loss value : 0.257376 -- Test loss value : 0.246754\n",
      "LOSS at step 1320: - Train loss value : 0.224061 -- Test loss value : 0.246663\n",
      "LOSS at step 1340: - Train loss value : 0.236678 -- Test loss value : 0.244639\n",
      "LOSS at step 1340: - Train loss value : 0.203366 -- Test loss value : 0.244627\n",
      "LOSS at step 1360: - Train loss value : 0.237873 -- Test loss value : 0.244523\n",
      "LOSS at step 1360: - Train loss value : 0.222038 -- Test loss value : 0.244570\n",
      "LOSS at step 1380: - Train loss value : 0.227593 -- Test loss value : 0.244478\n",
      "LOSS at step 1380: - Train loss value : 0.206375 -- Test loss value : 0.244491\n",
      "LOSS at step 1400: - Train loss value : 0.195286 -- Test loss value : 0.243961\n",
      "LOSS at step 1400: - Train loss value : 0.182327 -- Test loss value : 0.243802\n",
      "LOSS at step 1420: - Train loss value : 0.224289 -- Test loss value : 0.242214\n",
      "LOSS at step 1420: - Train loss value : 0.272500 -- Test loss value : 0.242248\n",
      "LOSS at step 1440: - Train loss value : 0.258172 -- Test loss value : 0.242250\n",
      "LOSS at step 1440: - Train loss value : 0.254300 -- Test loss value : 0.242213\n",
      "LOSS at step 1460: - Train loss value : 0.193626 -- Test loss value : 0.243290\n",
      "LOSS at step 1460: - Train loss value : 0.179505 -- Test loss value : 0.243293\n",
      "LOSS at step 1480: - Train loss value : 0.182969 -- Test loss value : 0.243455\n",
      "LOSS at step 1480: - Train loss value : 0.189699 -- Test loss value : 0.243562\n",
      "LOSS at step 1500: - Train loss value : 0.249868 -- Test loss value : 0.242347\n",
      "LOSS at step 1500: - Train loss value : 0.242685 -- Test loss value : 0.242270\n",
      "LOSS at step 1520: - Train loss value : 0.143233 -- Test loss value : 0.240423\n",
      "LOSS at step 1520: - Train loss value : 0.242268 -- Test loss value : 0.240406\n",
      "LOSS at step 1540: - Train loss value : 0.241998 -- Test loss value : 0.240749\n",
      "LOSS at step 1540: - Train loss value : 0.195176 -- Test loss value : 0.240785\n",
      "LOSS at step 1560: - Train loss value : 0.186556 -- Test loss value : 0.241550\n",
      "LOSS at step 1560: - Train loss value : 0.272612 -- Test loss value : 0.241617\n",
      "LOSS at step 1580: - Train loss value : 0.272813 -- Test loss value : 0.238653\n",
      "LOSS at step 1580: - Train loss value : 0.165476 -- Test loss value : 0.238556\n",
      "LOSS at step 1600: - Train loss value : 0.177321 -- Test loss value : 0.239733\n",
      "LOSS at step 1600: - Train loss value : 0.230913 -- Test loss value : 0.239788\n",
      "LOSS at step 1620: - Train loss value : 0.214778 -- Test loss value : 0.238842\n",
      "LOSS at step 1620: - Train loss value : 0.233467 -- Test loss value : 0.238825\n",
      "LOSS at step 1640: - Train loss value : 0.215992 -- Test loss value : 0.237788\n",
      "LOSS at step 1640: - Train loss value : 0.247920 -- Test loss value : 0.237858\n",
      "LOSS at step 1660: - Train loss value : 0.270266 -- Test loss value : 0.239307\n",
      "LOSS at step 1660: - Train loss value : 0.158381 -- Test loss value : 0.239218\n",
      "LOSS at step 1680: - Train loss value : 0.217063 -- Test loss value : 0.239844\n",
      "LOSS at step 1680: - Train loss value : 0.211424 -- Test loss value : 0.239823\n",
      "LOSS at step 1700: - Train loss value : 0.225329 -- Test loss value : 0.239717\n",
      "LOSS at step 1700: - Train loss value : 0.222762 -- Test loss value : 0.239763\n",
      "LOSS at step 1720: - Train loss value : 0.209797 -- Test loss value : 0.239757\n",
      "LOSS at step 1720: - Train loss value : 0.206615 -- Test loss value : 0.239844\n",
      "LOSS at step 1740: - Train loss value : 0.199104 -- Test loss value : 0.239175\n",
      "LOSS at step 1740: - Train loss value : 0.240739 -- Test loss value : 0.239213\n",
      "LOSS at step 1760: - Train loss value : 0.210330 -- Test loss value : 0.237898\n",
      "LOSS at step 1760: - Train loss value : 0.226600 -- Test loss value : 0.237843\n",
      "LOSS at step 1780: - Train loss value : 0.162680 -- Test loss value : 0.237038\n",
      "LOSS at step 1780: - Train loss value : 0.170553 -- Test loss value : 0.236965\n",
      "LOSS at step 1800: - Train loss value : 0.125685 -- Test loss value : 0.237974\n",
      "LOSS at step 1800: - Train loss value : 0.226285 -- Test loss value : 0.237945\n",
      "LOSS at step 1820: - Train loss value : 0.232813 -- Test loss value : 0.237499\n",
      "LOSS at step 1820: - Train loss value : 0.238890 -- Test loss value : 0.237588\n",
      "LOSS at step 1840: - Train loss value : 0.205906 -- Test loss value : 0.239258\n",
      "LOSS at step 1840: - Train loss value : 0.246119 -- Test loss value : 0.239195\n",
      "LOSS at step 1860: - Train loss value : 0.203682 -- Test loss value : 0.239002\n",
      "LOSS at step 1860: - Train loss value : 0.213119 -- Test loss value : 0.239112\n",
      "LOSS at step 1880: - Train loss value : 0.228046 -- Test loss value : 0.237377\n",
      "LOSS at step 1880: - Train loss value : 0.176311 -- Test loss value : 0.237282\n",
      "LOSS at step 1900: - Train loss value : 0.175426 -- Test loss value : 0.236020\n",
      "LOSS at step 1900: - Train loss value : 0.252592 -- Test loss value : 0.235970\n",
      "LOSS at step 1920: - Train loss value : 0.187218 -- Test loss value : 0.238299\n",
      "LOSS at step 1920: - Train loss value : 0.191678 -- Test loss value : 0.238290\n",
      "LOSS at step 1940: - Train loss value : 0.193209 -- Test loss value : 0.238020\n",
      "LOSS at step 1940: - Train loss value : 0.199655 -- Test loss value : 0.237928\n",
      "LOSS at step 1960: - Train loss value : 0.175662 -- Test loss value : 0.235560\n",
      "LOSS at step 1960: - Train loss value : 0.187807 -- Test loss value : 0.235469\n",
      "LOSS at step 1980: - Train loss value : 0.177122 -- Test loss value : 0.233775\n",
      "LOSS at step 1980: - Train loss value : 0.227979 -- Test loss value : 0.233714\n",
      "LOSS at step 2000: - Train loss value : 0.229928 -- Test loss value : 0.234891\n",
      "LOSS at step 2000: - Train loss value : 0.209427 -- Test loss value : 0.234800\n",
      "LOSS at step 2020: - Train loss value : 0.167740 -- Test loss value : 0.236152\n",
      "LOSS at step 2020: - Train loss value : 0.303569 -- Test loss value : 0.236162\n",
      "LOSS at step 2040: - Train loss value : 0.236529 -- Test loss value : 0.236411\n",
      "LOSS at step 2040: - Train loss value : 0.226368 -- Test loss value : 0.236459\n",
      "LOSS at step 2060: - Train loss value : 0.265794 -- Test loss value : 0.235391\n",
      "LOSS at step 2060: - Train loss value : 0.194068 -- Test loss value : 0.235319\n",
      "LOSS at step 2080: - Train loss value : 0.206995 -- Test loss value : 0.233175\n",
      "LOSS at step 2080: - Train loss value : 0.299139 -- Test loss value : 0.233245\n",
      "LOSS at step 2100: - Train loss value : 0.149115 -- Test loss value : 0.235774\n",
      "LOSS at step 2100: - Train loss value : 0.222164 -- Test loss value : 0.235939\n",
      "LOSS at step 2120: - Train loss value : 0.197888 -- Test loss value : 0.236921\n",
      "LOSS at step 2120: - Train loss value : 0.197423 -- Test loss value : 0.236861\n",
      "LOSS at step 2140: - Train loss value : 0.203028 -- Test loss value : 0.234740\n",
      "LOSS at step 2140: - Train loss value : 0.289164 -- Test loss value : 0.234999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 2160: - Train loss value : 0.182999 -- Test loss value : 0.234954\n",
      "LOSS at step 2160: - Train loss value : 0.199735 -- Test loss value : 0.235007\n",
      "LOSS at step 2180: - Train loss value : 0.209420 -- Test loss value : 0.232715\n",
      "LOSS at step 2180: - Train loss value : 0.216079 -- Test loss value : 0.232725\n",
      "LOSS at step 2200: - Train loss value : 0.214233 -- Test loss value : 0.233985\n",
      "LOSS at step 2200: - Train loss value : 0.207392 -- Test loss value : 0.233927\n",
      "LOSS at step 2220: - Train loss value : 0.172895 -- Test loss value : 0.233506\n",
      "LOSS at step 2220: - Train loss value : 0.220757 -- Test loss value : 0.233494\n",
      "LOSS at step 2240: - Train loss value : 0.253472 -- Test loss value : 0.233637\n",
      "LOSS at step 2240: - Train loss value : 0.215169 -- Test loss value : 0.233608\n",
      "LOSS at step 2260: - Train loss value : 0.202599 -- Test loss value : 0.231795\n",
      "LOSS at step 2260: - Train loss value : 0.193915 -- Test loss value : 0.231755\n",
      "LOSS at step 2280: - Train loss value : 0.183667 -- Test loss value : 0.232399\n",
      "LOSS at step 2280: - Train loss value : 0.266650 -- Test loss value : 0.232430\n",
      "LOSS at step 2300: - Train loss value : 0.170940 -- Test loss value : 0.232833\n",
      "LOSS at step 2300: - Train loss value : 0.275543 -- Test loss value : 0.232942\n",
      "LOSS at step 2320: - Train loss value : 0.258271 -- Test loss value : 0.232271\n",
      "LOSS at step 2320: - Train loss value : 0.189877 -- Test loss value : 0.232188\n",
      "LOSS at step 2340: - Train loss value : 0.143316 -- Test loss value : 0.233007\n",
      "LOSS at step 2340: - Train loss value : 0.274692 -- Test loss value : 0.232912\n",
      "LOSS at step 2360: - Train loss value : 0.174634 -- Test loss value : 0.231683\n",
      "LOSS at step 2360: - Train loss value : 0.192893 -- Test loss value : 0.231664\n",
      "LOSS at step 2380: - Train loss value : 0.194797 -- Test loss value : 0.231518\n",
      "LOSS at step 2380: - Train loss value : 0.219279 -- Test loss value : 0.231510\n",
      "LOSS at step 2400: - Train loss value : 0.227423 -- Test loss value : 0.230785\n",
      "LOSS at step 2400: - Train loss value : 0.253980 -- Test loss value : 0.230890\n",
      "LOSS at step 2420: - Train loss value : 0.232909 -- Test loss value : 0.232639\n",
      "LOSS at step 2420: - Train loss value : 0.234721 -- Test loss value : 0.232722\n",
      "LOSS at step 2440: - Train loss value : 0.212366 -- Test loss value : 0.230002\n",
      "LOSS at step 2440: - Train loss value : 0.171549 -- Test loss value : 0.229898\n",
      "LOSS at step 2460: - Train loss value : 0.220024 -- Test loss value : 0.231624\n",
      "LOSS at step 2460: - Train loss value : 0.214689 -- Test loss value : 0.231673\n",
      "LOSS at step 2480: - Train loss value : 0.211558 -- Test loss value : 0.231914\n",
      "LOSS at step 2480: - Train loss value : 0.196342 -- Test loss value : 0.231878\n",
      "LOSS at step 2500: - Train loss value : 0.211136 -- Test loss value : 0.232241\n",
      "LOSS at step 2500: - Train loss value : 0.205833 -- Test loss value : 0.232138\n",
      "LOSS at step 2520: - Train loss value : 0.216753 -- Test loss value : 0.230101\n",
      "LOSS at step 2520: - Train loss value : 0.195527 -- Test loss value : 0.230035\n",
      "LOSS at step 2540: - Train loss value : 0.316510 -- Test loss value : 0.229130\n",
      "LOSS at step 2540: - Train loss value : 0.227897 -- Test loss value : 0.229058\n",
      "LOSS at step 2560: - Train loss value : 0.197953 -- Test loss value : 0.230059\n",
      "LOSS at step 2560: - Train loss value : 0.210711 -- Test loss value : 0.230129\n",
      "LOSS at step 2580: - Train loss value : 0.261821 -- Test loss value : 0.229296\n",
      "LOSS at step 2580: - Train loss value : 0.209911 -- Test loss value : 0.229250\n",
      "LOSS at step 2600: - Train loss value : 0.270340 -- Test loss value : 0.230746\n",
      "LOSS at step 2600: - Train loss value : 0.174496 -- Test loss value : 0.230729\n",
      "LOSS at step 2620: - Train loss value : 0.222664 -- Test loss value : 0.228563\n",
      "LOSS at step 2620: - Train loss value : 0.288092 -- Test loss value : 0.228576\n",
      "LOSS at step 2640: - Train loss value : 0.282181 -- Test loss value : 0.228329\n",
      "LOSS at step 2640: - Train loss value : 0.152041 -- Test loss value : 0.228443\n",
      "LOSS at step 2660: - Train loss value : 0.150846 -- Test loss value : 0.230437\n",
      "LOSS at step 2660: - Train loss value : 0.229685 -- Test loss value : 0.230507\n",
      "LOSS at step 2680: - Train loss value : 0.247743 -- Test loss value : 0.228785\n",
      "LOSS at step 2680: - Train loss value : 0.228520 -- Test loss value : 0.228644\n",
      "LOSS at step 2700: - Train loss value : 0.261784 -- Test loss value : 0.226851\n",
      "LOSS at step 2700: - Train loss value : 0.176445 -- Test loss value : 0.226725\n",
      "LOSS at step 2720: - Train loss value : 0.176299 -- Test loss value : 0.227195\n",
      "LOSS at step 2720: - Train loss value : 0.174212 -- Test loss value : 0.227227\n",
      "LOSS at step 2740: - Train loss value : 0.166841 -- Test loss value : 0.228950\n",
      "LOSS at step 2740: - Train loss value : 0.261213 -- Test loss value : 0.229027\n",
      "LOSS at step 2760: - Train loss value : 0.291916 -- Test loss value : 0.229558\n",
      "LOSS at step 2760: - Train loss value : 0.226387 -- Test loss value : 0.229474\n",
      "LOSS at step 2780: - Train loss value : 0.262230 -- Test loss value : 0.227900\n",
      "LOSS at step 2780: - Train loss value : 0.270046 -- Test loss value : 0.227899\n",
      "LOSS at step 2800: - Train loss value : 0.260818 -- Test loss value : 0.227576\n",
      "LOSS at step 2800: - Train loss value : 0.220149 -- Test loss value : 0.227557\n",
      "LOSS at step 2820: - Train loss value : 0.230274 -- Test loss value : 0.228373\n",
      "LOSS at step 2820: - Train loss value : 0.224806 -- Test loss value : 0.228361\n",
      "LOSS at step 2840: - Train loss value : 0.197248 -- Test loss value : 0.228679\n",
      "LOSS at step 2840: - Train loss value : 0.262228 -- Test loss value : 0.228637\n",
      "LOSS at step 2860: - Train loss value : 0.180738 -- Test loss value : 0.227022\n",
      "LOSS at step 2860: - Train loss value : 0.206615 -- Test loss value : 0.227028\n",
      "LOSS at step 2880: - Train loss value : 0.195066 -- Test loss value : 0.228155\n",
      "LOSS at step 2880: - Train loss value : 0.251220 -- Test loss value : 0.228216\n",
      "LOSS at step 2900: - Train loss value : 0.188341 -- Test loss value : 0.226493\n",
      "LOSS at step 2900: - Train loss value : 0.190511 -- Test loss value : 0.226385\n",
      "LOSS at step 2920: - Train loss value : 0.161990 -- Test loss value : 0.227278\n",
      "LOSS at step 2920: - Train loss value : 0.210430 -- Test loss value : 0.227266\n",
      "LOSS at step 2940: - Train loss value : 0.180939 -- Test loss value : 0.226717\n",
      "LOSS at step 2940: - Train loss value : 0.210947 -- Test loss value : 0.226763\n",
      "LOSS at step 2960: - Train loss value : 0.179110 -- Test loss value : 0.227306\n",
      "LOSS at step 2960: - Train loss value : 0.286220 -- Test loss value : 0.227278\n",
      "LOSS at step 2980: - Train loss value : 0.204953 -- Test loss value : 0.227819\n",
      "LOSS at step 2980: - Train loss value : 0.223479 -- Test loss value : 0.227850\n",
      "LOSS at step 3000: - Train loss value : 0.170822 -- Test loss value : 0.228364\n",
      "LOSS at step 3000: - Train loss value : 0.164507 -- Test loss value : 0.228436\n",
      "LOSS at step 3020: - Train loss value : 0.226531 -- Test loss value : 0.226831\n",
      "LOSS at step 3020: - Train loss value : 0.185076 -- Test loss value : 0.226762\n",
      "LOSS at step 3040: - Train loss value : 0.200208 -- Test loss value : 0.228529\n",
      "LOSS at step 3040: - Train loss value : 0.255818 -- Test loss value : 0.228446\n",
      "LOSS at step 3060: - Train loss value : 0.255033 -- Test loss value : 0.228087\n",
      "LOSS at step 3060: - Train loss value : 0.178098 -- Test loss value : 0.227983\n",
      "LOSS at step 3080: - Train loss value : 0.206540 -- Test loss value : 0.226319\n",
      "LOSS at step 3080: - Train loss value : 0.188149 -- Test loss value : 0.226253\n",
      "LOSS at step 3100: - Train loss value : 0.241433 -- Test loss value : 0.226336\n",
      "LOSS at step 3100: - Train loss value : 0.225667 -- Test loss value : 0.226334\n",
      "LOSS at step 3120: - Train loss value : 0.157201 -- Test loss value : 0.228005\n",
      "LOSS at step 3120: - Train loss value : 0.233055 -- Test loss value : 0.227948\n",
      "LOSS at step 3140: - Train loss value : 0.190787 -- Test loss value : 0.225881\n",
      "LOSS at step 3140: - Train loss value : 0.155693 -- Test loss value : 0.225835\n",
      "LOSS at step 3160: - Train loss value : 0.230984 -- Test loss value : 0.224154\n",
      "LOSS at step 3160: - Train loss value : 0.268031 -- Test loss value : 0.224087\n",
      "LOSS at step 3180: - Train loss value : 0.116298 -- Test loss value : 0.224717\n",
      "LOSS at step 3180: - Train loss value : 0.203940 -- Test loss value : 0.224708\n",
      "LOSS at step 3200: - Train loss value : 0.192372 -- Test loss value : 0.224321\n",
      "LOSS at step 3200: - Train loss value : 0.181903 -- Test loss value : 0.224471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 3220: - Train loss value : 0.193249 -- Test loss value : 0.226459\n",
      "LOSS at step 3220: - Train loss value : 0.237119 -- Test loss value : 0.226401\n",
      "LOSS at step 3240: - Train loss value : 0.249140 -- Test loss value : 0.223840\n",
      "LOSS at step 3240: - Train loss value : 0.174145 -- Test loss value : 0.223796\n",
      "LOSS at step 3260: - Train loss value : 0.211152 -- Test loss value : 0.226185\n",
      "LOSS at step 3260: - Train loss value : 0.190529 -- Test loss value : 0.226191\n",
      "LOSS at step 3280: - Train loss value : 0.187302 -- Test loss value : 0.225738\n",
      "LOSS at step 3280: - Train loss value : 0.223370 -- Test loss value : 0.225800\n",
      "LOSS at step 3300: - Train loss value : 0.227646 -- Test loss value : 0.225434\n",
      "LOSS at step 3300: - Train loss value : 0.201734 -- Test loss value : 0.225518\n",
      "LOSS at step 3320: - Train loss value : 0.223333 -- Test loss value : 0.224586\n",
      "LOSS at step 3320: - Train loss value : 0.204442 -- Test loss value : 0.224557\n",
      "LOSS at step 3340: - Train loss value : 0.184921 -- Test loss value : 0.223329\n",
      "LOSS at step 3340: - Train loss value : 0.268729 -- Test loss value : 0.223317\n",
      "LOSS at step 3360: - Train loss value : 0.205467 -- Test loss value : 0.224241\n",
      "LOSS at step 3360: - Train loss value : 0.128488 -- Test loss value : 0.224218\n",
      "LOSS at step 3380: - Train loss value : 0.247773 -- Test loss value : 0.224941\n",
      "LOSS at step 3380: - Train loss value : 0.155738 -- Test loss value : 0.225046\n",
      "LOSS at step 3400: - Train loss value : 0.139746 -- Test loss value : 0.224358\n",
      "LOSS at step 3400: - Train loss value : 0.283205 -- Test loss value : 0.224292\n",
      "LOSS at step 3420: - Train loss value : 0.169422 -- Test loss value : 0.224067\n",
      "LOSS at step 3420: - Train loss value : 0.251056 -- Test loss value : 0.224147\n",
      "LOSS at step 3440: - Train loss value : 0.220168 -- Test loss value : 0.225985\n",
      "LOSS at step 3440: - Train loss value : 0.288021 -- Test loss value : 0.225979\n",
      "LOSS at step 3460: - Train loss value : 0.236600 -- Test loss value : 0.225655\n",
      "LOSS at step 3460: - Train loss value : 0.192494 -- Test loss value : 0.225607\n",
      "LOSS at step 3480: - Train loss value : 0.199679 -- Test loss value : 0.225456\n",
      "LOSS at step 3480: - Train loss value : 0.184035 -- Test loss value : 0.225339\n"
     ]
    }
   ],
   "source": [
    "#ssh sydney\n",
    "#setPython3 \n",
    "\n",
    "import numpy \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import arff\n",
    "data = arff.loadarff('edm.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "covariables = df.iloc[:,0:16].values\n",
    "response = df.iloc[:,16:18].values\n",
    "positions = np.arange(154)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "covariables_train, covariables_test, response_train, response_test,positions_train,positions_test = train_test_split(covariables, response,positions, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "##APPRENTISSAGE\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = numpy.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = numpy.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = numpy.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "#import tensorflow as tf \n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "# Import data\n",
    "# Tensorflow is finicky about shapes, so resize\n",
    "\n",
    "\n",
    "X_datatrain, X_datatest, targets_train, targets_test = covariables_train, covariables_test, response_train, response_test\n",
    "\n",
    "#les dimensions de X for training\n",
    "NbrLignes_train = X_datatrain.shape[0]  ###\n",
    "NbrColonnes_train= X_datatrain.shape[1] ###\n",
    "\n",
    "#les dimensions de X for testing\n",
    "NbrLignes_test = X_datatest.shape[0]  ###\n",
    "NbrColonnes_test = X_datatest.shape[1] ###\n",
    "#Y_data = targets \n",
    "\n",
    "Y_datatrain = tf.reshape(targets_train, [NbrLignes_train,targets_train.shape[1]])\n",
    "Y_datatest = tf.reshape(targets_test, [NbrLignes_test,targets_train.shape[1]])\n",
    "\n",
    "#build the model\n",
    "\n",
    "# input X: batch_size x NbrColonnes, the first dimension (None) will index the data in the mini-batch\n",
    "Xfill = tf.placeholder(tf.float32, shape= [None, NbrColonnes_train])\n",
    "# correct answers will go here\n",
    "Yfill = tf.placeholder(tf.float32, shape=[None, targets_train.shape[1]]) \n",
    "#Let's add some layers\n",
    "\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "L = 10 #200\n",
    "M = 2 #160\n",
    "#N = 100\n",
    "#O = 30\n",
    "\n",
    "# Weights initialised with small random values between -0.2 and +0.2\n",
    "# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "W1 = tf.Variable(tf.truncated_normal([NbrColonnes_train, L], stddev=0.1))  # \n",
    "B1 = tf.Variable(tf.ones([L])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([M])/10)\n",
    "#W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "#B3 = tf.Variable(tf.ones([N])/10)\n",
    "#W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "#B4 = tf.Variable(tf.ones([O])/10)\n",
    "#W5 = tf.Variable(tf.truncated_normal([O, 6], stddev=0.1))\n",
    "#B5 = tf.Variable(tf.zeros([6]))\n",
    "\n",
    "# The model, with dropout at each layer\n",
    "\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(Xfill, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "#Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "#Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "#Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "#Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "#Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "#Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "#Y_ = tf.matmul(Y4d, W5) + B5\n",
    "\n",
    "\n",
    "Y_ = tf.matmul(Y1d, W2) + B2\n",
    "\n",
    "cross_entropy = tf.losses.mean_squared_error(Yfill, Y_)\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "#Modification de l'accuracy \n",
    "\n",
    "#calcul de l'accuracy, une nouvelle manière: :  \n",
    "\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy) \n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 100.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# Gradient descent loop for 500 steps\n",
    "for i in range(3500):\n",
    " # Select random minibatch\n",
    "    j = 1\n",
    "    ##### shuffling our data::: really important\n",
    "    X_datasf, Y_datasf = shuffle_in_unison(X_datatrain, Y_datatrain.eval())\n",
    "    \n",
    "    ##### defining a decreasing learning rate: \n",
    "    # learning rate decay\n",
    "    learning_rate = 0.0001001329829992624\n",
    "    #learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    \n",
    "    ##### \n",
    "    for start, end in zip(range(0, NbrLignes_train, 52), range(20, NbrLignes_train, 52)):\n",
    "\n",
    "             X_batch,Y_batch = X_datasf[start:end,],Y_datasf[start:end,]\n",
    "             j +=1\n",
    "             # Do gradient descent step\n",
    "             _, loss_val_train = sess.run([train_step, cross_entropy], feed_dict={Xfill: X_batch, Yfill: Y_batch, pkeep: 0.75, lr: learning_rate})\n",
    "             if i %20==0:    \n",
    "                 \n",
    "             \n",
    "                #if j%20==0:\n",
    "                 loss_val_test = sess.run(cross_entropy, feed_dict={Xfill: X_datatest, Yfill: Y_datatest.eval(), pkeep: 1.0, lr:  learning_rate})\n",
    "                 train_loss.append(loss_val_train)\n",
    "                 \n",
    "                 test_loss.append(loss_val_test)\n",
    "                 print('LOSS at step %s: - Train loss value : %f -- Test loss value : %f' % (i, loss_val_train,loss_val_test))\n",
    "#print(test_loss)\n",
    "#print(test_loss)\n",
    "#LOSS at step 420: - Train loss value : 22.385496 -- Test loss value : 9.721767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us compute the aRRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001001329829992624\n",
    "\n",
    "Y_testpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatest, tf.float32).eval(), Yfill: tf.cast(Y_datatest, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n",
    "#numpy.savetxt('Y_testpred_with_stock_%s_and_ai_%s_modelcut_%s_vartimeframe_%s.txt'%(stock, ai, cut, vartimeframe), Y_testpred, fmt='%f')\n",
    "Y_trainpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatrain, tf.float32).eval(), Yfill: tf.cast(Y_datatrain, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01336936, -0.29627833],\n",
       "       [-0.0222036 , -0.40931678],\n",
       "       [-0.01517815, -0.3877168 ],\n",
       "       [ 0.27723137,  0.46779692],\n",
       "       [ 0.03970774, -0.23299764],\n",
       "       [ 0.2342838 ,  0.25013196],\n",
       "       [ 0.2211327 ,  0.27662715],\n",
       "       [ 0.21348323,  0.27496138],\n",
       "       [ 0.2598354 ,  0.2917165 ],\n",
       "       [-0.00603638, -0.35114124],\n",
       "       [ 0.25159645,  0.38576394],\n",
       "       [ 0.22891743,  0.31393927],\n",
       "       [ 0.15962963,  0.10807078],\n",
       "       [-0.02340811, -0.4194054 ],\n",
       "       [ 0.2834255 ,  0.4508801 ],\n",
       "       [-0.01550719, -0.38096148],\n",
       "       [ 0.03851022, -0.2209176 ],\n",
       "       [ 0.00666024, -0.27359796],\n",
       "       [ 0.00658341, -0.28682032],\n",
       "       [ 0.32136458,  0.5277236 ],\n",
       "       [ 0.27250588,  0.37393773],\n",
       "       [ 0.20317493,  0.20584066],\n",
       "       [ 0.21253932,  0.30614597],\n",
       "       [ 0.02180384, -0.2776199 ],\n",
       "       [ 0.2327356 ,  0.32615134],\n",
       "       [ 0.28014517,  0.40500492],\n",
       "       [-0.02600013, -0.41755128],\n",
       "       [ 0.2755431 ,  0.4384333 ],\n",
       "       [-0.0038654 , -0.34581518],\n",
       "       [ 0.00565112, -0.30660334],\n",
       "       [ 0.00928442, -0.2861292 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_testpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 16.959328\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 14.604560\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aRRMSE_list = []\n",
    "\n",
    "for i in range(Y_testpred.shape[1]) :\n",
    "    \n",
    "    Errors_test =  Y_testpred[:,i] - response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    ytrain_mean_i = np.tile(Y_trainpred[:,i].mean(axis = 0), (covariables_test.shape[0], 1))\n",
    "\n",
    "    Errors_relative = ytrain_mean_i - response_test[:,i]\n",
    "\n",
    "    Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%Final_SPNNR_aRRMSE)\n",
    "\n",
    "    aRRMSE_list.append(Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.781944266743007"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking aRRMSE on PCA or inverted PCA. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if old way calculation is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final_Partition aRRMSE is : 82.647642\n"
     ]
    }
   ],
   "source": [
    "Errors_test =  Y_testpred - response_test\n",
    "\n",
    "\n",
    "print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "ytrain_mean = np.tile(Y_trainpred.mean(axis = 0), (response_test.shape[0], 1))\n",
    "\n",
    "Errors_relative = ytrain_mean - response_test\n",
    "\n",
    "Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "print(\"Final_Partition aRRMSE is : %f\"%Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The answer is yes ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking True aRRMSE of S-SPNNR on Slump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_covariables_train, spnnr_covariables_test, spnnr_response_train, spnnr_response_test,spnnr_positions_train,spnnr_positions_test = train_test_split(covariables, response,positions, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_Y_testpred =np.loadtxt('All_Y_test_predictions.out', delimiter=',' )\n",
    "spnnr_Y_trainpred =np.loadtxt('All_Y_train_predictions.out', delimiter=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 9.212108\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 17.784301\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.862991\n"
     ]
    }
   ],
   "source": [
    "spnnr_aRRMSE_list = []\n",
    "\n",
    "for i in range(spnnr_Y_testpred.shape[1]) :\n",
    "    \n",
    "    spnnr_Errors_test =  spnnr_Y_testpred[:,i] - spnnr_response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    spnnr_ytrain_mean_i = np.tile(spnnr_Y_trainpred[:,i].mean(axis = 0), (spnnr_covariables_test.shape[0], 1))\n",
    "\n",
    "    spnnr_Errors_relative = spnnr_ytrain_mean_i - spnnr_response_test[:,i]\n",
    "\n",
    "    spnnr_Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(spnnr_Errors_test)**2)/(LA.norm(spnnr_Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%spnnr_Final_SPNNR_aRRMSE)\n",
    "\n",
    "    spnnr_aRRMSE_list.append(spnnr_Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.286466834678842"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spnnr_aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
