{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/misc/Python/Python-3.6.7/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/misc/Python/Python-3.6.7/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 0: - Train loss value : 2648.545166 -- Test loss value : 1968.223511\n",
      "LOSS at step 0: - Train loss value : 2907.702637 -- Test loss value : 1865.133667\n",
      "LOSS at step 20: - Train loss value : 1097.554321 -- Test loss value : 522.088867\n",
      "LOSS at step 20: - Train loss value : 1040.938843 -- Test loss value : 513.570190\n",
      "LOSS at step 40: - Train loss value : 715.231384 -- Test loss value : 423.931946\n",
      "LOSS at step 40: - Train loss value : 621.348816 -- Test loss value : 424.582611\n",
      "LOSS at step 60: - Train loss value : 761.149048 -- Test loss value : 426.759216\n",
      "LOSS at step 60: - Train loss value : 778.664307 -- Test loss value : 428.861450\n",
      "LOSS at step 80: - Train loss value : 690.229187 -- Test loss value : 373.820923\n",
      "LOSS at step 80: - Train loss value : 603.710815 -- Test loss value : 371.508240\n",
      "LOSS at step 100: - Train loss value : 524.154114 -- Test loss value : 350.718964\n",
      "LOSS at step 100: - Train loss value : 546.477905 -- Test loss value : 350.615082\n",
      "LOSS at step 120: - Train loss value : 430.618744 -- Test loss value : 385.815796\n",
      "LOSS at step 120: - Train loss value : 485.930756 -- Test loss value : 385.560699\n",
      "LOSS at step 140: - Train loss value : 369.101898 -- Test loss value : 373.053955\n",
      "LOSS at step 140: - Train loss value : 415.636993 -- Test loss value : 373.136200\n",
      "LOSS at step 160: - Train loss value : 534.457092 -- Test loss value : 356.950531\n",
      "LOSS at step 160: - Train loss value : 315.561462 -- Test loss value : 359.514313\n",
      "LOSS at step 180: - Train loss value : 457.469940 -- Test loss value : 338.540375\n",
      "LOSS at step 180: - Train loss value : 434.494659 -- Test loss value : 338.392548\n",
      "LOSS at step 200: - Train loss value : 302.442810 -- Test loss value : 384.194122\n",
      "LOSS at step 200: - Train loss value : 400.550903 -- Test loss value : 382.430145\n",
      "LOSS at step 220: - Train loss value : 265.099609 -- Test loss value : 330.766296\n",
      "LOSS at step 220: - Train loss value : 343.063812 -- Test loss value : 332.995880\n",
      "LOSS at step 240: - Train loss value : 460.583801 -- Test loss value : 380.320526\n",
      "LOSS at step 240: - Train loss value : 304.308075 -- Test loss value : 381.940216\n",
      "LOSS at step 260: - Train loss value : 206.479446 -- Test loss value : 314.047119\n",
      "LOSS at step 260: - Train loss value : 319.747833 -- Test loss value : 316.131317\n",
      "LOSS at step 280: - Train loss value : 316.374603 -- Test loss value : 322.480286\n",
      "LOSS at step 280: - Train loss value : 337.747406 -- Test loss value : 321.120239\n",
      "LOSS at step 300: - Train loss value : 381.917023 -- Test loss value : 316.297974\n",
      "LOSS at step 300: - Train loss value : 378.868561 -- Test loss value : 320.936432\n",
      "LOSS at step 320: - Train loss value : 225.516632 -- Test loss value : 361.616028\n",
      "LOSS at step 320: - Train loss value : 198.665619 -- Test loss value : 361.589203\n",
      "LOSS at step 340: - Train loss value : 206.842102 -- Test loss value : 303.660645\n",
      "LOSS at step 340: - Train loss value : 247.052307 -- Test loss value : 302.921509\n",
      "LOSS at step 360: - Train loss value : 228.479355 -- Test loss value : 351.485413\n",
      "LOSS at step 360: - Train loss value : 282.221222 -- Test loss value : 349.243744\n",
      "LOSS at step 380: - Train loss value : 279.971710 -- Test loss value : 299.090607\n",
      "LOSS at step 380: - Train loss value : 345.434692 -- Test loss value : 299.970154\n",
      "LOSS at step 400: - Train loss value : 327.795563 -- Test loss value : 354.060760\n",
      "LOSS at step 400: - Train loss value : 290.949493 -- Test loss value : 357.658264\n",
      "LOSS at step 420: - Train loss value : 312.237885 -- Test loss value : 334.235687\n",
      "LOSS at step 420: - Train loss value : 454.636353 -- Test loss value : 336.907379\n",
      "LOSS at step 440: - Train loss value : 336.844635 -- Test loss value : 330.055054\n",
      "LOSS at step 440: - Train loss value : 214.499695 -- Test loss value : 331.746796\n",
      "LOSS at step 460: - Train loss value : 222.272552 -- Test loss value : 354.654572\n",
      "LOSS at step 460: - Train loss value : 245.914688 -- Test loss value : 353.752960\n",
      "LOSS at step 480: - Train loss value : 301.517670 -- Test loss value : 349.229279\n",
      "LOSS at step 480: - Train loss value : 308.278412 -- Test loss value : 351.572113\n",
      "LOSS at step 500: - Train loss value : 332.907745 -- Test loss value : 349.937897\n",
      "LOSS at step 500: - Train loss value : 413.323914 -- Test loss value : 351.225769\n",
      "LOSS at step 520: - Train loss value : 389.686584 -- Test loss value : 323.907684\n",
      "LOSS at step 520: - Train loss value : 241.534348 -- Test loss value : 322.897949\n",
      "LOSS at step 540: - Train loss value : 269.378754 -- Test loss value : 388.076721\n",
      "LOSS at step 540: - Train loss value : 276.668427 -- Test loss value : 390.792877\n",
      "LOSS at step 560: - Train loss value : 269.240265 -- Test loss value : 379.318726\n",
      "LOSS at step 560: - Train loss value : 198.938522 -- Test loss value : 379.337646\n",
      "LOSS at step 580: - Train loss value : 287.675171 -- Test loss value : 405.563507\n",
      "LOSS at step 580: - Train loss value : 265.148224 -- Test loss value : 407.227783\n",
      "LOSS at step 600: - Train loss value : 244.776688 -- Test loss value : 385.122955\n",
      "LOSS at step 600: - Train loss value : 217.019333 -- Test loss value : 386.408112\n",
      "LOSS at step 620: - Train loss value : 252.394669 -- Test loss value : 426.152771\n",
      "LOSS at step 620: - Train loss value : 385.289154 -- Test loss value : 425.699768\n",
      "LOSS at step 640: - Train loss value : 234.036880 -- Test loss value : 353.629913\n",
      "LOSS at step 640: - Train loss value : 202.778183 -- Test loss value : 353.976318\n",
      "LOSS at step 660: - Train loss value : 252.378174 -- Test loss value : 442.871063\n",
      "LOSS at step 660: - Train loss value : 320.697845 -- Test loss value : 442.783600\n",
      "LOSS at step 680: - Train loss value : 282.963013 -- Test loss value : 386.904266\n",
      "LOSS at step 680: - Train loss value : 172.682297 -- Test loss value : 390.274719\n",
      "LOSS at step 700: - Train loss value : 223.524078 -- Test loss value : 379.076996\n",
      "LOSS at step 700: - Train loss value : 139.422562 -- Test loss value : 376.358978\n",
      "LOSS at step 720: - Train loss value : 305.926178 -- Test loss value : 419.218231\n",
      "LOSS at step 720: - Train loss value : 225.537903 -- Test loss value : 418.279816\n",
      "LOSS at step 740: - Train loss value : 238.605499 -- Test loss value : 385.684143\n",
      "LOSS at step 740: - Train loss value : 211.946152 -- Test loss value : 387.096832\n",
      "LOSS at step 760: - Train loss value : 202.775879 -- Test loss value : 431.485199\n",
      "LOSS at step 760: - Train loss value : 215.126785 -- Test loss value : 435.019318\n",
      "LOSS at step 780: - Train loss value : 220.040985 -- Test loss value : 397.091705\n",
      "LOSS at step 780: - Train loss value : 203.138870 -- Test loss value : 393.479431\n",
      "LOSS at step 800: - Train loss value : 203.905411 -- Test loss value : 419.778992\n",
      "LOSS at step 800: - Train loss value : 330.718628 -- Test loss value : 419.425293\n",
      "LOSS at step 820: - Train loss value : 267.664520 -- Test loss value : 417.948669\n",
      "LOSS at step 820: - Train loss value : 173.860123 -- Test loss value : 420.793793\n",
      "LOSS at step 840: - Train loss value : 150.882874 -- Test loss value : 431.637573\n",
      "LOSS at step 840: - Train loss value : 240.904755 -- Test loss value : 431.412384\n",
      "LOSS at step 860: - Train loss value : 185.059052 -- Test loss value : 430.842285\n",
      "LOSS at step 860: - Train loss value : 315.319214 -- Test loss value : 430.755890\n",
      "LOSS at step 880: - Train loss value : 352.269623 -- Test loss value : 429.156342\n",
      "LOSS at step 880: - Train loss value : 176.427673 -- Test loss value : 425.978333\n",
      "LOSS at step 900: - Train loss value : 250.208069 -- Test loss value : 406.907440\n",
      "LOSS at step 900: - Train loss value : 226.603836 -- Test loss value : 406.576294\n",
      "LOSS at step 920: - Train loss value : 301.473267 -- Test loss value : 400.848816\n",
      "LOSS at step 920: - Train loss value : 227.685516 -- Test loss value : 404.101898\n",
      "LOSS at step 940: - Train loss value : 200.932434 -- Test loss value : 393.099152\n",
      "LOSS at step 940: - Train loss value : 215.050919 -- Test loss value : 391.765228\n",
      "LOSS at step 960: - Train loss value : 317.653259 -- Test loss value : 353.214844\n",
      "LOSS at step 960: - Train loss value : 188.356155 -- Test loss value : 355.086182\n",
      "LOSS at step 980: - Train loss value : 326.516876 -- Test loss value : 370.338989\n",
      "LOSS at step 980: - Train loss value : 202.431595 -- Test loss value : 376.887634\n",
      "LOSS at step 1000: - Train loss value : 203.066071 -- Test loss value : 365.349396\n",
      "LOSS at step 1000: - Train loss value : 201.301956 -- Test loss value : 366.592346\n",
      "LOSS at step 1020: - Train loss value : 214.191574 -- Test loss value : 388.791473\n",
      "LOSS at step 1020: - Train loss value : 168.381088 -- Test loss value : 389.817322\n",
      "LOSS at step 1040: - Train loss value : 264.437531 -- Test loss value : 401.099762\n",
      "LOSS at step 1040: - Train loss value : 206.669159 -- Test loss value : 402.310181\n",
      "LOSS at step 1060: - Train loss value : 168.907043 -- Test loss value : 361.242950\n",
      "LOSS at step 1060: - Train loss value : 243.634720 -- Test loss value : 360.516174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 1080: - Train loss value : 255.095322 -- Test loss value : 379.104034\n",
      "LOSS at step 1080: - Train loss value : 244.914871 -- Test loss value : 378.472260\n",
      "LOSS at step 1100: - Train loss value : 186.303909 -- Test loss value : 366.129272\n",
      "LOSS at step 1100: - Train loss value : 230.126419 -- Test loss value : 366.130859\n",
      "LOSS at step 1120: - Train loss value : 263.565613 -- Test loss value : 405.819458\n",
      "LOSS at step 1120: - Train loss value : 179.826141 -- Test loss value : 407.617920\n",
      "LOSS at step 1140: - Train loss value : 276.055176 -- Test loss value : 377.442139\n",
      "LOSS at step 1140: - Train loss value : 174.595535 -- Test loss value : 373.502777\n",
      "LOSS at step 1160: - Train loss value : 246.531769 -- Test loss value : 371.306305\n",
      "LOSS at step 1160: - Train loss value : 213.242844 -- Test loss value : 368.135345\n",
      "LOSS at step 1180: - Train loss value : 181.663483 -- Test loss value : 352.177826\n",
      "LOSS at step 1180: - Train loss value : 206.509766 -- Test loss value : 354.674500\n",
      "LOSS at step 1200: - Train loss value : 167.019028 -- Test loss value : 366.376404\n",
      "LOSS at step 1200: - Train loss value : 262.078766 -- Test loss value : 366.407318\n",
      "LOSS at step 1220: - Train loss value : 265.789551 -- Test loss value : 385.852844\n",
      "LOSS at step 1220: - Train loss value : 137.320511 -- Test loss value : 385.505035\n",
      "LOSS at step 1240: - Train loss value : 175.522873 -- Test loss value : 350.349091\n",
      "LOSS at step 1240: - Train loss value : 253.857224 -- Test loss value : 352.118622\n",
      "LOSS at step 1260: - Train loss value : 221.935104 -- Test loss value : 373.276154\n",
      "LOSS at step 1260: - Train loss value : 218.305359 -- Test loss value : 375.168488\n",
      "LOSS at step 1280: - Train loss value : 173.093674 -- Test loss value : 415.719849\n",
      "LOSS at step 1280: - Train loss value : 266.866058 -- Test loss value : 413.443634\n",
      "LOSS at step 1300: - Train loss value : 129.913422 -- Test loss value : 374.746735\n",
      "LOSS at step 1300: - Train loss value : 201.861618 -- Test loss value : 374.919983\n",
      "LOSS at step 1320: - Train loss value : 267.821594 -- Test loss value : 368.988251\n",
      "LOSS at step 1320: - Train loss value : 172.389252 -- Test loss value : 366.752625\n",
      "LOSS at step 1340: - Train loss value : 270.479828 -- Test loss value : 382.843872\n",
      "LOSS at step 1340: - Train loss value : 159.301773 -- Test loss value : 384.565948\n",
      "LOSS at step 1360: - Train loss value : 291.868805 -- Test loss value : 373.334381\n",
      "LOSS at step 1360: - Train loss value : 163.502136 -- Test loss value : 371.361206\n",
      "LOSS at step 1380: - Train loss value : 166.426270 -- Test loss value : 359.692017\n",
      "LOSS at step 1380: - Train loss value : 181.696808 -- Test loss value : 359.603729\n",
      "LOSS at step 1400: - Train loss value : 171.178574 -- Test loss value : 368.117767\n",
      "LOSS at step 1400: - Train loss value : 174.951660 -- Test loss value : 367.911530\n",
      "LOSS at step 1420: - Train loss value : 137.318008 -- Test loss value : 352.993744\n",
      "LOSS at step 1420: - Train loss value : 203.140366 -- Test loss value : 349.335541\n",
      "LOSS at step 1440: - Train loss value : 149.643494 -- Test loss value : 353.336304\n",
      "LOSS at step 1440: - Train loss value : 169.084610 -- Test loss value : 354.247253\n",
      "LOSS at step 1460: - Train loss value : 223.477951 -- Test loss value : 341.180267\n",
      "LOSS at step 1460: - Train loss value : 194.437820 -- Test loss value : 343.725647\n",
      "LOSS at step 1480: - Train loss value : 166.972107 -- Test loss value : 377.738220\n",
      "LOSS at step 1480: - Train loss value : 134.740082 -- Test loss value : 373.980835\n",
      "LOSS at step 1500: - Train loss value : 126.580811 -- Test loss value : 373.168671\n",
      "LOSS at step 1500: - Train loss value : 183.542908 -- Test loss value : 373.588409\n",
      "LOSS at step 1520: - Train loss value : 235.766861 -- Test loss value : 351.193115\n",
      "LOSS at step 1520: - Train loss value : 179.464096 -- Test loss value : 346.961975\n",
      "LOSS at step 1540: - Train loss value : 256.699127 -- Test loss value : 339.388977\n",
      "LOSS at step 1540: - Train loss value : 200.258408 -- Test loss value : 339.105927\n",
      "LOSS at step 1560: - Train loss value : 162.676849 -- Test loss value : 338.137878\n",
      "LOSS at step 1560: - Train loss value : 239.864914 -- Test loss value : 336.137360\n",
      "LOSS at step 1580: - Train loss value : 151.691345 -- Test loss value : 359.449402\n",
      "LOSS at step 1580: - Train loss value : 199.709305 -- Test loss value : 363.459229\n",
      "LOSS at step 1600: - Train loss value : 330.335175 -- Test loss value : 326.052673\n",
      "LOSS at step 1600: - Train loss value : 188.032394 -- Test loss value : 323.373230\n",
      "LOSS at step 1620: - Train loss value : 212.560364 -- Test loss value : 349.972290\n",
      "LOSS at step 1620: - Train loss value : 216.250214 -- Test loss value : 345.830261\n",
      "LOSS at step 1640: - Train loss value : 190.647049 -- Test loss value : 348.318695\n",
      "LOSS at step 1640: - Train loss value : 193.667099 -- Test loss value : 348.051056\n",
      "LOSS at step 1660: - Train loss value : 141.920853 -- Test loss value : 338.844299\n",
      "LOSS at step 1660: - Train loss value : 183.887833 -- Test loss value : 340.283081\n",
      "LOSS at step 1680: - Train loss value : 226.280960 -- Test loss value : 343.088776\n",
      "LOSS at step 1680: - Train loss value : 255.070801 -- Test loss value : 345.951355\n",
      "LOSS at step 1700: - Train loss value : 135.404587 -- Test loss value : 333.740021\n",
      "LOSS at step 1700: - Train loss value : 217.694412 -- Test loss value : 335.858673\n",
      "LOSS at step 1720: - Train loss value : 240.333008 -- Test loss value : 336.521149\n",
      "LOSS at step 1720: - Train loss value : 210.459396 -- Test loss value : 336.762115\n",
      "LOSS at step 1740: - Train loss value : 217.325821 -- Test loss value : 310.384827\n",
      "LOSS at step 1740: - Train loss value : 216.151459 -- Test loss value : 309.329163\n",
      "LOSS at step 1760: - Train loss value : 171.570511 -- Test loss value : 330.127625\n",
      "LOSS at step 1760: - Train loss value : 146.340759 -- Test loss value : 332.629486\n",
      "LOSS at step 1780: - Train loss value : 176.576233 -- Test loss value : 338.670563\n",
      "LOSS at step 1780: - Train loss value : 161.023041 -- Test loss value : 335.538849\n",
      "LOSS at step 1800: - Train loss value : 123.910172 -- Test loss value : 312.549164\n",
      "LOSS at step 1800: - Train loss value : 171.616760 -- Test loss value : 307.906494\n",
      "LOSS at step 1820: - Train loss value : 240.880859 -- Test loss value : 322.782440\n",
      "LOSS at step 1820: - Train loss value : 127.385124 -- Test loss value : 321.169861\n",
      "LOSS at step 1840: - Train loss value : 151.932373 -- Test loss value : 313.809937\n",
      "LOSS at step 1840: - Train loss value : 245.870697 -- Test loss value : 316.325256\n",
      "LOSS at step 1860: - Train loss value : 241.267776 -- Test loss value : 317.400848\n",
      "LOSS at step 1860: - Train loss value : 214.690765 -- Test loss value : 315.393677\n",
      "LOSS at step 1880: - Train loss value : 184.215820 -- Test loss value : 356.858673\n",
      "LOSS at step 1880: - Train loss value : 214.783890 -- Test loss value : 354.922821\n",
      "LOSS at step 1900: - Train loss value : 198.455719 -- Test loss value : 319.366974\n",
      "LOSS at step 1900: - Train loss value : 176.147217 -- Test loss value : 317.713440\n",
      "LOSS at step 1920: - Train loss value : 181.667343 -- Test loss value : 311.292908\n",
      "LOSS at step 1920: - Train loss value : 185.451523 -- Test loss value : 307.739288\n",
      "LOSS at step 1940: - Train loss value : 218.302704 -- Test loss value : 299.566986\n",
      "LOSS at step 1940: - Train loss value : 169.707031 -- Test loss value : 301.530457\n",
      "LOSS at step 1960: - Train loss value : 160.507065 -- Test loss value : 308.117096\n",
      "LOSS at step 1960: - Train loss value : 184.450653 -- Test loss value : 310.131897\n",
      "LOSS at step 1980: - Train loss value : 185.105896 -- Test loss value : 333.186676\n",
      "LOSS at step 1980: - Train loss value : 205.225555 -- Test loss value : 329.678802\n",
      "LOSS at step 2000: - Train loss value : 186.360855 -- Test loss value : 333.769440\n",
      "LOSS at step 2000: - Train loss value : 248.780594 -- Test loss value : 335.172028\n",
      "LOSS at step 2020: - Train loss value : 182.996597 -- Test loss value : 336.667236\n",
      "LOSS at step 2020: - Train loss value : 196.062363 -- Test loss value : 334.918732\n",
      "LOSS at step 2040: - Train loss value : 210.586349 -- Test loss value : 338.724304\n",
      "LOSS at step 2040: - Train loss value : 147.459564 -- Test loss value : 340.199554\n",
      "LOSS at step 2060: - Train loss value : 264.478516 -- Test loss value : 309.926880\n",
      "LOSS at step 2060: - Train loss value : 85.214371 -- Test loss value : 313.638641\n",
      "LOSS at step 2080: - Train loss value : 191.883072 -- Test loss value : 311.450378\n",
      "LOSS at step 2080: - Train loss value : 120.403282 -- Test loss value : 310.848022\n",
      "LOSS at step 2100: - Train loss value : 137.438828 -- Test loss value : 308.460938\n",
      "LOSS at step 2100: - Train loss value : 248.925919 -- Test loss value : 310.327209\n",
      "LOSS at step 2120: - Train loss value : 135.305954 -- Test loss value : 333.668304\n",
      "LOSS at step 2120: - Train loss value : 161.394073 -- Test loss value : 331.483276\n",
      "LOSS at step 2140: - Train loss value : 190.294922 -- Test loss value : 300.595947\n",
      "LOSS at step 2140: - Train loss value : 172.138458 -- Test loss value : 298.311127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 2160: - Train loss value : 237.511261 -- Test loss value : 336.975250\n",
      "LOSS at step 2160: - Train loss value : 219.205093 -- Test loss value : 339.701019\n",
      "LOSS at step 2180: - Train loss value : 144.460419 -- Test loss value : 325.639526\n",
      "LOSS at step 2180: - Train loss value : 164.438019 -- Test loss value : 326.488159\n",
      "LOSS at step 2200: - Train loss value : 173.937042 -- Test loss value : 314.235626\n",
      "LOSS at step 2200: - Train loss value : 207.428360 -- Test loss value : 314.705627\n",
      "LOSS at step 2220: - Train loss value : 227.205231 -- Test loss value : 331.454132\n",
      "LOSS at step 2220: - Train loss value : 239.076004 -- Test loss value : 334.844543\n",
      "LOSS at step 2240: - Train loss value : 151.558899 -- Test loss value : 338.336548\n",
      "LOSS at step 2240: - Train loss value : 267.319672 -- Test loss value : 339.887909\n",
      "LOSS at step 2260: - Train loss value : 202.506638 -- Test loss value : 337.653259\n",
      "LOSS at step 2260: - Train loss value : 125.393753 -- Test loss value : 339.260681\n",
      "LOSS at step 2280: - Train loss value : 173.053894 -- Test loss value : 313.457764\n",
      "LOSS at step 2280: - Train loss value : 170.883331 -- Test loss value : 314.727325\n",
      "LOSS at step 2300: - Train loss value : 170.642029 -- Test loss value : 339.747681\n",
      "LOSS at step 2300: - Train loss value : 198.364075 -- Test loss value : 340.957916\n",
      "LOSS at step 2320: - Train loss value : 188.849213 -- Test loss value : 362.385864\n",
      "LOSS at step 2320: - Train loss value : 139.650360 -- Test loss value : 363.898590\n",
      "LOSS at step 2340: - Train loss value : 244.409836 -- Test loss value : 303.060059\n",
      "LOSS at step 2340: - Train loss value : 174.844788 -- Test loss value : 297.697754\n",
      "LOSS at step 2360: - Train loss value : 157.472626 -- Test loss value : 334.565765\n",
      "LOSS at step 2360: - Train loss value : 132.468292 -- Test loss value : 334.663055\n",
      "LOSS at step 2380: - Train loss value : 193.388535 -- Test loss value : 322.396545\n",
      "LOSS at step 2380: - Train loss value : 178.806732 -- Test loss value : 324.397247\n",
      "LOSS at step 2400: - Train loss value : 168.912079 -- Test loss value : 309.757660\n",
      "LOSS at step 2400: - Train loss value : 206.496994 -- Test loss value : 312.683624\n",
      "LOSS at step 2420: - Train loss value : 123.673538 -- Test loss value : 318.278198\n",
      "LOSS at step 2420: - Train loss value : 204.867050 -- Test loss value : 320.726593\n",
      "LOSS at step 2440: - Train loss value : 157.463211 -- Test loss value : 340.896790\n",
      "LOSS at step 2440: - Train loss value : 199.725815 -- Test loss value : 340.966370\n",
      "LOSS at step 2460: - Train loss value : 166.095474 -- Test loss value : 323.602356\n",
      "LOSS at step 2460: - Train loss value : 182.742859 -- Test loss value : 324.834106\n",
      "LOSS at step 2480: - Train loss value : 211.454117 -- Test loss value : 324.975403\n",
      "LOSS at step 2480: - Train loss value : 152.764435 -- Test loss value : 330.633911\n",
      "LOSS at step 2500: - Train loss value : 158.596710 -- Test loss value : 294.234711\n",
      "LOSS at step 2500: - Train loss value : 179.615463 -- Test loss value : 295.505829\n",
      "LOSS at step 2520: - Train loss value : 199.020767 -- Test loss value : 285.268036\n",
      "LOSS at step 2520: - Train loss value : 132.493759 -- Test loss value : 287.073853\n",
      "LOSS at step 2540: - Train loss value : 175.865662 -- Test loss value : 312.347839\n",
      "LOSS at step 2540: - Train loss value : 140.311707 -- Test loss value : 314.414154\n",
      "LOSS at step 2560: - Train loss value : 158.294174 -- Test loss value : 343.319611\n",
      "LOSS at step 2560: - Train loss value : 234.406601 -- Test loss value : 345.665833\n",
      "LOSS at step 2580: - Train loss value : 205.826309 -- Test loss value : 296.000183\n",
      "LOSS at step 2580: - Train loss value : 182.102936 -- Test loss value : 293.087494\n",
      "LOSS at step 2600: - Train loss value : 220.063812 -- Test loss value : 312.886627\n",
      "LOSS at step 2600: - Train loss value : 201.543640 -- Test loss value : 311.056549\n",
      "LOSS at step 2620: - Train loss value : 175.447556 -- Test loss value : 331.349457\n",
      "LOSS at step 2620: - Train loss value : 151.198059 -- Test loss value : 332.221893\n",
      "LOSS at step 2640: - Train loss value : 209.547836 -- Test loss value : 355.175629\n",
      "LOSS at step 2640: - Train loss value : 125.523506 -- Test loss value : 355.519409\n",
      "LOSS at step 2660: - Train loss value : 152.038284 -- Test loss value : 324.589966\n",
      "LOSS at step 2660: - Train loss value : 181.760300 -- Test loss value : 322.425293\n",
      "LOSS at step 2680: - Train loss value : 197.371780 -- Test loss value : 322.035309\n",
      "LOSS at step 2680: - Train loss value : 177.915115 -- Test loss value : 319.756805\n",
      "LOSS at step 2700: - Train loss value : 147.169632 -- Test loss value : 307.909576\n",
      "LOSS at step 2700: - Train loss value : 241.668884 -- Test loss value : 309.496948\n",
      "LOSS at step 2720: - Train loss value : 123.228203 -- Test loss value : 325.237610\n",
      "LOSS at step 2720: - Train loss value : 207.486130 -- Test loss value : 322.327026\n",
      "LOSS at step 2740: - Train loss value : 149.024155 -- Test loss value : 328.276825\n",
      "LOSS at step 2740: - Train loss value : 132.327072 -- Test loss value : 325.582306\n",
      "LOSS at step 2760: - Train loss value : 194.358994 -- Test loss value : 333.681366\n",
      "LOSS at step 2760: - Train loss value : 170.585419 -- Test loss value : 333.112396\n",
      "LOSS at step 2780: - Train loss value : 177.535355 -- Test loss value : 298.349731\n",
      "LOSS at step 2780: - Train loss value : 196.730988 -- Test loss value : 302.230621\n",
      "LOSS at step 2800: - Train loss value : 109.405975 -- Test loss value : 315.497009\n",
      "LOSS at step 2800: - Train loss value : 159.519531 -- Test loss value : 318.646301\n",
      "LOSS at step 2820: - Train loss value : 101.419731 -- Test loss value : 297.606293\n",
      "LOSS at step 2820: - Train loss value : 167.083466 -- Test loss value : 300.211151\n",
      "LOSS at step 2840: - Train loss value : 103.335197 -- Test loss value : 278.994293\n",
      "LOSS at step 2840: - Train loss value : 203.626373 -- Test loss value : 284.785614\n",
      "LOSS at step 2860: - Train loss value : 233.683029 -- Test loss value : 297.958496\n",
      "LOSS at step 2860: - Train loss value : 160.661652 -- Test loss value : 295.688110\n",
      "LOSS at step 2880: - Train loss value : 161.534958 -- Test loss value : 322.575104\n",
      "LOSS at step 2880: - Train loss value : 159.418488 -- Test loss value : 324.123077\n",
      "LOSS at step 2900: - Train loss value : 151.693649 -- Test loss value : 285.995361\n",
      "LOSS at step 2900: - Train loss value : 132.615219 -- Test loss value : 286.731873\n",
      "LOSS at step 2920: - Train loss value : 173.523346 -- Test loss value : 313.378296\n",
      "LOSS at step 2920: - Train loss value : 152.919891 -- Test loss value : 311.424988\n",
      "LOSS at step 2940: - Train loss value : 183.158859 -- Test loss value : 263.532013\n",
      "LOSS at step 2940: - Train loss value : 154.881027 -- Test loss value : 260.235413\n",
      "LOSS at step 2960: - Train loss value : 109.396675 -- Test loss value : 290.100067\n",
      "LOSS at step 2960: - Train loss value : 175.950851 -- Test loss value : 289.626221\n",
      "LOSS at step 2980: - Train loss value : 144.890823 -- Test loss value : 300.962860\n",
      "LOSS at step 2980: - Train loss value : 189.795090 -- Test loss value : 300.471649\n",
      "LOSS at step 3000: - Train loss value : 112.179771 -- Test loss value : 278.870514\n",
      "LOSS at step 3000: - Train loss value : 145.086334 -- Test loss value : 277.693817\n",
      "LOSS at step 3020: - Train loss value : 148.705139 -- Test loss value : 293.318787\n",
      "LOSS at step 3020: - Train loss value : 190.293259 -- Test loss value : 293.836426\n",
      "LOSS at step 3040: - Train loss value : 151.619400 -- Test loss value : 319.446930\n",
      "LOSS at step 3040: - Train loss value : 190.900375 -- Test loss value : 318.182007\n",
      "LOSS at step 3060: - Train loss value : 165.210754 -- Test loss value : 304.377380\n",
      "LOSS at step 3060: - Train loss value : 165.839798 -- Test loss value : 300.417023\n",
      "LOSS at step 3080: - Train loss value : 172.370575 -- Test loss value : 339.524261\n",
      "LOSS at step 3080: - Train loss value : 206.162048 -- Test loss value : 342.778229\n",
      "LOSS at step 3100: - Train loss value : 125.210381 -- Test loss value : 301.822052\n",
      "LOSS at step 3100: - Train loss value : 142.686829 -- Test loss value : 299.553925\n",
      "LOSS at step 3120: - Train loss value : 173.231934 -- Test loss value : 264.971985\n",
      "LOSS at step 3120: - Train loss value : 167.301041 -- Test loss value : 266.827728\n",
      "LOSS at step 3140: - Train loss value : 139.359573 -- Test loss value : 249.768082\n",
      "LOSS at step 3140: - Train loss value : 181.829193 -- Test loss value : 247.784973\n",
      "LOSS at step 3160: - Train loss value : 188.859818 -- Test loss value : 248.067917\n",
      "LOSS at step 3160: - Train loss value : 187.032547 -- Test loss value : 244.391922\n",
      "LOSS at step 3180: - Train loss value : 148.657578 -- Test loss value : 291.892242\n",
      "LOSS at step 3180: - Train loss value : 216.579025 -- Test loss value : 294.075287\n",
      "LOSS at step 3200: - Train loss value : 153.783112 -- Test loss value : 293.936066\n",
      "LOSS at step 3200: - Train loss value : 121.269661 -- Test loss value : 293.889893\n",
      "LOSS at step 3220: - Train loss value : 123.109604 -- Test loss value : 320.989746\n",
      "LOSS at step 3220: - Train loss value : 126.814621 -- Test loss value : 320.190887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 3240: - Train loss value : 157.544571 -- Test loss value : 289.376343\n",
      "LOSS at step 3240: - Train loss value : 201.407745 -- Test loss value : 290.674194\n",
      "LOSS at step 3260: - Train loss value : 126.109375 -- Test loss value : 267.680420\n",
      "LOSS at step 3260: - Train loss value : 195.024109 -- Test loss value : 268.484985\n",
      "LOSS at step 3280: - Train loss value : 156.598724 -- Test loss value : 278.105835\n",
      "LOSS at step 3280: - Train loss value : 161.956497 -- Test loss value : 275.824982\n",
      "LOSS at step 3300: - Train loss value : 147.738739 -- Test loss value : 303.748810\n",
      "LOSS at step 3300: - Train loss value : 171.856522 -- Test loss value : 304.495209\n",
      "LOSS at step 3320: - Train loss value : 175.507614 -- Test loss value : 288.736206\n",
      "LOSS at step 3320: - Train loss value : 146.347702 -- Test loss value : 290.512726\n",
      "LOSS at step 3340: - Train loss value : 121.478630 -- Test loss value : 297.834320\n",
      "LOSS at step 3340: - Train loss value : 107.974953 -- Test loss value : 298.070892\n",
      "LOSS at step 3360: - Train loss value : 161.370209 -- Test loss value : 292.964142\n",
      "LOSS at step 3360: - Train loss value : 267.996277 -- Test loss value : 290.493835\n",
      "LOSS at step 3380: - Train loss value : 147.077469 -- Test loss value : 303.710724\n",
      "LOSS at step 3380: - Train loss value : 168.532135 -- Test loss value : 301.112976\n",
      "LOSS at step 3400: - Train loss value : 136.515411 -- Test loss value : 295.338043\n",
      "LOSS at step 3400: - Train loss value : 142.611008 -- Test loss value : 293.413879\n",
      "LOSS at step 3420: - Train loss value : 130.555801 -- Test loss value : 272.383301\n",
      "LOSS at step 3420: - Train loss value : 165.833023 -- Test loss value : 277.749084\n",
      "LOSS at step 3440: - Train loss value : 130.414795 -- Test loss value : 275.794495\n",
      "LOSS at step 3440: - Train loss value : 101.231827 -- Test loss value : 277.182495\n",
      "LOSS at step 3460: - Train loss value : 132.239685 -- Test loss value : 302.480988\n",
      "LOSS at step 3460: - Train loss value : 122.232201 -- Test loss value : 302.422882\n",
      "LOSS at step 3480: - Train loss value : 129.361313 -- Test loss value : 292.571350\n",
      "LOSS at step 3480: - Train loss value : 183.090988 -- Test loss value : 295.178162\n",
      "LOSS at step 3500: - Train loss value : 212.023361 -- Test loss value : 298.112732\n",
      "LOSS at step 3500: - Train loss value : 121.246780 -- Test loss value : 300.540527\n",
      "LOSS at step 3520: - Train loss value : 155.213348 -- Test loss value : 289.244873\n",
      "LOSS at step 3520: - Train loss value : 129.773117 -- Test loss value : 288.894928\n",
      "LOSS at step 3540: - Train loss value : 145.277084 -- Test loss value : 258.641479\n",
      "LOSS at step 3540: - Train loss value : 210.211426 -- Test loss value : 257.464966\n",
      "LOSS at step 3560: - Train loss value : 91.273720 -- Test loss value : 298.170410\n",
      "LOSS at step 3560: - Train loss value : 163.186981 -- Test loss value : 294.467316\n",
      "LOSS at step 3580: - Train loss value : 142.846252 -- Test loss value : 300.846344\n",
      "LOSS at step 3580: - Train loss value : 124.836189 -- Test loss value : 300.599915\n",
      "LOSS at step 3600: - Train loss value : 117.685158 -- Test loss value : 296.862854\n",
      "LOSS at step 3600: - Train loss value : 155.421127 -- Test loss value : 291.607574\n",
      "LOSS at step 3620: - Train loss value : 177.157883 -- Test loss value : 280.255066\n",
      "LOSS at step 3620: - Train loss value : 171.859146 -- Test loss value : 282.894531\n",
      "LOSS at step 3640: - Train loss value : 159.676727 -- Test loss value : 283.366089\n",
      "LOSS at step 3640: - Train loss value : 165.947052 -- Test loss value : 280.195892\n",
      "LOSS at step 3660: - Train loss value : 156.705505 -- Test loss value : 307.855438\n",
      "LOSS at step 3660: - Train loss value : 165.719147 -- Test loss value : 306.058990\n",
      "LOSS at step 3680: - Train loss value : 190.504745 -- Test loss value : 282.089386\n",
      "LOSS at step 3680: - Train loss value : 145.972580 -- Test loss value : 281.080750\n",
      "LOSS at step 3700: - Train loss value : 167.043396 -- Test loss value : 259.354401\n",
      "LOSS at step 3700: - Train loss value : 171.266312 -- Test loss value : 261.402283\n",
      "LOSS at step 3720: - Train loss value : 139.544830 -- Test loss value : 271.962677\n",
      "LOSS at step 3720: - Train loss value : 158.138580 -- Test loss value : 268.172729\n",
      "LOSS at step 3740: - Train loss value : 145.999680 -- Test loss value : 281.500275\n",
      "LOSS at step 3740: - Train loss value : 173.358780 -- Test loss value : 279.666656\n",
      "LOSS at step 3760: - Train loss value : 123.710533 -- Test loss value : 303.281433\n",
      "LOSS at step 3760: - Train loss value : 125.577003 -- Test loss value : 303.429810\n",
      "LOSS at step 3780: - Train loss value : 115.599541 -- Test loss value : 303.893860\n",
      "LOSS at step 3780: - Train loss value : 123.104095 -- Test loss value : 304.919739\n",
      "LOSS at step 3800: - Train loss value : 153.128143 -- Test loss value : 291.347870\n",
      "LOSS at step 3800: - Train loss value : 87.107582 -- Test loss value : 290.116425\n",
      "LOSS at step 3820: - Train loss value : 136.432480 -- Test loss value : 289.499298\n",
      "LOSS at step 3820: - Train loss value : 123.803535 -- Test loss value : 293.843018\n",
      "LOSS at step 3840: - Train loss value : 112.127510 -- Test loss value : 261.273560\n",
      "LOSS at step 3840: - Train loss value : 198.424515 -- Test loss value : 264.666656\n",
      "LOSS at step 3860: - Train loss value : 131.344406 -- Test loss value : 271.519409\n",
      "LOSS at step 3860: - Train loss value : 153.630829 -- Test loss value : 276.426849\n",
      "LOSS at step 3880: - Train loss value : 125.675568 -- Test loss value : 286.205048\n",
      "LOSS at step 3880: - Train loss value : 149.240692 -- Test loss value : 287.022125\n",
      "LOSS at step 3900: - Train loss value : 216.658783 -- Test loss value : 263.419556\n",
      "LOSS at step 3900: - Train loss value : 136.423752 -- Test loss value : 260.818909\n",
      "LOSS at step 3920: - Train loss value : 215.916214 -- Test loss value : 283.452515\n",
      "LOSS at step 3920: - Train loss value : 122.185890 -- Test loss value : 277.657379\n",
      "LOSS at step 3940: - Train loss value : 185.040359 -- Test loss value : 295.374298\n",
      "LOSS at step 3940: - Train loss value : 130.790588 -- Test loss value : 297.426849\n",
      "LOSS at step 3960: - Train loss value : 162.274582 -- Test loss value : 275.330597\n",
      "LOSS at step 3960: - Train loss value : 164.294296 -- Test loss value : 270.803009\n",
      "LOSS at step 3980: - Train loss value : 118.641685 -- Test loss value : 277.189636\n",
      "LOSS at step 3980: - Train loss value : 142.334915 -- Test loss value : 279.914429\n",
      "LOSS at step 4000: - Train loss value : 121.698593 -- Test loss value : 262.260864\n",
      "LOSS at step 4000: - Train loss value : 162.010345 -- Test loss value : 261.517273\n",
      "LOSS at step 4020: - Train loss value : 174.398956 -- Test loss value : 280.065979\n",
      "LOSS at step 4020: - Train loss value : 182.031219 -- Test loss value : 279.595795\n",
      "LOSS at step 4040: - Train loss value : 145.562363 -- Test loss value : 289.015808\n",
      "LOSS at step 4040: - Train loss value : 183.039001 -- Test loss value : 287.915955\n",
      "LOSS at step 4060: - Train loss value : 137.417984 -- Test loss value : 263.984619\n",
      "LOSS at step 4060: - Train loss value : 119.709648 -- Test loss value : 260.010010\n",
      "LOSS at step 4080: - Train loss value : 117.560905 -- Test loss value : 212.285919\n",
      "LOSS at step 4080: - Train loss value : 198.761261 -- Test loss value : 216.965683\n",
      "LOSS at step 4100: - Train loss value : 89.179054 -- Test loss value : 258.007416\n",
      "LOSS at step 4100: - Train loss value : 210.311295 -- Test loss value : 258.600952\n",
      "LOSS at step 4120: - Train loss value : 123.460739 -- Test loss value : 262.629272\n",
      "LOSS at step 4120: - Train loss value : 128.718246 -- Test loss value : 259.134033\n",
      "LOSS at step 4140: - Train loss value : 163.108109 -- Test loss value : 247.824615\n",
      "LOSS at step 4140: - Train loss value : 153.386337 -- Test loss value : 250.521820\n",
      "LOSS at step 4160: - Train loss value : 133.259659 -- Test loss value : 266.825226\n",
      "LOSS at step 4160: - Train loss value : 129.906158 -- Test loss value : 267.110229\n",
      "LOSS at step 4180: - Train loss value : 168.409882 -- Test loss value : 263.490082\n",
      "LOSS at step 4180: - Train loss value : 224.338348 -- Test loss value : 264.762146\n",
      "LOSS at step 4200: - Train loss value : 178.402695 -- Test loss value : 281.540100\n",
      "LOSS at step 4200: - Train loss value : 157.095673 -- Test loss value : 280.636688\n",
      "LOSS at step 4220: - Train loss value : 122.132179 -- Test loss value : 272.493561\n",
      "LOSS at step 4220: - Train loss value : 131.471558 -- Test loss value : 271.308472\n",
      "LOSS at step 4240: - Train loss value : 174.402634 -- Test loss value : 261.740143\n",
      "LOSS at step 4240: - Train loss value : 157.261887 -- Test loss value : 264.082428\n",
      "LOSS at step 4260: - Train loss value : 122.115608 -- Test loss value : 266.427582\n",
      "LOSS at step 4260: - Train loss value : 165.290573 -- Test loss value : 268.154663\n",
      "LOSS at step 4280: - Train loss value : 112.132668 -- Test loss value : 263.101624\n",
      "LOSS at step 4280: - Train loss value : 138.851501 -- Test loss value : 259.575836\n",
      "LOSS at step 4300: - Train loss value : 178.831940 -- Test loss value : 276.329437\n",
      "LOSS at step 4300: - Train loss value : 120.584785 -- Test loss value : 279.084259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 4320: - Train loss value : 137.254898 -- Test loss value : 272.287201\n",
      "LOSS at step 4320: - Train loss value : 198.909271 -- Test loss value : 270.776886\n",
      "LOSS at step 4340: - Train loss value : 142.749542 -- Test loss value : 285.122742\n",
      "LOSS at step 4340: - Train loss value : 133.385025 -- Test loss value : 282.827057\n",
      "LOSS at step 4360: - Train loss value : 95.156624 -- Test loss value : 247.081345\n",
      "LOSS at step 4360: - Train loss value : 145.755371 -- Test loss value : 245.603226\n",
      "LOSS at step 4380: - Train loss value : 247.336258 -- Test loss value : 255.613754\n",
      "LOSS at step 4380: - Train loss value : 87.587860 -- Test loss value : 249.159714\n",
      "LOSS at step 4400: - Train loss value : 184.719925 -- Test loss value : 266.529358\n",
      "LOSS at step 4400: - Train loss value : 142.056152 -- Test loss value : 270.095337\n",
      "LOSS at step 4420: - Train loss value : 190.692322 -- Test loss value : 263.735443\n",
      "LOSS at step 4420: - Train loss value : 140.530701 -- Test loss value : 261.543213\n",
      "LOSS at step 4440: - Train loss value : 113.229370 -- Test loss value : 293.599579\n",
      "LOSS at step 4440: - Train loss value : 110.591438 -- Test loss value : 289.106781\n",
      "LOSS at step 4460: - Train loss value : 163.715088 -- Test loss value : 257.846710\n",
      "LOSS at step 4460: - Train loss value : 197.914612 -- Test loss value : 260.330933\n",
      "LOSS at step 4480: - Train loss value : 131.873230 -- Test loss value : 245.432449\n",
      "LOSS at step 4480: - Train loss value : 135.557816 -- Test loss value : 247.517075\n",
      "LOSS at step 4500: - Train loss value : 114.251381 -- Test loss value : 227.591553\n",
      "LOSS at step 4500: - Train loss value : 208.609833 -- Test loss value : 230.629379\n",
      "LOSS at step 4520: - Train loss value : 117.784637 -- Test loss value : 278.284973\n",
      "LOSS at step 4520: - Train loss value : 104.700333 -- Test loss value : 279.224182\n",
      "LOSS at step 4540: - Train loss value : 113.621246 -- Test loss value : 264.022797\n",
      "LOSS at step 4540: - Train loss value : 120.526192 -- Test loss value : 266.545868\n",
      "LOSS at step 4560: - Train loss value : 117.798637 -- Test loss value : 249.420929\n",
      "LOSS at step 4560: - Train loss value : 129.197861 -- Test loss value : 243.990570\n",
      "LOSS at step 4580: - Train loss value : 171.413574 -- Test loss value : 245.100998\n",
      "LOSS at step 4580: - Train loss value : 129.194580 -- Test loss value : 243.422348\n",
      "LOSS at step 4600: - Train loss value : 127.586311 -- Test loss value : 246.205948\n",
      "LOSS at step 4600: - Train loss value : 121.076042 -- Test loss value : 249.626892\n",
      "LOSS at step 4620: - Train loss value : 94.306320 -- Test loss value : 260.008514\n",
      "LOSS at step 4620: - Train loss value : 230.475784 -- Test loss value : 261.687225\n",
      "LOSS at step 4640: - Train loss value : 138.747040 -- Test loss value : 255.638351\n",
      "LOSS at step 4640: - Train loss value : 134.328232 -- Test loss value : 256.556183\n",
      "LOSS at step 4660: - Train loss value : 195.763474 -- Test loss value : 307.211853\n",
      "LOSS at step 4660: - Train loss value : 127.784035 -- Test loss value : 305.000458\n",
      "LOSS at step 4680: - Train loss value : 165.675751 -- Test loss value : 281.472565\n",
      "LOSS at step 4680: - Train loss value : 147.638870 -- Test loss value : 276.264801\n",
      "LOSS at step 4700: - Train loss value : 139.327866 -- Test loss value : 232.848053\n",
      "LOSS at step 4700: - Train loss value : 146.847885 -- Test loss value : 230.619156\n",
      "LOSS at step 4720: - Train loss value : 98.134346 -- Test loss value : 220.661865\n",
      "LOSS at step 4720: - Train loss value : 132.150696 -- Test loss value : 223.545425\n",
      "LOSS at step 4740: - Train loss value : 131.446976 -- Test loss value : 253.613083\n",
      "LOSS at step 4740: - Train loss value : 123.277977 -- Test loss value : 250.188278\n",
      "LOSS at step 4760: - Train loss value : 147.847168 -- Test loss value : 254.742477\n",
      "LOSS at step 4760: - Train loss value : 108.664711 -- Test loss value : 255.511307\n",
      "LOSS at step 4780: - Train loss value : 165.136520 -- Test loss value : 268.791656\n",
      "LOSS at step 4780: - Train loss value : 127.601265 -- Test loss value : 269.046570\n",
      "LOSS at step 4800: - Train loss value : 132.809738 -- Test loss value : 227.296997\n",
      "LOSS at step 4800: - Train loss value : 157.935837 -- Test loss value : 225.867874\n",
      "LOSS at step 4820: - Train loss value : 90.163177 -- Test loss value : 235.595322\n",
      "LOSS at step 4820: - Train loss value : 151.537796 -- Test loss value : 234.945023\n",
      "LOSS at step 4840: - Train loss value : 141.733002 -- Test loss value : 236.469162\n",
      "LOSS at step 4840: - Train loss value : 151.172226 -- Test loss value : 237.360962\n",
      "LOSS at step 4860: - Train loss value : 171.136124 -- Test loss value : 287.641266\n",
      "LOSS at step 4860: - Train loss value : 108.616508 -- Test loss value : 280.575409\n",
      "LOSS at step 4880: - Train loss value : 104.863800 -- Test loss value : 248.076935\n",
      "LOSS at step 4880: - Train loss value : 92.542694 -- Test loss value : 243.666931\n",
      "LOSS at step 4900: - Train loss value : 103.434479 -- Test loss value : 259.007599\n",
      "LOSS at step 4900: - Train loss value : 149.222610 -- Test loss value : 256.619690\n",
      "LOSS at step 4920: - Train loss value : 195.050613 -- Test loss value : 269.173767\n",
      "LOSS at step 4920: - Train loss value : 177.860184 -- Test loss value : 272.535675\n",
      "LOSS at step 4940: - Train loss value : 120.802689 -- Test loss value : 280.173523\n",
      "LOSS at step 4940: - Train loss value : 64.031357 -- Test loss value : 279.509003\n",
      "LOSS at step 4960: - Train loss value : 134.669174 -- Test loss value : 242.567795\n",
      "LOSS at step 4960: - Train loss value : 146.148865 -- Test loss value : 240.469635\n",
      "LOSS at step 4980: - Train loss value : 144.589417 -- Test loss value : 272.858948\n",
      "LOSS at step 4980: - Train loss value : 103.491600 -- Test loss value : 269.500427\n"
     ]
    }
   ],
   "source": [
    "#ssh sydney\n",
    "#setPython3 \n",
    "\n",
    "import numpy \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "data = arff.loadarff('slump.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "covariables = df.iloc[:,0:7].values\n",
    "response = df.iloc[:,7:10].values\n",
    "positions = np.arange(103)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "covariables_train, covariables_test, response_train, response_test,positions_train,positions_test = train_test_split(covariables, response,positions, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "##APPRENTISSAGE\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = numpy.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = numpy.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = numpy.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import data\n",
    "# Tensorflow is finicky about shapes, so resize\n",
    "\n",
    "\n",
    "X_datatrain, X_datatest, targets_train, targets_test = covariables_train, covariables_test, response_train, response_test\n",
    "\n",
    "#les dimensions de X for training\n",
    "NbrLignes_train = X_datatrain.shape[0]  ###\n",
    "NbrColonnes_train= X_datatrain.shape[1] ###\n",
    "\n",
    "#les dimensions de X for testing\n",
    "NbrLignes_test = X_datatest.shape[0]  ###\n",
    "NbrColonnes_test = X_datatest.shape[1] ###\n",
    "#Y_data = targets \n",
    "\n",
    "Y_datatrain = tf.reshape(targets_train, [NbrLignes_train,3])\n",
    "Y_datatest = tf.reshape(targets_test, [NbrLignes_test,3])\n",
    "\n",
    "#build the model\n",
    "\n",
    "# input X: batch_size x NbrColonnes, the first dimension (None) will index the data in the mini-batch\n",
    "Xfill = tf.placeholder(tf.float32, shape= [None, NbrColonnes_train])\n",
    "# correct answers will go here\n",
    "Yfill = tf.placeholder(tf.float32, shape=[None, 3]) \n",
    "#Let's add some layers\n",
    "\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "L = 200\n",
    "M = 160\n",
    "N = 100\n",
    "O = 30\n",
    "# Weights initialised with small random values between -0.2 and +0.2\n",
    "# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "W1 = tf.Variable(tf.truncated_normal([NbrColonnes_train, L], stddev=0.1))  # \n",
    "B1 = tf.Variable(tf.ones([L])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([M])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([N])/10)\n",
    "W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([O])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([O, 3], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# The model, with dropout at each layer\n",
    "\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(Xfill, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "Y_ = tf.matmul(Y4d, W5) + B5\n",
    "\n",
    "cross_entropy = tf.losses.mean_squared_error(Yfill, Y_)\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "#Modification de l'accuracy \n",
    "\n",
    "#calcul de l'accuracy, une nouvelle manire: :  \n",
    "\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy) \n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 100.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# Gradient descent loop for 500 steps\n",
    "for i in range(5000):\n",
    " # Select random minibatch\n",
    "    j = 1\n",
    "    ##### shuffling our data::: really important\n",
    "    X_datasf, Y_datasf = shuffle_in_unison(X_datatrain, Y_datatrain.eval())\n",
    "    \n",
    "    ##### defining a decreasing learning rate: \n",
    "    # learning rate decay\n",
    "    learning_rate = 0.0001001329829992624\n",
    "    #learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    \n",
    "    ##### \n",
    "    for start, end in zip(range(0, NbrLignes_train, 52), range(20, NbrLignes_train, 52)):\n",
    "\n",
    "             X_batch,Y_batch = X_datasf[start:end,],Y_datasf[start:end,]\n",
    "             j +=1\n",
    "             # Do gradient descent step\n",
    "             _, loss_val_train = sess.run([train_step, cross_entropy], feed_dict={Xfill: X_batch, Yfill: Y_batch, pkeep: 0.75, lr: learning_rate})\n",
    "             if i %20==0:    \n",
    "                 \n",
    "             \n",
    "                #if j%20==0:\n",
    "                 loss_val_test = sess.run(cross_entropy, feed_dict={Xfill: X_datatest, Yfill: Y_datatest.eval(), pkeep: 1.0, lr:  learning_rate})\n",
    "                 train_loss.append(loss_val_train)\n",
    "                 \n",
    "                 test_loss.append(loss_val_test)\n",
    "                 print('LOSS at step %s: - Train loss value : %f -- Test loss value : %f' % (i, loss_val_train,loss_val_test))\n",
    "#print(test_loss)\n",
    "#print(test_loss)\n",
    "#LOSS at step 420: - Train loss value : 22.385496 -- Test loss value : 9.721767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us compute the aRRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001001329829992624\n",
    "\n",
    "Y_testpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatest, tf.float32).eval(), Yfill: tf.cast(Y_datatest, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n",
    "#numpy.savetxt('Y_testpred_with_stock_%s_and_ai_%s_modelcut_%s_vartimeframe_%s.txt'%(stock, ai, cut, vartimeframe), Y_testpred, fmt='%f')\n",
    "Y_trainpred = Y_.eval(feed_dict={Xfill: tf.cast(X_datatrain, tf.float32).eval(), Yfill: tf.cast(Y_datatrain, tf.float32).eval(), pkeep: 1.0, lr: learning_rate})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.296146 , 34.893208 , 30.414185 ],\n",
       "       [11.781056 , 32.261723 , 24.179066 ],\n",
       "       [13.292901 , 34.60828  , 17.925142 ],\n",
       "       [10.104949 , 29.140244 , 26.763617 ],\n",
       "       [11.720647 , 31.629486 , 20.722048 ],\n",
       "       [12.233914 , 33.004753 , 21.418697 ],\n",
       "       [ 4.7921243, 15.741513 , 25.799324 ],\n",
       "       [13.029567 , 34.03006  , 18.372917 ],\n",
       "       [16.654474 , 43.550407 , 21.774126 ],\n",
       "       [14.964795 , 39.756718 , 24.234705 ],\n",
       "       [ 8.181367 , 24.92323  , 27.531197 ],\n",
       "       [19.602608 , 52.52906  , 33.022785 ],\n",
       "       [20.79553  , 55.292828 , 33.03003  ],\n",
       "       [10.144857 , 27.216574 , 19.2749   ],\n",
       "       [10.987417 , 29.162989 , 18.494675 ],\n",
       "       [15.787713 , 41.20526  , 20.786592 ],\n",
       "       [13.110942 , 34.30029  , 19.419786 ],\n",
       "       [13.622944 , 36.826923 , 25.099596 ],\n",
       "       [17.283867 , 45.213917 , 22.724628 ],\n",
       "       [15.626089 , 42.194233 , 28.253662 ],\n",
       "       [ 6.7807565, 21.777218 , 34.36345  ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_testpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.957201\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.145412\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 18.770048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aRRMSE_list = []\n",
    "\n",
    "for i in [0,1,2] :\n",
    "    \n",
    "    Errors_test =  Y_testpred[:,i] - response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    ytrain_mean_i = np.tile(Y_trainpred[:,i].mean(axis = 0), (covariables_test.shape[0], 1))\n",
    "\n",
    "    Errors_relative = ytrain_mean_i - response_test[:,i]\n",
    "\n",
    "    Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%Final_SPNNR_aRRMSE)\n",
    "\n",
    "    aRRMSE_list.append(Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.624220293491167"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if old way calculation is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final_Partition aRRMSE is : 84.178972\n"
     ]
    }
   ],
   "source": [
    "Errors_test =  Y_testpred - response_test\n",
    "\n",
    "\n",
    "print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "ytrain_mean = np.tile(Y_trainpred.mean(axis = 0), (response_test.shape[0], 1))\n",
    "\n",
    "Errors_relative = ytrain_mean - response_test\n",
    "\n",
    "Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(Errors_test)**2)/(LA.norm(Errors_relative)**2))  \n",
    "\n",
    "print(\"Final_Partition aRRMSE is : %f\"%Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The answer is yes ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking True aRRMSE of S-SPNNR on Slump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssh sydney\n",
    "#setPython3 \n",
    "\n",
    "import numpy \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "from numpy import linalg as LA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.io import arff\n",
    "data = arff.loadarff('slump.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "covariables = df.iloc[:,0:7].values\n",
    "response = df.iloc[:,7:10].values\n",
    "positions = np.arange(103)\n",
    "\n",
    "\n",
    "covariables_train, covariables_test, response_train, response_test,positions_train,positions_test = train_test_split(covariables, response,positions, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_covariables_train, spnnr_covariables_test, spnnr_response_train, spnnr_response_test,spnnr_positions_train,spnnr_positions_test = train_test_split(covariables, response,positions, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spnnr_Y_testpred =np.loadtxt('All_Y_test_predictions.out', delimiter=',' )\n",
    "spnnr_Y_trainpred =np.loadtxt('All_Y_train_predictions.out', delimiter=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 12.755554\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 12.263640\n",
      "<*********************************Let us check the aRRMSE*********************************>\n",
      "Final i-th aRRMSE is : 12.938749\n"
     ]
    }
   ],
   "source": [
    "spnnr_aRRMSE_list = []\n",
    "\n",
    "for i in [0,1,2] :\n",
    "    \n",
    "    spnnr_Errors_test =  spnnr_Y_testpred[:,i] - spnnr_response_test[:,i]\n",
    "    print('<*********************************Let us check the aRRMSE*********************************>')\n",
    "\n",
    "    spnnr_ytrain_mean_i = np.tile(spnnr_Y_trainpred[:,i].mean(axis = 0), (spnnr_covariables_test.shape[0], 1))\n",
    "\n",
    "    spnnr_Errors_relative = spnnr_ytrain_mean_i - spnnr_response_test[:,i]\n",
    "\n",
    "    spnnr_Final_SPNNR_aRRMSE = 100*np.sqrt((LA.norm(spnnr_Errors_test)**2)/(LA.norm(spnnr_Errors_relative)**2))  \n",
    "\n",
    "    print(\"Final i-th aRRMSE is : %f\"%spnnr_Final_SPNNR_aRRMSE)\n",
    "\n",
    "    spnnr_aRRMSE_list.append(spnnr_Final_SPNNR_aRRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.652647644355206"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spnnr_aRRMSE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
